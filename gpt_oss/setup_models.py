#!/usr/bin/env python3
"""
GPT-OSS æ¨¡å‹è¨­ç½®è…³æœ¬
RTX 4070 8GB å„ªåŒ–ç‰ˆæœ¬ - è‡ªå‹•é¸æ“‡åˆé©çš„é–‹æºæ¨¡å‹
"""

import os
import sys
import logging
from pathlib import Path
from typing import List, Dict, Any
try:
    from huggingface_hub import snapshot_download
except ImportError:
    snapshot_download = None
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

logging.basicConfig(level=logging.INFO, format='%(levelname)s:%(name)s:%(message)s')
logger = logging.getLogger(__name__)

def setup_model_directory():
    """è¨­ç½®æ¨¡å‹ç›®éŒ„"""
    base_dir = Path(os.path.dirname(__file__))
    model_dir = base_dir / "models"
    cache_dir = base_dir / "cache"
    
    model_dir.mkdir(exist_ok=True, parents=True)
    cache_dir.mkdir(exist_ok=True, parents=True)
    
    logger.info(f"æ¨¡å‹ç›®éŒ„: {model_dir}")
    logger.info(f"ç·©å­˜ç›®éŒ„: {cache_dir}")
    
    return model_dir, cache_dir

def get_recommended_models() -> List[Dict[str, Any]]:
    """ç²å–æ¨è–¦æ¨¡å‹åˆ—è¡¨"""
    return [
        {
            "name": "microsoft/DialoGPT-medium",
            "memory_gb": 0.5,
            "description": "è¼•é‡å°è©±æ¨¡å‹ï¼Œå¿«é€Ÿå•Ÿå‹•",
            "chinese_support": "åŸºç¤",
            "financial_support": "ä¸€èˆ¬"
        },
        {
            "name": "microsoft/DialoGPT-large", 
            "memory_gb": 1.5,
            "description": "ä¸­ç­‰å°è©±æ¨¡å‹ï¼Œå¹³è¡¡æ€§èƒ½",
            "chinese_support": "åŸºç¤",
            "financial_support": "ä¸€èˆ¬"
        },
        {
            "name": "Qwen/Qwen2-1.5B-Instruct",
            "memory_gb": 2.0,
            "description": "ä¸­æ–‡å„ªåŒ–æ¨¡å‹ï¼Œæ¨è–¦é¸æ“‡",
            "chinese_support": "å„ªç§€",
            "financial_support": "è‰¯å¥½",
            "trust_remote_code": True
        },
        {
            "name": "THUDM/chatglm3-6b",
            "memory_gb": 4.0, 
            "description": "ä¸­æ–‡å¤§æ¨¡å‹ï¼Œæ€§èƒ½å¼·å‹",
            "chinese_support": "å„ªç§€",
            "financial_support": "å„ªç§€",
            "trust_remote_code": True
        }
    ]

def download_model(model_name: str, cache_dir: Path, trust_remote_code: bool = False) -> bool:
    """ä¸‹è¼‰æŒ‡å®šæ¨¡å‹"""
    logger.info(f"é–‹å§‹ä¸‹è¼‰æ¨¡å‹: {model_name}")
    
    try:
        # ä¸‹è¼‰ tokenizer
        logger.info("  - ä¸‹è¼‰ tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            cache_dir=str(cache_dir),
            trust_remote_code=trust_remote_code
        )
        
        # ä¸‹è¼‰æ¨¡å‹ (ä½¿ç”¨CausalLMä»¥æ”¯æŒç”Ÿæˆ)
        logger.info("  - ä¸‹è¼‰æ¨¡å‹...")
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            cache_dir=str(cache_dir),
            trust_remote_code=trust_remote_code,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            low_cpu_mem_usage=True
        )
        
        logger.info(f"âœ… æ¨¡å‹ {model_name} ä¸‹è¼‰å®Œæˆ")
        return True
        
    except Exception as e:
        logger.error(f"âŒ ä¸‹è¼‰æ¨¡å‹å¤±æ•—: {e}")
        if trust_remote_code:
            logger.info("å˜—è©¦ä¸ä½¿ç”¨trust_remote_codeé‡æ–°ä¸‹è¼‰...")
            return download_model(model_name, cache_dir, trust_remote_code=False)
        return False

def verify_gpu_setup() -> Dict[str, Any]:
    """é©—è­‰GPUè¨­ç½®ä¸¦è¿”å›ç¡¬é«”ä¿¡æ¯"""
    if torch.cuda.is_available():
        gpu_count = torch.cuda.device_count()
        gpu_name = torch.cuda.get_device_name(0) if gpu_count > 0 else "Unknown"
        memory_total = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # GB
        
        logger.info(f"ğŸ® GPUå¯ç”¨: {gpu_count} å€‹è¨­å‚™")
        logger.info(f"ğŸ® ä¸»GPU: {gpu_name}")
        logger.info(f"ğŸ® GPUè¨˜æ†¶é«”: {memory_total:.1f} GB")
        
        return {
            "available": True,
            "count": gpu_count,
            "name": gpu_name,
            "memory_gb": memory_total
        }
    else:
        logger.info("ğŸ’» CUDAä¸å¯ç”¨ï¼Œå°‡ä½¿ç”¨CPUæ¨¡å¼")
        return {
            "available": False,
            "count": 0,
            "name": "CPU",
            "memory_gb": 0
        }

def create_model_config(model_name: str, base_dir: Path, gpu_info: Dict[str, Any]):
    """å‰µå»ºæ¨¡å‹é…ç½®æ–‡ä»¶"""
    config_content = f"""# GPT-OSS æ¨¡å‹é…ç½® - RTX 4070 8GB å„ªåŒ–
BASE_MODEL={model_name}
TORCH_DTYPE=float16
LOAD_IN_4BIT={'true' if gpu_info['memory_gb'] <= 8 and gpu_info['available'] else 'false'}
DEVICE_MAP={'auto' if gpu_info['available'] else 'cpu'}
MAX_LENGTH=4096
MAX_NEW_TOKENS=2048
TEMPERATURE=0.3
TOP_P=0.9
HOST=0.0.0.0
PORT=8080
WORKERS=1
"""
    
    config_path = base_dir / "model.env"
    with open(config_path, 'w', encoding='utf-8') as f:
        f.write(config_content)
    
    logger.info(f"âœ… æ¨¡å‹é…ç½®æ–‡ä»¶å·²å‰µå»º: {config_path}")
    return config_path

def select_best_model(models: List[Dict[str, Any]], gpu_info: Dict[str, Any]) -> Dict[str, Any]:
    """æ ¹æ“šç¡¬é«”é¸æ“‡æœ€ä½³æ¨¡å‹"""
    if not gpu_info['available']:
        # CPUæ¨¡å¼é¸æ“‡æœ€è¼•é‡çš„æ¨¡å‹
        return models[0]  # DialoGPT-medium
    
    # GPUæ¨¡å¼æ ¹æ“šVRAMé¸æ“‡
    max_memory = gpu_info['memory_gb'] * 0.8  # é ç•™20%è¨˜æ†¶é«”
    
    suitable_models = [m for m in models if m['memory_gb'] <= max_memory]
    
    if not suitable_models:
        return models[0]  # å‚™é¸æœ€å°çš„æ¨¡å‹
    
    # å„ªå…ˆé¸æ“‡ä¸­æ–‡æ”¯æ´å¥½çš„æ¨¡å‹
    for model in suitable_models:
        if model.get('chinese_support') == 'å„ªç§€':
            return model
    
    # å¦å‰‡é¸æ“‡æœ€å¤§çš„åˆé©æ¨¡å‹
    return max(suitable_models, key=lambda x: x['memory_gb'])

def test_model_generation(model_name: str, cache_dir: Path, trust_remote_code: bool = False) -> bool:
    """æ¸¬è©¦æ¨¡å‹ç”Ÿæˆèƒ½åŠ›"""
    logger.info(f"ğŸ§ª æ¸¬è©¦æ¨¡å‹ç”Ÿæˆ: {model_name}")
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            model_name, 
            cache_dir=str(cache_dir),
            trust_remote_code=trust_remote_code
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name, 
            cache_dir=str(cache_dir),
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else "cpu",
            trust_remote_code=trust_remote_code
        )
        
        # è¨­å®špad_token
        if tokenizer.pad_token is None:
            if tokenizer.eos_token is not None:
                tokenizer.pad_token = tokenizer.eos_token
            else:
                tokenizer.pad_token = tokenizer.unk_token
        
        # æ¸¬è©¦ç”Ÿæˆ
        test_prompt = "è«‹åˆ†æå°ç©é›»è‚¡ç¥¨"
        inputs = tokenizer(test_prompt, return_tensors="pt")
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=30,
                temperature=0.7,
                pad_token_id=tokenizer.pad_token_id,
                do_sample=True
            )
        
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        logger.info(f"  âœ… ç”Ÿæˆçµæœ: {generated_text[:100]}...")
        return True
        
    except Exception as e:
        logger.error(f"  âŒ æ¨¡å‹ç”Ÿæˆæ¸¬è©¦å¤±æ•—: {e}")
        return False

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ¯ GPT-OSS æ¨¡å‹è¨­ç½®å·¥å…· (RTX 4070 8GB å„ªåŒ–)")
    print("=" * 60)
    
    # æª¢æŸ¥GPU
    gpu_info = verify_gpu_setup()
    
    # è¨­ç½®ç›®éŒ„
    model_dir, cache_dir = setup_model_directory()
    
    # ç²å–æ¨è–¦æ¨¡å‹
    models = get_recommended_models()
    
    print(f"\nğŸ“‹ æ¨è–¦æ¨¡å‹ (æ ¹æ“šæ‚¨çš„ç¡¬é«”: {gpu_info['name']}, {gpu_info['memory_gb']:.1f}GB):")
    for i, model in enumerate(models, 1):
        emoji = "ğŸš€" if model.get('chinese_support') == 'å„ªç§€' else "ğŸ“"
        print(f"  {i}. {emoji} {model['name']}")
        print(f"     è¨˜æ†¶é«”éœ€æ±‚: {model['memory_gb']}GB")
        print(f"     ä¸­æ–‡æ”¯æŒ: {model['chinese_support']}")
        print(f"     é‡‘èé ˜åŸŸ: {model['financial_support']}")
        print(f"     æè¿°: {model['description']}")
        print()
    
    # è‡ªå‹•é¸æ“‡æœ€ä½³æ¨¡å‹
    best_model = select_best_model(models, gpu_info)
    
    print(f"âš¡ è‡ªå‹•é¸æ“‡æœ€ä½³æ¨¡å‹: {best_model['name']}")
    print(f"   ç†ç”±: {best_model['description']}")
    print(f"   è¨˜æ†¶é«”éœ€æ±‚: {best_model['memory_gb']}GB")
    
    # ä¸‹è¼‰æ¨¡å‹
    trust_remote_code = best_model.get('trust_remote_code', False)
    success = download_model(best_model['name'], cache_dir, trust_remote_code)
    
    if success:
        # æ¸¬è©¦æ¨¡å‹
        print("\nğŸ§ª æ¸¬è©¦æ¨¡å‹ç”Ÿæˆèƒ½åŠ›...")
        test_success = test_model_generation(best_model['name'], cache_dir, trust_remote_code)
        
        if test_success:
            print("âœ… æ¨¡å‹æ¸¬è©¦é€šé")
        else:
            print("âš ï¸  æ¨¡å‹æ¸¬è©¦å¤±æ•—ï¼Œä½†ä»å¯å˜—è©¦ä½¿ç”¨")
        
        # å‰µå»ºé…ç½®æ–‡ä»¶
        config_path = create_model_config(best_model['name'], cache_dir.parent, gpu_info)
        
        # æˆåŠŸå®Œæˆ
        print("\n" + "=" * 60)
        print("ğŸ‰ GPT-OSS æ¨¡å‹è¨­ç½®å®Œæˆï¼")
        print("=" * 60)
        print(f"ğŸ“ å·²é…ç½®æ¨¡å‹: {best_model['name']}")
        print(f"ğŸ“ é…ç½®æ–‡ä»¶: {config_path}")
        print(f"ğŸ® é‹è¡Œç’°å¢ƒ: {gpu_info['name']} ({gpu_info['memory_gb']:.1f}GB)")
        print("\nğŸ’¡ ç¾åœ¨å¯ä»¥é‹è¡Œå•Ÿå‹•å‘½ä»¤:")
        print("   Windows: start_gpt_oss.bat")
        print("   Linux:   ./start_gpt_oss.sh")
        print("   Python:  python server.py")
        print("\nğŸŒ å•Ÿå‹•å¾Œè¨ªå•: http://localhost:8080/health")
        print("=" * 60)
        
    else:
        # å˜—è©¦å‚™é¸æ¨¡å‹
        print("âŒ é¦–é¸æ¨¡å‹ä¸‹è¼‰å¤±æ•—ï¼Œå˜—è©¦å‚™é¸æ¨¡å‹...")
        backup_model = models[0]  # DialoGPT-medium
        
        if download_model(backup_model['name'], cache_dir, False):
            config_path = create_model_config(backup_model['name'], cache_dir.parent, gpu_info)
            print(f"âœ… å‚™é¸æ¨¡å‹ {backup_model['name']} è¨­ç½®å®Œæˆ")
            print(f"ğŸ“ é…ç½®æ–‡ä»¶: {config_path}")
        else:
            print("âŒ æ‰€æœ‰æ¨¡å‹è¨­ç½®å¤±æ•—")
            print("ğŸ’¡ è«‹æª¢æŸ¥ç¶²è·¯é€£æ¥æˆ–æ‰‹å‹•ä¸‹è¼‰æ¨¡å‹")
            sys.exit(1)

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nâŒ ç”¨æˆ¶ä¸­æ–·æ“ä½œ")
        sys.exit(1)
    except Exception as e:
        logger.error(f"âŒ è¨­ç½®éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
        sys.exit(1)