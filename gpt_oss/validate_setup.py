#!/usr/bin/env python3
"""
GPT-OSS é…ç½®é©—è­‰è…³æœ¬
é©—è­‰æ‰€æœ‰é…ç½®å’Œç’°å¢ƒæ˜¯å¦æ­£ç¢ºè¨­ç½®
"""

import os
import sys
import yaml
import torch
from pathlib import Path
from typing import Dict, Any
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class GPTOSSValidator:
    """GPT-OSS é…ç½®é©—è­‰å™¨"""
    
    def __init__(self):
        self.base_dir = Path(os.path.dirname(__file__))
        self.errors = []
        self.warnings = []
        
    def validate_environment(self) -> bool:
        """é©—è­‰é‹è¡Œç’°å¢ƒ"""
        logger.info("ğŸ” æª¢æŸ¥é‹è¡Œç’°å¢ƒ...")
        
        # æª¢æŸ¥Pythonç‰ˆæœ¬
        if sys.version_info < (3, 8):
            self.errors.append(f"Pythonç‰ˆæœ¬éä½: {sys.version_info}, éœ€è¦3.8+")
        else:
            logger.info(f"âœ… Pythonç‰ˆæœ¬: {sys.version_info.major}.{sys.version_info.minor}")
        
        # æª¢æŸ¥CUDA
        cuda_available = torch.cuda.is_available()
        if cuda_available:
            gpu_name = torch.cuda.get_device_name(0)
            memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            logger.info(f"âœ… CUDAå¯ç”¨: {gpu_name} ({memory_gb:.1f}GB)")
            
            if memory_gb < 6:
                self.warnings.append(f"GPUè¨˜æ†¶é«”è¼ƒå°‘ ({memory_gb:.1f}GB), å»ºè­°ä½¿ç”¨è¼•é‡æ¨¡å‹")
        else:
            self.warnings.append("CUDAä¸å¯ç”¨ï¼Œå°‡ä½¿ç”¨CPUæ¨¡å¼")
            logger.info("âš ï¸  CUDAä¸å¯ç”¨ï¼Œå°‡ä½¿ç”¨CPUæ¨¡å¼")
        
        return len(self.errors) == 0
    
    def validate_dependencies(self) -> bool:
        """é©—è­‰ä¾è³´åŒ…"""
        logger.info("ğŸ” æª¢æŸ¥ä¾è³´åŒ…...")
        
        required_packages = [
            ('torch', 'PyTorch'),
            ('transformers', 'Transformers'),
            ('fastapi', 'FastAPI'),
            ('uvicorn', 'Uvicorn'),
            ('pydantic', 'Pydantic')
        ]
        
        optional_packages = [
            ('bitsandbytes', 'BitsAndBytes (é‡åŒ–æ”¯æŒ)'),
            ('peft', 'PEFT (LoRAæ”¯æŒ)'),
            ('flash_attn', 'Flash Attention (æ€§èƒ½å„ªåŒ–)')
        ]
        
        for package, name in required_packages:
            try:
                __import__(package)
                logger.info(f"âœ… {name}")
            except ImportError:
                self.errors.append(f"ç¼ºå°‘å¿…è¦åŒ…: {name} ({package})")
        
        for package, name in optional_packages:
            try:
                __import__(package)
                logger.info(f"âœ… {name}")
            except ImportError:
                self.warnings.append(f"ç¼ºå°‘å¯é¸åŒ…: {name} ({package})")
        
        return len(self.errors) == 0
    
    def validate_config_file(self) -> bool:
        """é©—è­‰é…ç½®æ–‡ä»¶"""
        logger.info("ğŸ” æª¢æŸ¥é…ç½®æ–‡ä»¶...")
        
        config_path = self.base_dir / "config.yaml"
        if not config_path.exists():
            self.errors.append(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
            return False
        
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            
            # æª¢æŸ¥å¿…è¦é…ç½®
            required_sections = ['server', 'model', 'performance']
            for section in required_sections:
                if section not in config:
                    self.errors.append(f"é…ç½®æ–‡ä»¶ç¼ºå°‘å¿…è¦ç¯€: {section}")
                else:
                    logger.info(f"âœ… é…ç½®ç¯€ '{section}' å­˜åœ¨")
            
            # æª¢æŸ¥æ¨¡å‹é…ç½®
            if 'model' in config:
                model_config = config['model']
                model_name = model_config.get('name')
                if model_name:
                    logger.info(f"âœ… é…ç½®çš„æ¨¡å‹: {model_name}")
                    
                    # é©—è­‰æ¨¡å‹è¨˜æ†¶é«”éœ€æ±‚
                    memory_requirements = {
                        "microsoft/DialoGPT-medium": 0.5,
                        "microsoft/DialoGPT-large": 1.5,
                        "Qwen/Qwen2-1.5B-Instruct": 2.0,
                        "THUDM/chatglm3-6b": 4.0,
                        "baichuan-inc/Baichuan2-7B-Chat": 5.0
                    }
                    
                    model_memory = memory_requirements.get(model_name, 2.0)
                    if torch.cuda.is_available():
                        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                        if model_memory > gpu_memory * 0.8:
                            self.warnings.append(f"æ¨¡å‹è¨˜æ†¶é«”éœ€æ±‚ ({model_memory}GB) å¯èƒ½è¶…éå¯ç”¨VRAM ({gpu_memory:.1f}GB)")
                else:
                    self.errors.append("é…ç½®æ–‡ä»¶æœªæŒ‡å®šæ¨¡å‹åç¨±")
            
            logger.info("âœ… é…ç½®æ–‡ä»¶æ ¼å¼æ­£ç¢º")
            return True
            
        except Exception as e:
            self.errors.append(f"é…ç½®æ–‡ä»¶è§£æå¤±æ•—: {e}")
            return False
    
    def validate_startup_scripts(self) -> bool:
        """é©—è­‰å•Ÿå‹•è…³æœ¬"""
        logger.info("ğŸ” æª¢æŸ¥å•Ÿå‹•è…³æœ¬...")
        
        scripts = [
            ("start_gpt_oss.bat", "Windowså•Ÿå‹•è…³æœ¬"),
            ("start_gpt_oss.sh", "Linuxå•Ÿå‹•è…³æœ¬"),
            ("server.py", "ä¸»æœå‹™å™¨è…³æœ¬")
        ]
        
        for script, description in scripts:
            script_path = self.base_dir / script
            if script_path.exists():
                logger.info(f"âœ… {description}")
                
                # æª¢æŸ¥åŸ·è¡Œæ¬Šé™ (åƒ…Linux)
                if script.endswith('.sh') and os.name == 'posix':
                    if not os.access(script_path, os.X_OK):
                        self.warnings.append(f"{script} ç„¡åŸ·è¡Œæ¬Šé™ï¼Œé‹è¡Œ: chmod +x {script}")
            else:
                self.errors.append(f"ç¼ºå°‘{description}: {script}")
        
        return len([e for e in self.errors if any(s in e for s in ['start_gpt_oss', 'server.py'])]) == 0
    
    def validate_model_setup_script(self) -> bool:
        """é©—è­‰æ¨¡å‹è¨­ç½®è…³æœ¬"""
        logger.info("ğŸ” æª¢æŸ¥æ¨¡å‹è¨­ç½®è…³æœ¬...")
        
        setup_script = self.base_dir / "setup_models.py"
        if setup_script.exists():
            logger.info("âœ… æ¨¡å‹è¨­ç½®è…³æœ¬å­˜åœ¨")
            return True
        else:
            self.warnings.append("æ¨¡å‹è¨­ç½®è…³æœ¬ä¸å­˜åœ¨ï¼Œéœ€è¦æ‰‹å‹•ä¸‹è¼‰æ¨¡å‹")
            return False
    
    def run_validation(self) -> bool:
        """é‹è¡Œå®Œæ•´é©—è­‰"""
        logger.info("ğŸ¯ é–‹å§‹ GPT-OSS é…ç½®é©—è­‰")
        logger.info("=" * 50)
        
        validations = [
            ("ç’°å¢ƒæª¢æŸ¥", self.validate_environment),
            ("ä¾è³´åŒ…æª¢æŸ¥", self.validate_dependencies),
            ("é…ç½®æ–‡ä»¶æª¢æŸ¥", self.validate_config_file),
            ("å•Ÿå‹•è…³æœ¬æª¢æŸ¥", self.validate_startup_scripts),
            ("æ¨¡å‹è¨­ç½®è…³æœ¬æª¢æŸ¥", self.validate_model_setup_script)
        ]
        
        all_passed = True
        for name, validator in validations:
            try:
                result = validator()
                if not result:
                    all_passed = False
            except Exception as e:
                self.errors.append(f"{name}éç¨‹å‡ºéŒ¯: {e}")
                all_passed = False
        
        # é¡¯ç¤ºçµæœ
        self.show_results()
        
        return all_passed and len(self.errors) == 0
    
    def show_results(self):
        """é¡¯ç¤ºé©—è­‰çµæœ"""
        logger.info("=" * 50)
        logger.info("ğŸ“Š é©—è­‰çµæœç¸½çµ")
        logger.info("=" * 50)
        
        if self.errors:
            logger.error(f"âŒ ç™¼ç¾ {len(self.errors)} å€‹éŒ¯èª¤:")
            for i, error in enumerate(self.errors, 1):
                logger.error(f"  {i}. {error}")
        
        if self.warnings:
            logger.warning(f"âš ï¸  ç™¼ç¾ {len(self.warnings)} å€‹è­¦å‘Š:")
            for i, warning in enumerate(self.warnings, 1):
                logger.warning(f"  {i}. {warning}")
        
        if not self.errors and not self.warnings:
            logger.info("ğŸ‰ æ‰€æœ‰æª¢æŸ¥é€šéï¼é…ç½®å®Œç¾ï¼")
        elif not self.errors:
            logger.info("âœ… ä¸»è¦é…ç½®æ­£ç¢ºï¼Œæœ‰ä¸€äº›æ¬¡è¦è­¦å‘Š")
            logger.info("ğŸ’¡ å»ºè­°è™•ç†è­¦å‘Šä»¥ç²å¾—æœ€ä½³æ€§èƒ½")
        else:
            logger.error("âŒ é…ç½®å­˜åœ¨å•é¡Œï¼Œéœ€è¦ä¿®å¾©å¾Œæ‰èƒ½æ­£å¸¸é‹è¡Œ")
        
        # æä¾›ä¸‹ä¸€æ­¥å»ºè­°
        if not self.errors:
            logger.info("\nğŸ’¡ ä¸‹ä¸€æ­¥æ“ä½œ:")
            logger.info("1. é‹è¡Œæ¨¡å‹è¨­ç½®: python setup_models.py")
            logger.info("2. å•Ÿå‹•æœå‹™: start_gpt_oss.bat (Windows) æˆ– ./start_gpt_oss.sh (Linux)")
            logger.info("3. æ¸¬è©¦å¥åº·æª¢æŸ¥: curl http://localhost:8080/health")
        else:
            logger.info("\nğŸ”§ ä¿®å¾©å»ºè­°:")
            logger.info("1. å®‰è£ç¼ºå°‘çš„ä¾è³´åŒ…: pip install -r requirements-gpt-oss.txt")
            logger.info("2. æª¢æŸ¥ä¸¦ä¿®å¾©é…ç½®æ–‡ä»¶")
            logger.info("3. é‡æ–°é‹è¡Œé©—è­‰: python validate_setup.py")

def main():
    """ä¸»å‡½æ•¸"""
    validator = GPTOSSValidator()
    success = validator.run_validation()
    
    if success:
        logger.info("ğŸ¯ é…ç½®é©—è­‰æˆåŠŸï¼Œå¯ä»¥é–‹å§‹ä½¿ç”¨GPT-OSSï¼")
        return 0
    else:
        logger.error("âŒ é…ç½®é©—è­‰å¤±æ•—ï¼Œè«‹ä¿®å¾©å•é¡Œå¾Œé‡è©¦")
        return 1

if __name__ == "__main__":
    sys.exit(main())