# GPT-OSS RTX 4070 8GB è¨­ç½®æŒ‡å—

æœ¬æŒ‡å—å°ˆç‚º RTX 4070 8GB VRAM ç’°å¢ƒå„ªåŒ–ï¼Œæä¾›å®Œæ•´çš„é–‹æºæ¨¡å‹é…ç½®å’Œéƒ¨ç½²æ–¹æ¡ˆã€‚

## ğŸ¯ æ ¸å¿ƒç‰¹è‰²

- **è¨˜æ†¶é«”å„ªåŒ–**: é‡å° 8GB VRAM ç²¾ç¢ºèª¿æ•´
- **æ™ºèƒ½æ¨¡å‹é¸æ“‡**: æ ¹æ“šç¡¬é«”è‡ªå‹•é¸æ“‡æœ€é©åˆçš„é–‹æºæ¨¡å‹
- **ä¸­æ–‡é‡‘èå°ˆç”¨**: å„ªåŒ–é‡‘èåˆ†æå’Œä¸­æ–‡å°è©±èƒ½åŠ›
- **ä¸€éµå•Ÿå‹•**: å…¨è‡ªå‹•ç’°å¢ƒæª¢æ¸¬å’Œé…ç½®
- **ç„¡éœ€æˆæ¬Š**: å®Œå…¨ä½¿ç”¨é–‹æºæ¨¡å‹ï¼Œç„¡éœ€ç‰¹æ®Šæˆæ¬Š

## ğŸ“‹ æ¨è–¦æ¨¡å‹ (æŒ‰è¨˜æ†¶é«”éœ€æ±‚æ’åº)

| æ¨¡å‹ | VRAMéœ€æ±‚ | ä¸­æ–‡æ”¯æŒ | é‡‘èé ˜åŸŸ | èªªæ˜ |
|------|----------|----------|----------|------|
| **Qwen/Qwen2-1.5B-Instruct** | ~2GB | å„ªç§€ | è‰¯å¥½ | **æ¨è–¦é¸æ“‡** - æœ€ä½³å¹³è¡¡ |
| microsoft/DialoGPT-large | ~1.5GB | åŸºç¤ | ä¸€èˆ¬ | ç©©å®šçš„å°è©±æ¨¡å‹ |
| microsoft/DialoGPT-medium | ~0.5GB | åŸºç¤ | ä¸€èˆ¬ | è¶…è¼•é‡é¸æ“‡ |
| THUDM/chatglm3-6b | ~4GB | å„ªç§€ | å„ªç§€ | é«˜æ€§èƒ½ä¸­æ–‡æ¨¡å‹ |

## ğŸš€ å¿«é€Ÿé–‹å§‹

### 1. ç’°å¢ƒæº–å‚™

ç¢ºä¿å·²å®‰è£å¿…è¦ä¾è³´:

```bash
# å®‰è£ä¾è³´ (åœ¨ TradingAgents/gpt_oss ç›®éŒ„ä¸‹)
pip install -r requirements-gpt-oss.txt

# æˆ–å®‰è£CPUç‰ˆæœ¬ (å¦‚æœæ²’æœ‰GPU)
pip install -r requirements-gpt-oss-cpu.txt
```

### 2. é©—è­‰é…ç½®

é‹è¡Œé…ç½®é©—è­‰è…³æœ¬:

```bash
python validate_setup.py
```

æ­¤è…³æœ¬æœƒè‡ªå‹•æª¢æŸ¥:
- âœ… GPU/CPU ç’°å¢ƒ
- âœ… ä¾è³´åŒ…å®Œæ•´æ€§  
- âœ… é…ç½®æ–‡ä»¶æ­£ç¢ºæ€§
- âœ… å•Ÿå‹•è…³æœ¬å¯ç”¨æ€§

### 3. ä¸‹è¼‰å’Œè¨­ç½®æ¨¡å‹

```bash
# è‡ªå‹•ä¸‹è¼‰æœ€é©åˆçš„æ¨¡å‹
python setup_models.py
```

è…³æœ¬æœƒæ ¹æ“šä½ çš„ç¡¬é«”é…ç½®è‡ªå‹•:
- ğŸ” æª¢æ¸¬GPUè¨˜æ†¶é«”å¤§å°
- ğŸ¯ é¸æ“‡æœ€é©åˆçš„æ¨¡å‹
- ğŸ“¥ è‡ªå‹•ä¸‹è¼‰æ¨¡å‹æ–‡ä»¶
- ğŸ§ª æ¸¬è©¦æ¨¡å‹ç”Ÿæˆèƒ½åŠ›
- âš™ï¸ ç”Ÿæˆå„ªåŒ–é…ç½®

### 4. å•Ÿå‹•æœå‹™

#### Windows:
```cmd
start_gpt_oss.bat
```

#### Linux/Mac:
```bash
chmod +x start_gpt_oss.sh
./start_gpt_oss.sh
```

#### ç›´æ¥Python:
```bash
python server.py
```

### 5. æ¸¬è©¦æœå‹™

æœå‹™å•Ÿå‹•å¾Œï¼Œæ¸¬è©¦å¥åº·æª¢æŸ¥:

```bash
curl http://localhost:8080/health
```

é æœŸè¿”å›:
```json
{
  "status": "healthy",
  "service": "gpt-oss",
  "version": "2.0.0",
  "device": "cuda",
  "model": "Qwen/Qwen2-1.5B-Instruct",
  "model_loaded": true
}
```

## âš™ï¸ é…ç½®è©³è§£

### config.yaml ä¸»è¦é…ç½®é …

```yaml
model:
  # åŸºç¤æ¨¡å‹ (è‡ªå‹•é¸æ“‡æœ€ä½³)
  name: "Qwen/Qwen2-1.5B-Instruct"
  
  # è¨˜æ†¶é«”é…ç½®
  torch_dtype: "float16"     # ç¯€çœè¨˜æ†¶é«”
  load_in_4bit: true         # 4-bité‡åŒ–
  max_memory:
    0: "7GB"                 # ç‚º8GB VRAMé ç•™1GB

performance:
  # RTX 4070 8GB å„ªåŒ–è¨­ç½®
  max_memory_allocation: 0.85        # ä¿å®ˆçš„è¨˜æ†¶é«”åˆ†é…
  garbage_collection_threshold: 0.75  # ç©æ¥µçš„åƒåœ¾å›æ”¶
  use_flash_attention_2: true        # ä½¿ç”¨é«˜æ•ˆæ³¨æ„åŠ›æ©Ÿåˆ¶
  gradient_checkpointing: true       # çŠ§ç‰²å°‘è¨±é€Ÿåº¦æ›å–è¨˜æ†¶é«”
```

### ç’°å¢ƒè®Šæ•¸

| è®Šæ•¸ | é è¨­å€¼ | èªªæ˜ |
|------|--------|------|
| `BASE_MODEL` | Qwen/Qwen2-1.5B-Instruct | åŸºç¤æ¨¡å‹åç¨± |
| `LOAD_IN_4BIT` | true | æ˜¯å¦ä½¿ç”¨4-bité‡åŒ– |
| `HOST` | 0.0.0.0 | æœå‹™å™¨ç¶å®šåœ°å€ |
| `PORT` | 8080 | æœå‹™å™¨ç«¯å£ |
| `DEVICE` | auto | è¨ˆç®—è¨­å‚™ (auto/cuda/cpu) |

## ğŸ“Š æ€§èƒ½èª¿å„ª

### RTX 4070 8GB æœ€ä½³é…ç½®

```yaml
# è¨˜æ†¶é«”å„ªåŒ–é…ç½®
performance:
  max_memory_allocation: 0.85      # ä¸è¶…é85%è¨˜æ†¶é«”
  kv_cache_dtype: "float16"        # KVç·©å­˜ä½¿ç”¨float16
  attention_dropout: 0.0           # é—œé–‰dropoutç¯€çœè¨˜æ†¶é«”
  dataloader_num_workers: 2        # é©ä¸­çš„æ•¸æ“šè¼‰å…¥ä¸¦è¡Œæ•¸

# é‡åŒ–é…ç½®  
quantization:
  bits: 4                          # 4-bité‡åŒ–
  quant_type: "nf4"               # ä½¿ç”¨NF4é‡åŒ–
  compute_dtype: "float16"         # è¨ˆç®—ä½¿ç”¨float16
  bnb_4bit_quant_storage: "uint8"  # å­˜å„²é€²ä¸€æ­¥å£“ç¸®
```

### æ‰¹è™•ç†è¨­ç½®

```yaml
generation:
  batch_size: 4           # é©ä¸­çš„æ‰¹è™•ç†å¤§å°
  max_batch_size: 8       # æœ€å¤§æ‰¹è™•ç†
  stream: true            # å•Ÿç”¨æµå¼è¼¸å‡º
```

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è¦‹å•é¡Œ

#### 1. GPUè¨˜æ†¶é«”ä¸è¶³ (OOM)

**éŒ¯èª¤**: `torch.cuda.OutOfMemoryError`

**è§£æ±ºæ–¹æ¡ˆ**:
```bash
# 1. ä½¿ç”¨æ›´å°çš„æ¨¡å‹
export BASE_MODEL="microsoft/DialoGPT-medium"

# 2. å•Ÿç”¨4-bité‡åŒ–
export LOAD_IN_4BIT="true"

# 3. æ¸›å°‘æ‰¹è™•ç†å¤§å° (ä¿®æ”¹config.yaml)
generation:
  batch_size: 1
  max_batch_size: 2
```

#### 2. æ¨¡å‹ä¸‹è¼‰å¤±æ•—

**éŒ¯èª¤**: ç¶²è·¯é€£æ¥å•é¡Œæˆ–æ¬Šé™éŒ¯èª¤

**è§£æ±ºæ–¹æ¡ˆ**:
```bash
# 1. æª¢æŸ¥ç¶²è·¯é€£æ¥
ping huggingface.co

# 2. ä½¿ç”¨å‚™é¸æ¨¡å‹
python setup_models.py

# 3. æ‰‹å‹•è¨­ç½®ä»£ç† (å¦‚éœ€è¦)
export HTTP_PROXY=http://your-proxy:port
export HTTPS_PROXY=http://your-proxy:port
```

#### 3. æ¨¡å‹è¼‰å…¥éŒ¯èª¤

**éŒ¯èª¤**: `trust_remote_code` ç›¸é—œéŒ¯èª¤

**è§£æ±ºæ–¹æ¡ˆ**: è…³æœ¬æœƒè‡ªå‹•å›é€€åˆ°æ¨™æº–è¼‰å…¥æ¨¡å¼ï¼Œç„¡éœ€æ‰‹å‹•è™•ç†ã€‚

#### 4. CPUæ¨¡å¼æ€§èƒ½å·®

**ç¾è±¡**: GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPUé‹è¡Œå¾ˆæ…¢

**è§£æ±ºæ–¹æ¡ˆ**:
```bash
# 1. æª¢æŸ¥CUDAå®‰è£
nvidia-smi

# 2. æª¢æŸ¥PyTorch CUDAæ”¯æ´
python -c "import torch; print(torch.cuda.is_available())"

# 3. å¦‚æœç¢ºå¯¦åªèƒ½ç”¨CPUï¼Œé¸æ“‡æœ€å°æ¨¡å‹
export BASE_MODEL="microsoft/DialoGPT-medium"
```

### æ€§èƒ½ç›£æ§

#### æª¢æŸ¥è¨˜æ†¶é«”ä½¿ç”¨

```bash
# å¯¦æ™‚ç›£æ§GPUè¨˜æ†¶é«”
watch -n 1 nvidia-smi

# é€šéAPIæŸ¥çœ‹æœå‹™å™¨è¨˜æ†¶é«”ç‹€æ…‹
curl http://localhost:8080/memory
```

#### æª¢æŸ¥æœå‹™ç‹€æ…‹

```bash
# å¥åº·æª¢æŸ¥
curl http://localhost:8080/health

# æœå‹™å™¨ç‹€æ…‹
curl http://localhost:8080/status

# å¯ç”¨æ¨¡å‹åˆ—è¡¨
curl http://localhost:8080/models
```

## ğŸ§ª æ¸¬è©¦å’Œé©—è­‰

### APIæ¸¬è©¦

```bash
# æ¸¬è©¦èŠå¤©API
curl -X POST http://localhost:8080/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "è«‹åˆ†æå°ç©é›»è‚¡ç¥¨çš„æŠ•è³‡åƒ¹å€¼",
    "max_tokens": 200,
    "temperature": 0.7
  }'
```

### é‡‘èåˆ†ææ¸¬è©¦

```python
import requests

# æ¸¬è©¦é‡‘èåˆ†æèƒ½åŠ›
response = requests.post("http://localhost:8080/chat", json={
    "message": "æ ¹æ“šä»¥ä¸‹è³‡æ–™åˆ†æå°ç©é›»(2330): è‚¡åƒ¹450å…ƒï¼Œæœ¬ç›Šæ¯”25å€ï¼Œè«‹çµ¦å‡ºæŠ•è³‡å»ºè­°",
    "max_tokens": 300,
    "temperature": 0.3
})

print(response.json())
```

## ğŸ“ˆ èˆ‡TradingAgentsé›†æˆ

### é…ç½®TradingAgentsä½¿ç”¨GPT-OSS

åœ¨ `TradingAgents/config.py` ä¸­:

```python
# GPT-OSSé…ç½®
GPT_OSS_CONFIG = {
    'base_url': 'http://localhost:8080',
    'model': 'llama-3-8b',  # æˆ–å¯¦éš›é…ç½®çš„æ¨¡å‹
    'timeout': 30,
    'max_retries': 3
}
```

### æ¸¬è©¦é›†æˆ

```python
# æ¸¬è©¦TradingAgentsèˆ‡GPT-OSSé›†æˆ
python test_gpt_oss_integration.py
```

## ğŸ“š é€²éšé…ç½®

### LoRAé©é…å™¨æ”¯æ´

å¦‚æœæœ‰å¾®èª¿çš„LoRAé©é…å™¨:

```bash
# å•Ÿå‹•æ™‚æŒ‡å®šLoRAé©é…å™¨
export LORA_ADAPTER="/path/to/your/lora/adapter"
./start_gpt_oss.sh
```

### å¤šGPUæ”¯æ´

å°æ–¼å¤šGPUç’°å¢ƒ (å¦‚RTX 4070 + RTX 4070):

```yaml
model:
  device_map: "auto"  # è‡ªå‹•åˆ†é…åˆ°å¤šå€‹GPU
  max_memory:
    0: "7GB"         # ç¬¬ä¸€å€‹GPU
    1: "7GB"         # ç¬¬äºŒå€‹GPU
```

### ç”Ÿç”¢ç’°å¢ƒé…ç½®

```yaml
server:
  workers: 2                    # å¢åŠ å·¥ä½œé€²ç¨‹
  max_concurrent_requests: 16   # å¢åŠ ä¸¦ç™¼è«‹æ±‚æ•¸

monitoring:
  enable_metrics: true          # å•Ÿç”¨PrometheusæŒ‡æ¨™
  metrics_port: 9090           # æŒ‡æ¨™ç«¯å£

security:
  require_api_key: true        # å•Ÿç”¨APIå¯†é‘°é©—è­‰
  api_key_header: "Authorization"
```

## ğŸ†˜ æ”¯æ´å’Œç¤¾ç¾¤

- **Issues**: [GitHub Issues](https://github.com/your-repo/issues)
- **è¨è«–**: [GitHub Discussions](https://github.com/your-repo/discussions)  
- **æ–‡ä»¶**: [å®Œæ•´æ–‡ä»¶](./docs/)

## ğŸ“„ æˆæ¬Š

æœ¬é …ç›®ä½¿ç”¨é–‹æºæ¨¡å‹ï¼Œç„¡éœ€ç‰¹æ®Šæˆæ¬Šã€‚å…·é«”æ¨¡å‹æˆæ¬Šè«‹åƒè€ƒå„æ¨¡å‹çš„è¨±å¯è­‰ã€‚

---

## âš¡ å¿«é€Ÿç¸½çµ

1. **é©—è­‰ç’°å¢ƒ**: `python validate_setup.py`
2. **è¨­ç½®æ¨¡å‹**: `python setup_models.py` 
3. **å•Ÿå‹•æœå‹™**: `start_gpt_oss.bat` (Windows) æˆ– `./start_gpt_oss.sh` (Linux)
4. **æ¸¬è©¦æœå‹™**: `curl http://localhost:8080/health`
5. **é–‹å§‹ä½¿ç”¨**: ğŸ‰

é‡å°RTX 4070 8GBï¼Œæ¨è–¦ä½¿ç”¨ **Qwen/Qwen2-1.5B-Instruct** æ¨¡å‹ï¼Œæ—¢æœ‰å„ªç§€çš„ä¸­æ–‡æ”¯æ´ï¼Œåˆèƒ½å¾ˆå¥½åœ°é©é…8GB VRAMé™åˆ¶ã€‚