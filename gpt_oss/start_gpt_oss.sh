#!/bin/bash
# GPT-OSS æœ¬åœ°æŽ¨ç†æœå‹™å•Ÿå‹•è…³æœ¬
# æ”¯æ´ RTX 4070/4090 è‡ªå‹•é…ç½®

set -e

echo "ðŸš€ å•Ÿå‹• GPT-OSS æœ¬åœ°æŽ¨ç†æœå‹™..."

# æª¢æŸ¥ CUDA å¯ç”¨æ€§
if command -v nvidia-smi &> /dev/null; then
    echo "âœ… æª¢æ¸¬åˆ° NVIDIA GPU:"
    nvidia-smi --query-gpu=name,memory.total --format=csv,noheader,nounits
    export DEVICE="auto"
else
    echo "âš ï¸  æœªæª¢æ¸¬åˆ° NVIDIA GPUï¼Œä½¿ç”¨ CPU æ¨¡å¼"
    export DEVICE="cpu"
fi

# è¨­ç½®é»˜èªç’°å¢ƒè®Šæ•¸ (RTX 4070 8GB å„ªåŒ–)
export BASE_MODEL=${BASE_MODEL:-"Qwen/Qwen2-1.5B-Instruct"}
export LOAD_IN_4BIT=${LOAD_IN_4BIT:-"true"}
export HOST=${HOST:-"0.0.0.0"}
export PORT=${PORT:-"8080"}
export WORKERS=${WORKERS:-"1"}

# æª¢æŸ¥ VRAM å¤§å°ä¸¦èª¿æ•´æ¨¡åž‹ (Linux)
if command -v nvidia-smi &> /dev/null; then
    VRAM_MB=$(nvidia-smi --query-gpu=memory.total --format=csv,noheader,nounits | head -1)
    if [ "$VRAM_MB" -lt 10000 ]; then
        echo "ðŸŽ¯ æª¢æ¸¬åˆ°ä½Ž VRAM GPU ($VRAM_MB MB)ï¼Œä½¿ç”¨è¼•é‡æ¨¡åž‹"
        export BASE_MODEL="microsoft/DialoGPT-medium"
    fi
fi

# æª¢æŸ¥æ¨¡åž‹æ˜¯å¦å­˜åœ¨
if [ ! -z "$LORA_ADAPTER" ] && [ ! -d "$LORA_ADAPTER" ]; then
    echo "âŒ LoRA adapter è·¯å¾‘ä¸å­˜åœ¨: $LORA_ADAPTER"
    exit 1
fi

# å‰µå»ºæ—¥èªŒç›®éŒ„
mkdir -p logs

echo "ðŸ“‹ é…ç½®ä¿¡æ¯:"
echo "  - åŸºç¤Žæ¨¡åž‹: $BASE_MODEL"
echo "  - LoRA Adapter: ${LORA_ADAPTER:-"ç„¡"}"
echo "  - 4-bit é‡åŒ–: $LOAD_IN_4BIT"
echo "  - è¨­å‚™: $DEVICE"
echo "  - ä¸»æ©Ÿ: $HOST"
echo "  - ç«¯å£: $PORT"
echo "  - å·¥ä½œé€²ç¨‹: $WORKERS"

# å•Ÿå‹•æœå‹™
echo "ðŸ”¥ å•Ÿå‹•æœå‹™å™¨..."
python3 server.py \
    --host "$HOST" \
    --port "$PORT" \
    --device "$DEVICE" \
    --workers "$WORKERS" \
    ${BASE_MODEL:+--base-model "$BASE_MODEL"} \
    ${LORA_ADAPTER:+--lora-adapter "$LORA_ADAPTER"} \
    ${LOAD_IN_4BIT:+--load-in-4bit} \
    2>&1 | tee logs/gpt_oss_$(date +%Y%m%d_%H%M%S).log