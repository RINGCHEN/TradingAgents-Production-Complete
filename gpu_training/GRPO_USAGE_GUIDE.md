# GRPO/PPOè¨“ç·´å®Œæ•´å¯¦ç¾ä½¿ç”¨æŒ‡å—

## ğŸ“‹ çµ¦å°kï¼ˆAIè¨“ç·´å°ˆå®¶åœ˜éšŠï¼‰çš„ä½¿ç”¨èªªæ˜

é€™æ˜¯ç‚ºTask 4.1å°ˆé–€é–‹ç™¼çš„å®Œæ•´GRPO/PPOè¨“ç·´å¯¦ç¾ï¼ŒåŒ…å«ï¼š
- å®Œæ•´çš„é‡‘èé ˜åŸŸçå‹µæ¨¡å‹
- RTX 4070å„ªåŒ–é…ç½®
- ä¼æ¥­ç´šéŒ¯èª¤è™•ç†å’Œæ¢å¾©æ©Ÿåˆ¶
- W&Bç›£æ§é›†æˆ

## ğŸš€ å¿«é€Ÿé–‹å§‹

### 1. ç’°å¢ƒæº–å‚™
```bash
# å®‰è£å¿…è¦çš„ä¾è³´
pip install transformers trl datasets torch accelerate bitsandbytes wandb tqdm numpy

# è¨­ç½®ç’°å¢ƒè®Šé‡
export WANDB_PROJECT="tradingagents-grpo"
export CUDA_VISIBLE_DEVICES=0
```

### 2. å‰µå»ºç¤ºä¾‹æ•¸æ“šé›†
```bash
# å‰µå»º100å€‹ç¤ºä¾‹æ¨£æœ¬ç”¨æ–¼æ¸¬è©¦
python gpu_training/grpo_trainer.py --create-demo --dataset ./data/demo_grpo_dataset.jsonl --demo-samples 100
```

### 3. é–‹å§‹è¨“ç·´
```bash
# ä½¿ç”¨é»˜èªé…ç½®è¨“ç·´
python gpu_training/grpo_trainer.py \
    --dataset ./data/demo_grpo_dataset.jsonl \
    --output ./models/grpo_financial_model

# ä½¿ç”¨è‡ªå®šç¾©é…ç½®æ–‡ä»¶
python gpu_training/grpo_trainer.py \
    --dataset ./data/your_dataset.jsonl \
    --output ./models/your_model \
    --config ./configs/grpo_config.json
```

## ğŸ“Š æ•¸æ“šæ ¼å¼

### æ”¯æŒçš„æ•¸æ“šæ ¼å¼

#### æ ¼å¼1: GRPOå°ˆç”¨æ ¼å¼
```json
{"query": "åˆ†æå°ç©é›»çš„æŠ•è³‡åƒ¹å€¼", "context": "å°è‚¡æŠ•è³‡åˆ†æ", "reference": "åƒè€ƒå›ç­”"}
{"query": "å¦‚ä½•è©•ä¼°é´»æµ·çš„è²¡å‹™ç‹€æ³", "context": "è²¡å‹™åˆ†æ", "reference": "åƒè€ƒå›ç­”"}
```

#### æ ¼å¼2: æ¨™æº–æŒ‡ä»¤æ ¼å¼ï¼ˆè‡ªå‹•è½‰æ›ï¼‰
```json
{"instruction": "åˆ†æå°ç©é›»çš„æŠ•è³‡åƒ¹å€¼", "input": "", "output": "å°ç©é›»æ˜¯å…¨çƒé ˜å…ˆçš„æ™¶åœ“ä»£å·¥å» ..."}
{"instruction": "è©•ä¼°é¢¨éšª", "input": "é´»æµ·ç§‘æŠ€", "output": "éœ€è¦è€ƒæ…®ä»¥ä¸‹é¢¨éšªå› ç´ ..."}
```

## âš™ï¸ é…ç½®é¸é …

### RTX 4070å„ªåŒ–é…ç½®
```json
{
  "model_name": "microsoft/DialoGPT-medium",
  "max_length": 512,
  "learning_rate": 1.4e-5,
  "batch_size": 4,
  "mini_batch_size": 2,
  "gradient_accumulation_steps": 8,
  "ppo_epochs": 4,
  "num_train_epochs": 3,
  "use_fp16": true,
  "gradient_checkpointing": true,
  "use_wandb": true
}
```

### çå‹µæ¨¡å‹é…ç½®
```json
{
  "accuracy_weight": 0.4,
  "risk_assessment_weight": 0.3,
  "recommendation_weight": 0.2,
  "language_quality_weight": 0.1,
  "max_reward": 1.0,
  "min_reward": -1.0
}
```

## ğŸ’¡ é‡‘èçå‹µæ¨¡å‹ç‰¹æ€§

### è©•ä¼°ç¶­åº¦
1. **åˆ†ææº–ç¢ºæ€§ (40%)**
   - é‡‘èé—œéµè©ä½¿ç”¨
   - å›æ‡‰é•·åº¦åˆç†æ€§
   - å•é¡Œç›¸é—œæ€§

2. **é¢¨éšªè©•ä¼° (30%)**
   - é¢¨éšªè©å½™ä½¿ç”¨
   - é¢¨éšªè­¦å‘ŠæåŠ
   - é¿å…çµ•å°è¡¨è¿°

3. **æŠ•è³‡å»ºè­° (20%)**
   - å…·é«”å»ºè­°çµ¦å‡º
   - å»ºè­°ç†ç”±èªªæ˜
   - æ“ä½œæŒ‡å°æ˜ç¢º

4. **èªè¨€å“è³ª (10%)**
   - èªå¥å®Œæ•´æ€§
   - å°ˆæ¥­è¡“èªä½¿ç”¨
   - é‚è¼¯é€£è²«æ€§

### çå‹µæ©Ÿåˆ¶
- âœ… **æ­£é¢çå‹µ**: ä½¿ç”¨é‡‘èå°ˆæ¥­è©å½™ã€æåŠé¢¨éšªæ§åˆ¶
- âŒ **è² é¢æ‡²ç½°**: ä½¿ç”¨"ä¿è­‰"ã€"ç©©è³º"ç­‰ä¸ç•¶è¡¨è¿°
- ğŸ¯ **ç‰¹æ®Šçå‹µ**: é¢¨éšªè­¦å‘Šã€åˆ†æ•£æŠ•è³‡å»ºè­°

## ğŸ“ˆ ç›£æ§å’Œèª¿è©¦

### W&Bç›£æ§
è¨“ç·´éç¨‹ä¸­æœƒè‡ªå‹•è¨˜éŒ„ï¼š
- å¹³å‡çå‹µåˆ†æ•¸
- KLæ•£åº¦
- è¨“ç·´æå¤±
- GPUä½¿ç”¨ç‡

### æ—¥èªŒæ–‡ä»¶
- è¨“ç·´æ—¥èªŒ: `/app/logs/training/grpo_trainer.log`
- æ¨¡å‹é…ç½®: `{output_dir}/training_config.json`
- è¨“ç·´çµ±è¨ˆ: `{output_dir}/training_stats.json`

## ğŸ”§ å¸¸è¦‹å•é¡Œè§£æ±º

### 1. GPUè¨˜æ†¶é«”ä¸è¶³
```bash
# æ¸›å°‘æ‰¹æ¬¡å¤§å°
--config '{"batch_size": 2, "mini_batch_size": 1}'

# å•Ÿç”¨æ¢¯åº¦æª¢æŸ¥é»
--config '{"gradient_checkpointing": true}'
```

### 2. è¨“ç·´é€Ÿåº¦æ…¢
```bash
# å¢åŠ æ¢¯åº¦ç´¯ç©æ­¥é©Ÿ
--config '{"gradient_accumulation_steps": 16}'

# æ¸›å°‘PPO epochs
--config '{"ppo_epochs": 2}'
```

### 3. çå‹µåˆ†æ•¸éä½
- æª¢æŸ¥æ•¸æ“šé›†è³ªé‡
- èª¿æ•´çå‹µæ¨¡å‹æ¬Šé‡
- å¢åŠ é‡‘èé—œéµè©

## ğŸ“ é€²éšä½¿ç”¨

### è‡ªå®šç¾©çå‹µæ¨¡å‹
```python
from gpu_training.grpo_trainer import FinancialRewardModel, FinancialRewardConfig

# å‰µå»ºè‡ªå®šç¾©çå‹µé…ç½®
custom_reward_config = FinancialRewardConfig(
    accuracy_weight=0.5,  # æé«˜æº–ç¢ºæ€§æ¬Šé‡
    risk_assessment_weight=0.3,
    recommendation_weight=0.15,
    language_quality_weight=0.05
)

# å‰µå»ºè‡ªå®šç¾©çå‹µæ¨¡å‹
reward_model = FinancialRewardModel(custom_reward_config)
```

### æ¨¡å‹æ¨ç†æ¸¬è©¦
```python
from transformers import AutoTokenizer, AutoModelForCausalLM

# è¼‰å…¥è¨“ç·´å¥½çš„æ¨¡å‹
model_path = "./models/grpo_financial_model/best_model"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path)

# æ¸¬è©¦æ¨ç†
query = "åˆ†æå°ç©é›»çš„æŠ•è³‡åƒ¹å€¼"
inputs = tokenizer.encode(query, return_tensors="pt")
outputs = model.generate(inputs, max_length=200, do_sample=True)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)
```

## ğŸ¯ æ€§èƒ½åŸºæº–

### RTX 4070é æœŸæ€§èƒ½
- **æ‰¹æ¬¡å¤§å°**: 4 (æœ€ä½³å¹³è¡¡)
- **è¨“ç·´é€Ÿåº¦**: ~2-3 æ­¥/ç§’
- **è¨˜æ†¶é«”ä½¿ç”¨**: ~8-10GB
- **æ¨è–¦è¨“ç·´æ™‚é–“**: ä¸­ç­‰æ•¸æ“šé›† 2-4å°æ™‚

### çå‹µåˆ†æ•¸åƒè€ƒ
- **å„ªç§€å›æ‡‰**: 0.7-1.0
- **è‰¯å¥½å›æ‡‰**: 0.4-0.7
- **ä¸€èˆ¬å›æ‡‰**: 0.1-0.4
- **éœ€æ”¹é€²**: < 0.1

## ğŸ“ æŠ€è¡“æ”¯æ´

å¦‚é‡å•é¡Œè«‹è¯ç¹«ï¼š
- **å¤©å·¥é–‹ç‰©æ ¸å¿ƒåœ˜éšŠ**: æ¶æ§‹å’Œé›†æˆå•é¡Œ
- **GPUç¡¬é«”å°ˆå®¶åœ˜éšŠ**: æ€§èƒ½å„ªåŒ–å•é¡Œ
- **åŸºç¤è¨­æ–½åœ˜éšŠ**: ç›£æ§å’Œéƒ¨ç½²å•é¡Œ

---

**ç¥å°kè¨“ç·´é †åˆ©ï¼** ğŸš€

*é€™å€‹å¯¦ç¾å·²ç¶“è§£æ±ºäº†Task 4.1ä¸­"ç¼ºå°‘å®Œæ•´çš„GRPOå¯¦ç¾ç¯„ä¾‹"çš„å•é¡Œ*