#!/usr/bin/env python3
"""
å®Œæ•´çš„GRPO/PPOè¨“ç·´å¯¦ç¾
ç‚ºå°kï¼ˆAIè¨“ç·´å°ˆå®¶åœ˜éšŠï¼‰æä¾›çš„å®Œæ•´GRPOè¨“ç·´ç³»çµ±

This implementation provides:
- Complete GRPO training loop with financial reward model
- RTX 4070 optimized configuration
- Financial domain-specific reward calculations
- Comprehensive error handling and recovery
- Integration with TradingAgents ecosystem

Author: TradingAgents Team (å¤©å·¥é–‹ç‰©) - æ”¯æ´AIè¨“ç·´å°ˆå®¶åœ˜éšŠ
Version: 1.0.0
"""

import os
import sys
import json
import logging
import torch
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass
from pathlib import Path
from datetime import datetime
import random
from tqdm import tqdm
import warnings

# TRLå’ŒTransformersç›¸é—œå°å…¥
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM,
    TrainingArguments,
    BitsAndBytesConfig
)
from trl import (
    PPOConfig, 
    PPOTrainer, 
    AutoModelForCausalLMWithValueHead,
    create_reference_model
)
from datasets import Dataset
import wandb

# é…ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - [GRPOè¨“ç·´å¸«] - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/app/logs/training/grpo_trainer.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger("GRPOTrainer")


@dataclass
class FinancialRewardConfig:
    """é‡‘èé ˜åŸŸçå‹µæ¨¡å‹é…ç½®"""
    # åˆ†ææº–ç¢ºæ€§æ¬Šé‡
    accuracy_weight: float = 0.4
    # é¢¨éšªè©•ä¼°æ¬Šé‡
    risk_assessment_weight: float = 0.3
    # æŠ•è³‡å»ºè­°åˆç†æ€§æ¬Šé‡
    recommendation_weight: float = 0.2
    # èªè¨€å“è³ªæ¬Šé‡
    language_quality_weight: float = 0.1
    
    # çå‹µç¯„åœ
    max_reward: float = 1.0
    min_reward: float = -1.0
    
    # é‡‘èé—œéµè©çå‹µ
    financial_keywords_bonus: float = 0.1
    # é¢¨éšªè­¦å‘Šçå‹µ
    risk_warning_bonus: float = 0.15


@dataclass
class GRPOTrainingConfig:
    """GRPOè¨“ç·´é…ç½®"""
    # æ¨¡å‹é…ç½®
    model_name: str = "microsoft/DialoGPT-medium"
    max_length: int = 512
    
    # PPOé…ç½®
    learning_rate: float = 1.4e-5
    batch_size: int = 4  # RTX 4070å„ªåŒ–
    mini_batch_size: int = 2
    gradient_accumulation_steps: int = 8
    ppo_epochs: int = 4
    max_grad_norm: float = 1.0
    
    # çå‹µæ¨¡å‹é…ç½®
    kl_penalty: str = "kl"  # æˆ– "abs", "mse"
    adaptive_kl: bool = True
    target_kl: float = 0.1
    kl_beta: float = 0.1
    
    # è¨“ç·´é…ç½®
    num_train_epochs: int = 3
    save_freq: int = 100
    eval_freq: int = 50
    log_freq: int = 10
    
    # RTX 4070å„ªåŒ–
    use_fp16: bool = True
    gradient_checkpointing: bool = True
    dataloader_num_workers: int = 4
    
    # W&Bé…ç½®
    use_wandb: bool = True
    wandb_project: str = "tradingagents-grpo"


class FinancialRewardModel:
    """
    é‡‘èé ˜åŸŸå°ˆç”¨çå‹µæ¨¡å‹
    è©•ä¼°AIç”Ÿæˆçš„é‡‘èåˆ†æå“è³ª
    """
    
    def __init__(self, config: FinancialRewardConfig):
        self.config = config
        
        # é‡‘èé—œéµè©åº«
        self.financial_keywords = [
            "æŠ•è³‡", "é¢¨éšª", "å ±é…¬", "è‚¡åƒ¹", "å¸‚å€¼", "æœ¬ç›Šæ¯”", "æ®–åˆ©ç‡",
            "æŠ€è¡“åˆ†æ", "åŸºæœ¬é¢åˆ†æ", "è¶¨å‹¢", "æ”¯æ’", "å£“åŠ›", "çªç ´",
            "è²¡å ±", "ç‡Ÿæ”¶", "ç²åˆ©", "ç¾é‡‘æµ", "è² å‚µæ¯”", "ROE", "ROA",
            "å¸‚å ´", "ç”¢æ¥­", "æ™¯æ°£", "é€šè†¨", "åˆ©ç‡", "åŒ¯ç‡", "åŸç‰©æ–™"
        ]
        
        # é¢¨éšªç›¸é—œé—œéµè©
        self.risk_keywords = [
            "é¢¨éšª", "è¬¹æ…", "å°å¿ƒ", "æ³¨æ„", "è­¦å‘Š", "ä¸ç¢ºå®š", "æ³¢å‹•",
            "ä¸‹è·Œ", "è™§æ", "å¥—ç‰¢", "åœæ", "é¢¨æ§", "åˆ†æ•£æŠ•è³‡"
        ]
        
        # è² é¢é—œéµè©ï¼ˆé™ä½çå‹µï¼‰
        self.negative_keywords = [
            "ä¿è­‰", "ç©©è³º", "å¿…æ¼²", "å¿…è·Œ", "å…§ç·š", "æ˜ç‰Œ", "é£†è‚¡"
        ]
        
    def calculate_reward(self, query: str, response: str, context: Optional[Dict] = None) -> float:
        """
        è¨ˆç®—é‡‘èåˆ†æå›æ‡‰çš„çå‹µåˆ†æ•¸
        
        Args:
            query: ç”¨æˆ¶å•é¡Œ
            response: AIå›æ‡‰
            context: é¡å¤–ä¸Šä¸‹æ–‡ä¿¡æ¯
        
        Returns:
            çå‹µåˆ†æ•¸ (-1.0 åˆ° 1.0)
        """
        try:
            total_score = 0.0
            
            # 1. åˆ†ææº–ç¢ºæ€§è©•åˆ†
            accuracy_score = self._evaluate_accuracy(query, response)
            total_score += accuracy_score * self.config.accuracy_weight
            
            # 2. é¢¨éšªè©•ä¼°è©•åˆ†
            risk_score = self._evaluate_risk_assessment(response)
            total_score += risk_score * self.config.risk_assessment_weight
            
            # 3. æŠ•è³‡å»ºè­°åˆç†æ€§è©•åˆ†
            recommendation_score = self._evaluate_recommendation(response)
            total_score += recommendation_score * self.config.recommendation_weight
            
            # 4. èªè¨€å“è³ªè©•åˆ†
            language_score = self._evaluate_language_quality(response)
            total_score += language_score * self.config.language_quality_weight
            
            # 5. çå‹µèª¿æ•´
            total_score = self._apply_bonus_penalties(response, total_score)
            
            # é™åˆ¶çå‹µç¯„åœ
            reward = max(self.config.min_reward, 
                        min(self.config.max_reward, total_score))
            
            return float(reward)
            
        except Exception as e:
            logger.error(f"çå‹µè¨ˆç®—å¤±æ•—: {e}")
            return 0.0
    
    def _evaluate_accuracy(self, query: str, response: str) -> float:
        """è©•ä¼°åˆ†ææº–ç¢ºæ€§"""
        score = 0.5  # åŸºç¤åˆ†æ•¸
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«é‡‘èé—œéµè©
        financial_keyword_count = sum(1 for keyword in self.financial_keywords 
                                    if keyword in response)
        
        # æ ¹æ“šé—œéµè©æ•¸é‡çµ¦åˆ†
        if financial_keyword_count >= 5:
            score += 0.3
        elif financial_keyword_count >= 3:
            score += 0.2
        elif financial_keyword_count >= 1:
            score += 0.1
        
        # æª¢æŸ¥å›æ‡‰é•·åº¦åˆç†æ€§
        if 100 <= len(response) <= 800:
            score += 0.1
        elif len(response) < 50:
            score -= 0.2
        
        # æª¢æŸ¥æ˜¯å¦å›ç­”äº†å•é¡Œ
        if any(q_word in response for q_word in query.split() if len(q_word) > 2):
            score += 0.1
        
        return min(1.0, score)
    
    def _evaluate_risk_assessment(self, response: str) -> float:
        """è©•ä¼°é¢¨éšªè©•ä¼°å“è³ª"""
        score = 0.3  # åŸºç¤åˆ†æ•¸
        
        # æª¢æŸ¥é¢¨éšªç›¸é—œè©å½™
        risk_keyword_count = sum(1 for keyword in self.risk_keywords 
                               if keyword in response)
        
        if risk_keyword_count >= 2:
            score += 0.4
        elif risk_keyword_count >= 1:
            score += 0.2
        
        # æª¢æŸ¥æ˜¯å¦æœ‰é¢¨éšªè­¦å‘Š
        risk_phrases = ["æŠ•è³‡æœ‰é¢¨éšª", "è«‹è¬¹æ…è©•ä¼°", "å»ºè­°åˆ†æ•£æŠ•è³‡", "æ³¨æ„é¢¨æ§"]
        if any(phrase in response for phrase in risk_phrases):
            score += self.config.risk_warning_bonus
        
        # æ‡²ç½°éæ–¼çµ•å°çš„è¡¨è¿°
        absolute_phrases = ["100%", "çµ•å°", "è‚¯å®š", "ä¿è­‰"]
        if any(phrase in response for phrase in absolute_phrases):
            score -= 0.3
        
        return min(1.0, max(0.0, score))
    
    def _evaluate_recommendation(self, response: str) -> float:
        """è©•ä¼°æŠ•è³‡å»ºè­°åˆç†æ€§"""
        score = 0.4  # åŸºç¤åˆ†æ•¸
        
        # æª¢æŸ¥æ˜¯å¦æœ‰å…·é«”å»ºè­°
        recommendation_indicators = ["å»ºè­°", "æ¨è–¦", "å¯è€ƒæ…®", "é©åˆ", "ä¸é©åˆ"]
        if any(indicator in response for indicator in recommendation_indicators):
            score += 0.2
        
        # æª¢æŸ¥å»ºè­°çš„å…·é«”æ€§
        specific_terms = ["è²·é€²", "è³£å‡º", "æŒæœ‰", "è§€æœ›", "åŠ ç¢¼", "æ¸›ç¢¼"]
        if any(term in response for term in specific_terms):
            score += 0.2
        
        # æª¢æŸ¥æ˜¯å¦æœ‰ç†ç”±æ”¯æ’
        reasoning_indicators = ["å› ç‚º", "ç”±æ–¼", "åŸºæ–¼", "è€ƒé‡", "åˆ†æ"]
        if any(indicator in response for indicator in reasoning_indicators):
            score += 0.2
        
        return min(1.0, score)
    
    def _evaluate_language_quality(self, response: str) -> float:
        """è©•ä¼°èªè¨€å“è³ª"""
        score = 0.5  # åŸºç¤åˆ†æ•¸
        
        # æª¢æŸ¥èªå¥å®Œæ•´æ€§
        if response.count('ã€‚') >= 2:  # è‡³å°‘å…©å€‹å®Œæ•´å¥å­
            score += 0.2
        
        # æª¢æŸ¥å°ˆæ¥­è¡“èªä½¿ç”¨
        professional_terms = ["æŠ•è³‡çµ„åˆ", "è³‡ç”¢é…ç½®", "å¸‚å ´è¶¨å‹¢", "è²¡å‹™æŒ‡æ¨™"]
        professional_count = sum(1 for term in professional_terms if term in response)
        score += min(0.2, professional_count * 0.1)
        
        # æª¢æŸ¥é‚è¼¯é€£è²«æ€§ï¼ˆç°¡å–®æª¢æŸ¥ï¼‰
        connectors = ["ç„¶è€Œ", "å› æ­¤", "å¦å¤–", "æ­¤å¤–", "ç¶œåˆä¾†çœ‹"]
        if any(connector in response for connector in connectors):
            score += 0.1
        
        return min(1.0, score)
    
    def _apply_bonus_penalties(self, response: str, base_score: float) -> float:
        """æ‡‰ç”¨çå‹µå’Œæ‡²ç½°"""
        score = base_score
        
        # é‡‘èé—œéµè©çå‹µ
        financial_density = sum(1 for keyword in self.financial_keywords 
                              if keyword in response) / max(1, len(response.split()))
        if financial_density > 0.1:
            score += self.config.financial_keywords_bonus
        
        # è² é¢é—œéµè©æ‡²ç½°
        negative_count = sum(1 for keyword in self.negative_keywords 
                           if keyword in response)
        score -= negative_count * 0.2
        
        return score


class GRPOTrainer:
    """
    å®Œæ•´çš„GRPOè¨“ç·´å™¨å¯¦ç¾
    å°ˆç‚ºTradingAgentsé‡‘èAIè¨“ç·´å„ªåŒ–
    """
    
    def __init__(self, config: GRPOTrainingConfig, reward_config: FinancialRewardConfig):
        self.config = config
        self.reward_model = FinancialRewardModel(reward_config)
        
        # è¨­å‚™é…ç½®
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.gpu_device_id = 0
        
        # æ¨¡å‹å’Œè¨“ç·´å™¨
        self.model = None
        self.ref_model = None
        self.tokenizer = None
        self.ppo_trainer = None
        
        # è¨“ç·´ç‹€æ…‹
        self.current_epoch = 0
        self.global_step = 0
        self.best_reward = float('-inf')
        
        # çµ±è¨ˆä¿¡æ¯
        self.training_stats = {
            'total_episodes': 0,
            'average_reward': 0.0,
            'average_kl': 0.0,
            'training_loss': 0.0
        }
        
        logger.info("âœ… GRPOè¨“ç·´å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def setup_models(self):
        """è¨­ç½®æ¨¡å‹å’Œtokenizer"""
        logger.info(f"ğŸ”§ è¨­ç½®æ¨¡å‹: {self.config.model_name}")
        
        try:
            # è¨­ç½®4ä½é‡åŒ–é…ç½®ï¼ˆRTX 4070å„ªåŒ–ï¼‰
            if self.config.use_fp16:
                bnb_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_compute_dtype=torch.bfloat16
                )
            else:
                bnb_config = None
            
            # åˆå§‹åŒ–tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_name)
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
            
            # åˆå§‹åŒ–ä¸»æ¨¡å‹ï¼ˆå¸¶åƒ¹å€¼é ­ï¼‰
            self.model = AutoModelForCausalLMWithValueHead.from_pretrained(
                self.config.model_name,
                quantization_config=bnb_config,
                device_map="auto",
                torch_dtype=torch.bfloat16 if self.config.use_fp16 else torch.float32,
                low_cpu_mem_usage=True
            )
            
            # åˆå§‹åŒ–åƒè€ƒæ¨¡å‹
            self.ref_model = create_reference_model(self.model)
            
            # å•Ÿç”¨æ¢¯åº¦æª¢æŸ¥é»ï¼ˆç¯€çœå…§å­˜ï¼‰
            if self.config.gradient_checkpointing:
                self.model.gradient_checkpointing_enable()
            
            logger.info("âœ… æ¨¡å‹è¨­ç½®å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹è¨­ç½®å¤±æ•—: {e}")
            raise
    
    def setup_ppo_trainer(self):
        """è¨­ç½®PPOè¨“ç·´å™¨"""
        logger.info("ğŸ¯ è¨­ç½®PPOè¨“ç·´å™¨...")
        
        try:
            # PPOé…ç½®
            ppo_config = PPOConfig(
                model_name=self.config.model_name,
                learning_rate=self.config.learning_rate,
                log_with="wandb" if self.config.use_wandb else None,
                batch_size=self.config.batch_size,
                mini_batch_size=self.config.mini_batch_size,
                gradient_accumulation_steps=self.config.gradient_accumulation_steps,
                optimize_cuda_cache=True,
                early_stopping=True,
                target_kl=self.config.target_kl,
                ppo_epochs=self.config.ppo_epochs,
                seed=42,
                init_kl_coef=self.config.kl_beta,
                adap_kl_ctrl=self.config.adaptive_kl,
            )
            
            # åˆå§‹åŒ–PPOè¨“ç·´å™¨
            self.ppo_trainer = PPOTrainer(
                config=ppo_config,
                model=self.model,
                ref_model=self.ref_model,
                tokenizer=self.tokenizer,
            )
            
            logger.info("âœ… PPOè¨“ç·´å™¨è¨­ç½®å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ PPOè¨“ç·´å™¨è¨­ç½®å¤±æ•—: {e}")
            raise
    
    def prepare_dataset(self, dataset_path: str) -> Dataset:
        """æº–å‚™è¨“ç·´æ•¸æ“šé›†"""
        logger.info(f"ğŸ“Š æº–å‚™æ•¸æ“šé›†: {dataset_path}")
        
        try:
            # è®€å–JSONLæ•¸æ“š
            data = []
            with open(dataset_path, 'r', encoding='utf-8') as f:
                for line in f:
                    item = json.loads(line.strip())
                    if 'query' in item:  # GRPOæ ¼å¼
                        data.append({
                            'query': item['query'],
                            'context': item.get('context', ''),
                            'reference': item.get('reference', '')
                        })
                    elif 'instruction' in item:  # æ¨™æº–æ ¼å¼è½‰æ›
                        query = item['instruction']
                        if item.get('input'):
                            query += f" {item['input']}"
                        data.append({
                            'query': query,
                            'context': item.get('input', ''),
                            'reference': item.get('output', '')
                        })
            
            if not data:
                raise ValueError("æ•¸æ“šé›†ç‚ºç©ºæˆ–æ ¼å¼ä¸æ­£ç¢º")
            
            # éæ¿¾å’Œæ¸…ç†æ•¸æ“š
            filtered_data = []
            for item in data:
                if len(item['query']) > 10 and len(item['query']) < 1000:
                    filtered_data.append(item)
            
            logger.info(f"âœ… æ•¸æ“šé›†æº–å‚™å®Œæˆ: {len(filtered_data)} å€‹æ¨£æœ¬")
            
            return Dataset.from_list(filtered_data)
            
        except Exception as e:
            logger.error(f"âŒ æ•¸æ“šé›†æº–å‚™å¤±æ•—: {e}")
            raise
    
    def generate_response(self, query: str) -> str:
        """ç”Ÿæˆæ¨¡å‹å›æ‡‰"""
        try:
            # ç·¨ç¢¼è¼¸å…¥
            inputs = self.tokenizer.encode(query, return_tensors="pt").to(self.device)
            
            # ç”Ÿæˆå›æ‡‰
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    max_length=self.config.max_length,
                    num_return_sequences=1,
                    temperature=0.7,
                    do_sample=True,
                    top_p=0.9,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                )
            
            # è§£ç¢¼å›æ‡‰
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # ç§»é™¤è¼¸å…¥éƒ¨åˆ†ï¼Œåªä¿ç•™ç”Ÿæˆçš„å›æ‡‰
            if query in response:
                response = response.replace(query, "").strip()
            
            return response
            
        except Exception as e:
            logger.error(f"å›æ‡‰ç”Ÿæˆå¤±æ•—: {e}")
            return "æŠ±æ­‰ï¼Œæˆ‘ç„¡æ³•è™•ç†é€™å€‹å•é¡Œã€‚"
    
    def train_step(self, batch: Dict[str, Any]) -> Dict[str, float]:
        """åŸ·è¡Œä¸€æ­¥è¨“ç·´"""
        queries = batch['query']
        contexts = batch.get('context', [''] * len(queries))
        
        # ç”Ÿæˆå›æ‡‰
        responses = []
        for query in queries:
            response = self.generate_response(query)
            responses.append(response)
        
        # è¨ˆç®—çå‹µ
        rewards = []
        for query, response, context in zip(queries, responses, contexts):
            reward = self.reward_model.calculate_reward(query, response, {'context': context})
            rewards.append(reward)
        
        # è½‰æ›ç‚ºtensor
        reward_tensors = [torch.tensor(reward) for reward in rewards]
        
        # åŸ·è¡ŒPPOæ›´æ–°
        query_tensors = [self.tokenizer.encode(q, return_tensors="pt")[0] for q in queries]
        response_tensors = [self.tokenizer.encode(r, return_tensors="pt")[0] for r in responses]
        
        try:
            # PPOè¨“ç·´æ­¥é©Ÿ
            stats = self.ppo_trainer.step(query_tensors, response_tensors, reward_tensors)
            
            # æ›´æ–°çµ±è¨ˆä¿¡æ¯
            self.training_stats['average_reward'] = np.mean(rewards)
            self.training_stats['average_kl'] = stats.get('objective/kl', 0.0)
            self.training_stats['training_loss'] = stats.get('ppo/loss/total', 0.0)
            
            return {
                'reward': np.mean(rewards),
                'kl_divergence': stats.get('objective/kl', 0.0),
                'loss': stats.get('ppo/loss/total', 0.0)
            }
            
        except Exception as e:
            logger.error(f"PPOæ›´æ–°å¤±æ•—: {e}")
            return {'reward': 0.0, 'kl_divergence': 0.0, 'loss': float('inf')}
    
    def train(self, dataset_path: str, output_dir: str):
        """åŸ·è¡Œå®Œæ•´è¨“ç·´"""
        logger.info("ğŸš€ é–‹å§‹GRPOè¨“ç·´...")
        
        # è¨­ç½®æ¨¡å‹å’Œè¨“ç·´å™¨
        self.setup_models()
        self.setup_ppo_trainer()
        
        # æº–å‚™æ•¸æ“šé›†
        dataset = self.prepare_dataset(dataset_path)
        
        # å‰µå»ºè¼¸å‡ºç›®éŒ„
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–W&B
        if self.config.use_wandb:
            try:
                wandb.init(
                    project=self.config.wandb_project,
                    name=f"grpo-training-{datetime.now().strftime('%Y%m%d-%H%M%S')}",
                    config=self.config.__dict__
                )
            except Exception as e:
                logger.warning(f"W&Båˆå§‹åŒ–å¤±æ•—: {e}")
        
        # è¨“ç·´å¾ªç’°
        total_steps = len(dataset) * self.config.num_train_epochs // self.config.batch_size
        
        logger.info(f"ğŸ“ˆ é–‹å§‹è¨“ç·´: {self.config.num_train_epochs} epochs, {total_steps} steps")
        
        for epoch in range(self.config.num_train_epochs):
            self.current_epoch = epoch
            epoch_rewards = []
            epoch_losses = []
            
            # éš¨æ©Ÿæ‰“äº‚æ•¸æ“š
            shuffled_indices = list(range(len(dataset)))
            random.shuffle(shuffled_indices)
            
            # æ‰¹æ¬¡è¨“ç·´
            for i in tqdm(range(0, len(dataset), self.config.batch_size), 
                         desc=f"Epoch {epoch+1}/{self.config.num_train_epochs}"):
                
                # æº–å‚™æ‰¹æ¬¡æ•¸æ“š
                batch_indices = shuffled_indices[i:i+self.config.batch_size]
                batch = {
                    'query': [dataset[idx]['query'] for idx in batch_indices],
                    'context': [dataset[idx].get('context', '') for idx in batch_indices]
                }
                
                # åŸ·è¡Œè¨“ç·´æ­¥é©Ÿ
                step_stats = self.train_step(batch)
                
                epoch_rewards.append(step_stats['reward'])
                epoch_losses.append(step_stats['loss'])
                
                self.global_step += 1
                
                # è¨˜éŒ„æ—¥èªŒ
                if self.global_step % self.config.log_freq == 0:
                    avg_reward = np.mean(epoch_rewards[-10:])  # æœ€è¿‘10æ­¥å¹³å‡
                    avg_loss = np.mean(epoch_losses[-10:])
                    
                    logger.info(f"Step {self.global_step}: Reward={avg_reward:.4f}, Loss={avg_loss:.4f}")
                    
                    # W&Bè¨˜éŒ„
                    if self.config.use_wandb and wandb.run:
                        wandb.log({
                            'reward': avg_reward,
                            'loss': avg_loss,
                            'kl_divergence': step_stats['kl_divergence'],
                            'epoch': epoch,
                            'step': self.global_step
                        })
                
                # å®šæœŸä¿å­˜æ¨¡å‹
                if self.global_step % self.config.save_freq == 0:
                    checkpoint_dir = output_path / f"checkpoint-{self.global_step}"
                    self.save_model(str(checkpoint_dir))
                
                # å…§å­˜æ¸…ç†
                if self.global_step % 50 == 0:
                    torch.cuda.empty_cache()
            
            # EpochçµæŸçµ±è¨ˆ
            epoch_avg_reward = np.mean(epoch_rewards)
            epoch_avg_loss = np.mean(epoch_losses)
            
            logger.info(f"âœ… Epoch {epoch+1} å®Œæˆ: "
                       f"å¹³å‡çå‹µ={epoch_avg_reward:.4f}, "
                       f"å¹³å‡æå¤±={epoch_avg_loss:.4f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if epoch_avg_reward > self.best_reward:
                self.best_reward = epoch_avg_reward
                best_model_path = output_path / "best_model"
                self.save_model(str(best_model_path))
                logger.info(f"ğŸ’ æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {best_model_path}")
        
        # ä¿å­˜æœ€çµ‚æ¨¡å‹
        final_model_path = output_path / "final_model"
        self.save_model(str(final_model_path))
        
        logger.info("ğŸ‰ GRPOè¨“ç·´å®Œæˆï¼")
        
        # é—œé–‰W&B
        if self.config.use_wandb and wandb.run:
            wandb.finish()
    
    def save_model(self, output_dir: str):
        """ä¿å­˜æ¨¡å‹"""
        try:
            Path(output_dir).mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜æ¨¡å‹å’Œtokenizer
            self.model.save_pretrained(output_dir)
            self.tokenizer.save_pretrained(output_dir)
            
            # ä¿å­˜è¨“ç·´é…ç½®
            with open(f"{output_dir}/training_config.json", 'w', encoding='utf-8') as f:
                json.dump(self.config.__dict__, f, indent=2, ensure_ascii=False)
            
            # ä¿å­˜è¨“ç·´çµ±è¨ˆ
            with open(f"{output_dir}/training_stats.json", 'w', encoding='utf-8') as f:
                stats = self.training_stats.copy()
                stats.update({
                    'epoch': self.current_epoch,
                    'global_step': self.global_step,
                    'best_reward': self.best_reward
                })
                json.dump(stats, f, indent=2, ensure_ascii=False)
            
            logger.info(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {output_dir}")
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹ä¿å­˜å¤±æ•—: {e}")


def create_demo_dataset(output_path: str, num_samples: int = 100):
    """å‰µå»ºç¤ºä¾‹è¨“ç·´æ•¸æ“šé›†"""
    logger.info(f"ğŸ“ å‰µå»ºç¤ºä¾‹æ•¸æ“šé›†: {num_samples} å€‹æ¨£æœ¬")
    
    demo_queries = [
        "åˆ†æå°ç©é›»çš„æŠ•è³‡åƒ¹å€¼",
        "å¦‚ä½•è©•ä¼°é´»æµ·çš„è²¡å‹™ç‹€æ³",
        "å°è‚¡ç›®å‰çš„è¶¨å‹¢å¦‚ä½•",
        "ä»€éº¼æ˜¯æœ¬ç›Šæ¯”",
        "å¦‚ä½•åˆ†æ•£æŠ•è³‡é¢¨éšª",
        "è¯é›»å’Œå°ç©é›»å“ªå€‹æ›´é©åˆæŠ•è³‡",
        "åŠå°é«”ç”¢æ¥­çš„æœªä¾†å±•æœ›",
        "å¦‚ä½•åˆ¤æ–·è‚¡ç¥¨çš„è²·è³£æ™‚æ©Ÿ",
        "ä»€éº¼æ˜¯æŠ€è¡“åˆ†æ",
        "åŸºæœ¬é¢åˆ†æåŒ…å«å“ªäº›è¦ç´ "
    ]
    
    # ç”Ÿæˆè¨“ç·´æ¨£æœ¬
    samples = []
    for i in range(num_samples):
        query = random.choice(demo_queries)
        if i % 10 == 0:  # æ·»åŠ ä¸€äº›è®ŠåŒ–
            query = f"è«‹åˆ†æ {query}"
        
        sample = {
            "query": query,
            "context": "å°è‚¡æŠ•è³‡åˆ†æ",
            "reference": "é€™éœ€è¦ç¶œåˆè€ƒæ…®å¤šå€‹å› ç´ ï¼ŒåŒ…æ‹¬è²¡å‹™æ•¸æ“šã€å¸‚å ´è¶¨å‹¢å’Œé¢¨éšªè©•ä¼°ã€‚"
        }
        samples.append(sample)
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    with open(output_path, 'w', encoding='utf-8') as f:
        for sample in samples:
            f.write(json.dumps(sample, ensure_ascii=False) + '\n')
    
    logger.info(f"âœ… ç¤ºä¾‹æ•¸æ“šé›†å·²å‰µå»º: {output_path}")


def main():
    """ä¸»å‡½æ•¸ - ç‚ºå°kæä¾›çš„è¨“ç·´å…¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(description='GRPOè¨“ç·´å™¨ - æ”¯æ´AIè¨“ç·´å°ˆå®¶åœ˜éšŠ(å°k)')
    parser.add_argument('--dataset', type=str, required=True, help='è¨“ç·´æ•¸æ“šé›†è·¯å¾‘')
    parser.add_argument('--output', type=str, required=True, help='æ¨¡å‹è¼¸å‡ºè·¯å¾‘')
    parser.add_argument('--create-demo', action='store_true', help='å‰µå»ºç¤ºä¾‹æ•¸æ“šé›†')
    parser.add_argument('--demo-samples', type=int, default=100, help='ç¤ºä¾‹æ•¸æ“šé›†æ¨£æœ¬æ•¸')
    parser.add_argument('--config', type=str, help='é…ç½®æ–‡ä»¶è·¯å¾‘')
    
    args = parser.parse_args()
    
    try:
        # å‰µå»ºç¤ºä¾‹æ•¸æ“šé›†ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if args.create_demo:
            create_demo_dataset(args.dataset, args.demo_samples)
            logger.info("ç¤ºä¾‹æ•¸æ“šé›†å‰µå»ºå®Œæˆï¼Œå¯ä»¥é–‹å§‹è¨“ç·´äº†ï¼")
            return
        
        # åŠ è¼‰é…ç½®
        if args.config and Path(args.config).exists():
            with open(args.config, 'r', encoding='utf-8') as f:
                config_dict = json.load(f)
            training_config = GRPOTrainingConfig(**config_dict)
        else:
            training_config = GRPOTrainingConfig()
        
        reward_config = FinancialRewardConfig()
        
        # å‰µå»ºè¨“ç·´å™¨
        trainer = GRPOTrainer(training_config, reward_config)
        
        # é–‹å§‹è¨“ç·´
        trainer.train(args.dataset, args.output)
        
        logger.info("ğŸ‰ GRPOè¨“ç·´æˆåŠŸå®Œæˆï¼")
        
    except KeyboardInterrupt:
        logger.info("ğŸ‘¤ è¨“ç·´è¢«ç”¨æˆ¶ä¸­æ–·")
    except Exception as e:
        logger.error(f"âŒ è¨“ç·´å¤±æ•—: {e}")
        raise



# è‡ªå‹•æª¢æ¸¬å’Œåˆ‡æ›åˆ° TradingAgents ç›®éŒ„
def ensure_tradingagents_directory():
    """ç¢ºä¿ç•¶å‰å·¥ä½œç›®éŒ„åœ¨ TradingAgents/ ä¸‹ï¼Œä»¥æ­£ç¢ºè¨ªå•é…ç½®æ–‡ä»¶"""
    current_dir = Path.cwd()
    
    # å¦‚æœç•¶å‰ç›®éŒ„æ˜¯ TradingAgents çš„çˆ¶ç›®éŒ„ï¼Œåˆ‡æ›åˆ° TradingAgents
    if (current_dir / 'TradingAgents').exists():
        os.chdir(current_dir / 'TradingAgents')
        print(f"[DIR] è‡ªå‹•åˆ‡æ›å·¥ä½œç›®éŒ„åˆ°: {Path.cwd()}")
    
    # é©—è­‰å¿…è¦çš„ç›®éŒ„å­˜åœ¨
    required_dirs = ['configs', 'training', 'tradingagents']
    missing_dirs = [d for d in required_dirs if not Path(d).exists()]
    
    if missing_dirs:
        raise FileNotFoundError(f"ç¼ºå°‘å¿…è¦ç›®éŒ„: {missing_dirs}. è«‹ç¢ºä¿å¾ TradingAgents/ ç›®éŒ„åŸ·è¡Œæ­¤è…³æœ¬")

# ç›®éŒ„æª¢æŸ¥å‡½æ•¸å·²æº–å‚™å¥½ï¼Œä½†ä¸åœ¨æ¨¡çµ„å°å…¥æ™‚è‡ªå‹•åŸ·è¡Œ
# åªåœ¨éœ€è¦æ™‚æ‰‹å‹•èª¿ç”¨ ensure_tradingagents_directory()

if __name__ == "__main__":
    # ç¢ºä¿æ—¥èªŒç›®éŒ„å­˜åœ¨
    os.makedirs('/app/logs/training', exist_ok=True)
    
    # é‹è¡Œè¨“ç·´
    main()