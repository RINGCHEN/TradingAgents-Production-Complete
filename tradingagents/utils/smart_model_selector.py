#!/usr/bin/env python3
"""
Smart Model Selector - æ™ºèƒ½æ¨¡å‹é¸æ“‡å™¨
å¤©å·¥ (TianGong) - åŸºæ–¼AIçš„å‹•æ…‹æ¨¡å‹é¸æ“‡å’Œæ€§èƒ½å„ªåŒ–ç³»çµ±

æ­¤æ¨¡çµ„è² è²¬ï¼š
1. å‹•æ…‹æ¨¡å‹é¸æ“‡ç­–ç•¥
2. æ€§èƒ½å­¸ç¿’å’Œå„ªåŒ–
3. A/Bæ¸¬è©¦æ•´åˆ
4. å€‹æ€§åŒ–æ¨¡å‹æ¨è–¦
"""

import asyncio
import json
import logging
import time
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass, asdict
from enum import Enum
import statistics
import secrets  # å®‰å…¨ä¿®å¾©ï¼šä½¿ç”¨åŠ å¯†å®‰å…¨çš„éš¨æ©Ÿæ•¸ç”Ÿæˆå™¨æ›¿æ›random

from .llm_cost_optimizer import LLMCostOptimizer, ModelConfig, TaskComplexity, UsageRecord

class SelectionStrategy(Enum):
    """é¸æ“‡ç­–ç•¥"""
    COST_FIRST = "cost_first"           # æˆæœ¬å„ªå…ˆ
    QUALITY_FIRST = "quality_first"     # è³ªé‡å„ªå…ˆ
    BALANCED = "balanced"               # å¹³è¡¡é¸æ“‡
    SPEED_FIRST = "speed_first"         # é€Ÿåº¦å„ªå…ˆ
    ADAPTIVE = "adaptive"               # è‡ªé©æ‡‰
    AB_TEST = "ab_test"                 # A/Bæ¸¬è©¦

class PerformanceMetric(Enum):
    """æ€§èƒ½æŒ‡æ¨™"""
    COST_EFFICIENCY = "cost_efficiency"
    QUALITY_SCORE = "quality_score"
    RESPONSE_TIME = "response_time"
    SUCCESS_RATE = "success_rate"
    USER_SATISFACTION = "user_satisfaction"

@dataclass
class ModelPerformance:
    """æ¨¡å‹æ€§èƒ½è¨˜éŒ„"""
    model_name: str
    task_type: str
    user_segment: str
    avg_cost: float
    avg_quality: float
    avg_response_time: float
    success_rate: float
    sample_size: int
    last_updated: str
    confidence_score: float = 0.0

@dataclass
class ABTestConfig:
    """A/Bæ¸¬è©¦é…ç½®"""
    test_id: str
    name: str
    model_a: str
    model_b: str
    traffic_split: float  # Açµ„æµé‡æ¯”ä¾‹ (0.0-1.0)
    target_metric: PerformanceMetric
    minimum_samples: int
    start_date: str
    end_date: str
    is_active: bool = True

@dataclass
class UserPreference:
    """ç”¨æˆ¶åå¥½"""
    user_id: str
    preferred_strategy: SelectionStrategy
    quality_tolerance: float    # å¯æ¥å—çš„æœ€ä½è³ªé‡
    cost_sensitivity: float     # æˆæœ¬æ•æ„Ÿåº¦ (0-1)
    speed_importance: float     # é€Ÿåº¦é‡è¦æ€§ (0-1)
    learned_from_feedback: bool = False

class SmartModelSelector:
    """æ™ºèƒ½æ¨¡å‹é¸æ“‡å™¨"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.cost_optimizer = LLMCostOptimizer()
        
        # æ€§èƒ½è¿½è¹¤
        self.model_performances: Dict[str, ModelPerformance] = {}
        self.user_preferences: Dict[str, UserPreference] = {}
        
        # A/Bæ¸¬è©¦
        self.active_ab_tests: Dict[str, ABTestConfig] = {}
        self.ab_test_assignments: Dict[str, str] = {}  # user_id -> test_group
        
        # å­¸ç¿’åƒæ•¸
        self.learning_rate = 0.1
        self.min_samples_for_learning = 10
        
        # è¨­ç½®æ—¥èªŒ
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
            self.logger.setLevel(logging.INFO)
    
    async def select_model_intelligently(
        self,
        user_context: Dict[str, Any],
        task_type: str,
        estimated_tokens: int = 1000,
        force_strategy: SelectionStrategy = None
    ) -> Tuple[ModelConfig, Dict[str, Any]]:
        """æ™ºèƒ½é¸æ“‡æ¨¡å‹"""
        
        user_id = user_context.get('user_id', 'anonymous')
        
        # ç²å–ç”¨æˆ¶åå¥½
        user_preference = self._get_user_preference(user_id)
        strategy = force_strategy or user_preference.preferred_strategy
        
        # æª¢æŸ¥æ˜¯å¦åœ¨A/Bæ¸¬è©¦ä¸­
        ab_test_result = await self._check_ab_test_assignment(user_id, task_type)
        if ab_test_result:
            return ab_test_result
        
        # æ ¹æ“šç­–ç•¥é¸æ“‡æ¨¡å‹
        if strategy == SelectionStrategy.ADAPTIVE:
            return await self._adaptive_selection(user_context, task_type, estimated_tokens)
        elif strategy == SelectionStrategy.COST_FIRST:
            return await self._cost_first_selection(user_context, task_type, estimated_tokens)
        elif strategy == SelectionStrategy.QUALITY_FIRST:
            return await self._quality_first_selection(user_context, task_type, estimated_tokens)
        elif strategy == SelectionStrategy.SPEED_FIRST:
            return await self._speed_first_selection(user_context, task_type, estimated_tokens)
        elif strategy == SelectionStrategy.BALANCED:
            return await self._balanced_selection(user_context, task_type, estimated_tokens)
        else:
            # é è¨­ä½¿ç”¨æˆæœ¬å„ªåŒ–å™¨
            complexity = self._infer_task_complexity(task_type)
            return await self.cost_optimizer.select_optimal_model(
                complexity, user_context, estimated_tokens
            )
    
    def _get_user_preference(self, user_id: str) -> UserPreference:
        """ç²å–ç”¨æˆ¶åå¥½"""
        if user_id not in self.user_preferences:
            # ç‚ºæ–°ç”¨æˆ¶å‰µå»ºé è¨­åå¥½
            self.user_preferences[user_id] = UserPreference(
                user_id=user_id,
                preferred_strategy=SelectionStrategy.ADAPTIVE,
                quality_tolerance=7.0,
                cost_sensitivity=0.5,
                speed_importance=0.3
            )
        
        return self.user_preferences[user_id]
    
    async def _check_ab_test_assignment(
        self, 
        user_id: str, 
        task_type: str
    ) -> Optional[Tuple[ModelConfig, Dict[str, Any]]]:
        """æª¢æŸ¥A/Bæ¸¬è©¦åˆ†é…"""
        
        # æŸ¥æ‰¾é©ç”¨çš„A/Bæ¸¬è©¦
        applicable_tests = []
        for test_id, test_config in self.active_ab_tests.items():
            if (test_config.is_active and 
                self._is_test_applicable(test_config, task_type)):
                applicable_tests.append((test_id, test_config))
        
        if not applicable_tests:
            return None
        
        # é¸æ“‡æœ€é«˜å„ªå…ˆç´šçš„æ¸¬è©¦ï¼ˆé€™è£¡ç°¡åŒ–ç‚ºç¬¬ä¸€å€‹ï¼‰
        test_id, test_config = applicable_tests[0]
        
        # æª¢æŸ¥ç”¨æˆ¶æ˜¯å¦å·²åˆ†é…åˆ°çµ„åˆ¥
        assignment_key = f"{user_id}_{test_id}"
        if assignment_key not in self.ab_test_assignments:
            # æ–°ç”¨æˆ¶ï¼Œé€²è¡Œåˆ†é…
            is_group_a = (secrets.randbelow(10000) / 10000.0) < test_config.traffic_split  # å®‰å…¨ä¿®å¾©ï¼šä½¿ç”¨secretsæ›¿æ›random
            self.ab_test_assignments[assignment_key] = "A" if is_group_a else "B"
        
        group = self.ab_test_assignments[assignment_key]
        model_name = test_config.model_a if group == "A" else test_config.model_b
        
        # ç²å–æ¨¡å‹é…ç½®
        model_config = self.cost_optimizer.model_configs.get(model_name)
        if not model_config:
            self.logger.warning(f"A/Bæ¸¬è©¦æ¨¡å‹ä¸å­˜åœ¨: {model_name}")
            return None
        
        return model_config, {
            "selection_reason": "ab_test",
            "test_id": test_id,
            "test_group": group,
            "model_name": model_name,
            "target_metric": test_config.target_metric.value
        }
    
    def _is_test_applicable(self, test_config: ABTestConfig, task_type: str) -> bool:
        """æª¢æŸ¥A/Bæ¸¬è©¦æ˜¯å¦é©ç”¨"""
        # ç°¡åŒ–å¯¦ç¾ï¼šæ‰€æœ‰æ¸¬è©¦éƒ½é©ç”¨æ–¼æ‰€æœ‰ä»»å‹™
        # å¯¦éš›å¯¦ç¾å¯ä»¥æ ¹æ“šä»»å‹™é¡å‹ã€ç”¨æˆ¶ç‰¹å¾µç­‰é€²è¡Œéæ¿¾
        
        current_date = datetime.now().strftime('%Y-%m-%d')
        return (test_config.start_date <= current_date <= test_config.end_date)
    
    async def _adaptive_selection(
        self, 
        user_context: Dict[str, Any], 
        task_type: str, 
        estimated_tokens: int
    ) -> Tuple[ModelConfig, Dict[str, Any]]:
        """è‡ªé©æ‡‰é¸æ“‡"""
        
        user_id = user_context.get('user_id', 'anonymous')
        user_preference = self._get_user_preference(user_id)
        
        # ç²å–æ­·å²æ€§èƒ½æ•¸æ“š
        performance_data = self._get_task_performance_data(task_type, user_id)
        
        if not performance_data:
            # æ²’æœ‰æ­·å²æ•¸æ“šï¼Œä½¿ç”¨å¹³è¡¡ç­–ç•¥
            return await self._balanced_selection(user_context, task_type, estimated_tokens)
        
        # æ ¹æ“šæ­·å²æ€§èƒ½å’Œç”¨æˆ¶åå¥½è¨ˆç®—æ¨¡å‹è©•åˆ†
        model_scores = {}
        
        for model_name, performance in performance_data.items():
            if performance.sample_size < 3:  # æ¨£æœ¬å¤ªå°‘ï¼Œé™ä½æ¬Šé‡
                confidence_penalty = 0.5
            else:
                confidence_penalty = 1.0
            
            # ç¶œåˆè©•åˆ†è¨ˆç®—
            cost_score = max(0, (0.1 - performance.avg_cost) / 0.1)  # æˆæœ¬è¶Šä½è¶Šå¥½
            quality_score = performance.avg_quality / 10.0  # è³ªé‡è©•åˆ†
            speed_score = max(0, (5000 - performance.avg_response_time) / 5000)  # é€Ÿåº¦è©•åˆ†
            
            # æ ¹æ“šç”¨æˆ¶åå¥½åŠ æ¬Š
            composite_score = (
                cost_score * user_preference.cost_sensitivity +
                quality_score * (1 - user_preference.cost_sensitivity) * 0.7 +
                speed_score * user_preference.speed_importance * 0.3
            ) * confidence_penalty
            
            model_scores[model_name] = composite_score
        
        # é¸æ“‡è©•åˆ†æœ€é«˜çš„æ¨¡å‹
        best_model_name = max(model_scores, key=model_scores.get)
        best_model_config = self.cost_optimizer.model_configs.get(best_model_name)
        
        if not best_model_config:
            # å¾Œå‚™æ–¹æ¡ˆ
            return await self._balanced_selection(user_context, task_type, estimated_tokens)
        
        return best_model_config, {
            "selection_reason": "adaptive_learning",
            "composite_score": model_scores[best_model_name],
            "performance_based": True,
            "sample_size": performance_data[best_model_name].sample_size
        }
    
    async def _cost_first_selection(
        self, 
        user_context: Dict[str, Any], 
        task_type: str, 
        estimated_tokens: int
    ) -> Tuple[ModelConfig, Dict[str, Any]]:
        """æˆæœ¬å„ªå…ˆé¸æ“‡"""
        
        membership_tier = user_context.get('membership_tier', 'FREE')
        
        # ç²å–æ‰€æœ‰å¯ç”¨æ¨¡å‹ä¸¦æŒ‰æˆæœ¬æ’åº
        available_models = []
        
        for model_name, config in self.cost_optimizer.model_configs.items():
            if self.cost_optimizer._is_model_allowed_for_tier(config, membership_tier):
                estimated_cost = self.cost_optimizer._calculate_estimated_cost(config, estimated_tokens)
                available_models.append((config, estimated_cost))
        
        # æŒ‰æˆæœ¬æ’åºï¼Œé¸æ“‡æœ€ä¾¿å®œçš„
        available_models.sort(key=lambda x: x[1])
        
        if not available_models:
            # å¾Œå‚™æ–¹æ¡ˆ
            complexity = self._infer_task_complexity(task_type)
            return await self.cost_optimizer.select_optimal_model(
                complexity, user_context, estimated_tokens
            )
        
        best_model, best_cost = available_models[0]
        
        return best_model, {
            "selection_reason": "cost_optimization",
            "estimated_cost": best_cost,
            "cost_rank": 1,
            "alternatives": len(available_models) - 1
        }
    
    async def _quality_first_selection(
        self, 
        user_context: Dict[str, Any], 
        task_type: str, 
        estimated_tokens: int
    ) -> Tuple[ModelConfig, Dict[str, Any]]:
        """è³ªé‡å„ªå…ˆé¸æ“‡"""
        
        membership_tier = user_context.get('membership_tier', 'FREE')
        
        # ç²å–æ‰€æœ‰å¯ç”¨æ¨¡å‹ä¸¦æŒ‰è³ªé‡æ’åº
        available_models = []
        
        for model_name, config in self.cost_optimizer.model_configs.items():
            if self.cost_optimizer._is_model_allowed_for_tier(config, membership_tier):
                available_models.append(config)
        
        # æŒ‰è³ªé‡æ’åºï¼Œé¸æ“‡è³ªé‡æœ€é«˜çš„
        available_models.sort(key=lambda x: x.quality_score, reverse=True)
        
        if not available_models:
            # å¾Œå‚™æ–¹æ¡ˆ
            complexity = self._infer_task_complexity(task_type)
            return await self.cost_optimizer.select_optimal_model(
                complexity, user_context, estimated_tokens
            )
        
        best_model = available_models[0]
        estimated_cost = self.cost_optimizer._calculate_estimated_cost(best_model, estimated_tokens)
        
        return best_model, {
            "selection_reason": "quality_optimization",
            "quality_score": best_model.quality_score,
            "estimated_cost": estimated_cost,
            "quality_rank": 1
        }
    
    async def _speed_first_selection(
        self, 
        user_context: Dict[str, Any], 
        task_type: str, 
        estimated_tokens: int
    ) -> Tuple[ModelConfig, Dict[str, Any]]:
        """é€Ÿåº¦å„ªå…ˆé¸æ“‡"""
        
        membership_tier = user_context.get('membership_tier', 'FREE')
        
        # ç²å–æ‰€æœ‰å¯ç”¨æ¨¡å‹ä¸¦æŒ‰é€Ÿåº¦æ’åº
        available_models = []
        
        for model_name, config in self.cost_optimizer.model_configs.items():
            if self.cost_optimizer._is_model_allowed_for_tier(config, membership_tier):
                available_models.append(config)
        
        # æŒ‰é€Ÿåº¦æ’åºï¼Œé¸æ“‡é€Ÿåº¦æœ€å¿«çš„
        available_models.sort(key=lambda x: x.speed_score, reverse=True)
        
        if not available_models:
            # å¾Œå‚™æ–¹æ¡ˆ
            complexity = self._infer_task_complexity(task_type)
            return await self.cost_optimizer.select_optimal_model(
                complexity, user_context, estimated_tokens
            )
        
        best_model = available_models[0]
        estimated_cost = self.cost_optimizer._calculate_estimated_cost(best_model, estimated_tokens)
        
        return best_model, {
            "selection_reason": "speed_optimization",
            "speed_score": best_model.speed_score,
            "estimated_cost": estimated_cost,
            "speed_rank": 1
        }
    
    async def _balanced_selection(
        self, 
        user_context: Dict[str, Any], 
        task_type: str, 
        estimated_tokens: int
    ) -> Tuple[ModelConfig, Dict[str, Any]]:
        """å¹³è¡¡é¸æ“‡"""
        
        # ä½¿ç”¨æˆæœ¬å„ªåŒ–å™¨çš„é è¨­é‚è¼¯ï¼Œé€™å·²ç¶“æ˜¯ä¸€å€‹å¹³è¡¡çš„é¸æ“‡
        complexity = self._infer_task_complexity(task_type)
        return await self.cost_optimizer.select_optimal_model(
            complexity, user_context, estimated_tokens
        )
    
    def _infer_task_complexity(self, task_type: str) -> TaskComplexity:
        """æ¨æ–·ä»»å‹™è¤‡é›œåº¦"""
        complexity_mapping = {
            "basic_analysis": TaskComplexity.SIMPLE,
            "quick_summary": TaskComplexity.SIMPLE,
            "technical_analysis": TaskComplexity.MODERATE,
            "fundamental_analysis": TaskComplexity.MODERATE,
            "news_analysis": TaskComplexity.MODERATE,
            "comprehensive_analysis": TaskComplexity.COMPLEX,
            "portfolio_analysis": TaskComplexity.COMPLEX,
            "investment_advice": TaskComplexity.CRITICAL,
            "risk_assessment": TaskComplexity.CRITICAL
        }
        
        return complexity_mapping.get(task_type, TaskComplexity.MODERATE)
    
    def _get_task_performance_data(self, task_type: str, user_segment: str) -> Dict[str, ModelPerformance]:
        """ç²å–ä»»å‹™æ€§èƒ½æ•¸æ“š"""
        performance_data = {}
        
        for key, performance in self.model_performances.items():
            if (performance.task_type == task_type and 
                (performance.user_segment == user_segment or performance.user_segment == "all")):
                performance_data[performance.model_name] = performance
        
        return performance_data
    
    async def learn_from_feedback(
        self,
        user_id: str,
        model_name: str,
        task_type: str,
        actual_cost: float,
        actual_quality: float,
        actual_response_time: float,
        success: bool,
        user_rating: Optional[float] = None
    ):
        """å¾åé¥‹ä¸­å­¸ç¿’"""
        
        # æ›´æ–°æ¨¡å‹æ€§èƒ½è¨˜éŒ„
        performance_key = f"{model_name}_{task_type}_all"  # æš«æ™‚ä½¿ç”¨"all"ä½œç‚ºç”¨æˆ¶æ®µ
        
        if performance_key not in self.model_performances:
            self.model_performances[performance_key] = ModelPerformance(
                model_name=model_name,
                task_type=task_type,
                user_segment="all",
                avg_cost=actual_cost,
                avg_quality=actual_quality,
                avg_response_time=actual_response_time,
                success_rate=1.0 if success else 0.0,
                sample_size=1,
                last_updated=datetime.now().isoformat()
            )
        else:
            # æ›´æ–°ç¾æœ‰è¨˜éŒ„ï¼ˆä½¿ç”¨ç§»å‹•å¹³å‡ï¼‰
            perf = self.model_performances[performance_key]
            
            # è¨ˆç®—æ–°çš„ç§»å‹•å¹³å‡
            alpha = self.learning_rate
            perf.avg_cost = (1 - alpha) * perf.avg_cost + alpha * actual_cost
            perf.avg_quality = (1 - alpha) * perf.avg_quality + alpha * actual_quality
            perf.avg_response_time = (1 - alpha) * perf.avg_response_time + alpha * actual_response_time
            
            # æ›´æ–°æˆåŠŸç‡
            total_attempts = perf.sample_size + 1
            current_successes = perf.success_rate * perf.sample_size
            new_successes = current_successes + (1 if success else 0)
            perf.success_rate = new_successes / total_attempts
            
            perf.sample_size += 1
            perf.last_updated = datetime.now().isoformat()
            
            # è¨ˆç®—ä¿¡å¿ƒåº¦
            perf.confidence_score = min(1.0, perf.sample_size / 50.0)  # 50å€‹æ¨£æœ¬é”åˆ°æœ€é«˜ä¿¡å¿ƒåº¦
        
        # æ›´æ–°ç”¨æˆ¶åå¥½
        if user_rating is not None:
            await self._update_user_preference(user_id, model_name, user_rating, actual_cost)
        
        self.logger.info(f"å­¸ç¿’æ›´æ–°: {model_name} - {task_type} - æ¨£æœ¬æ•¸: {self.model_performances[performance_key].sample_size}")
    
    async def _update_user_preference(self, user_id: str, model_name: str, rating: float, cost: float):
        """æ›´æ–°ç”¨æˆ¶åå¥½"""
        user_pref = self._get_user_preference(user_id)
        
        # æ ¹æ“šç”¨æˆ¶è©•åˆ†å’Œæˆæœ¬èª¿æ•´åå¥½
        if rating >= 8.0:  # é«˜åˆ†
            if cost < 0.01:  # ä½æˆæœ¬
                # ç”¨æˆ¶å–œæ­¡ä½æˆæœ¬é«˜è³ªé‡ï¼Œæé«˜æˆæœ¬æ•æ„Ÿåº¦
                user_pref.cost_sensitivity = min(1.0, user_pref.cost_sensitivity + 0.1)
            else:  # é«˜æˆæœ¬
                # ç”¨æˆ¶ç‚ºè³ªé‡é¡˜æ„ä»˜è²»ï¼Œé™ä½æˆæœ¬æ•æ„Ÿåº¦
                user_pref.cost_sensitivity = max(0.0, user_pref.cost_sensitivity - 0.1)
        elif rating <= 5.0:  # ä½åˆ†
            # ç”¨æˆ¶ä¸æ»¿æ„ï¼Œèª¿æ•´åå¥½
            model_config = self.cost_optimizer.model_configs.get(model_name)
            if model_config and model_config.quality_score < 8.0:
                # ä½è³ªé‡æ¨¡å‹å¾—ä½åˆ†ï¼Œæé«˜è³ªé‡è¦æ±‚
                user_pref.quality_tolerance = min(10.0, user_pref.quality_tolerance + 0.5)
        
        user_pref.learned_from_feedback = True
        
        self.logger.debug(f"æ›´æ–°ç”¨æˆ¶åå¥½: {user_id} - æˆæœ¬æ•æ„Ÿåº¦: {user_pref.cost_sensitivity:.2f}")
    
    def create_ab_test(
        self,
        test_name: str,
        model_a: str,
        model_b: str,
        target_metric: PerformanceMetric,
        duration_days: int = 7,
        traffic_split: float = 0.5
    ) -> str:
        """å‰µå»ºA/Bæ¸¬è©¦"""
        
        test_id = f"ab_test_{int(time.time())}"
        start_date = datetime.now().strftime('%Y-%m-%d')
        end_date = (datetime.now() + timedelta(days=duration_days)).strftime('%Y-%m-%d')
        
        test_config = ABTestConfig(
            test_id=test_id,
            name=test_name,
            model_a=model_a,
            model_b=model_b,
            traffic_split=traffic_split,
            target_metric=target_metric,
            minimum_samples=50,
            start_date=start_date,
            end_date=end_date,
            is_active=True
        )
        
        self.active_ab_tests[test_id] = test_config
        
        self.logger.info(f"å‰µå»ºA/Bæ¸¬è©¦: {test_name} ({model_a} vs {model_b})")
        
        return test_id
    
    def get_ab_test_results(self, test_id: str) -> Optional[Dict[str, Any]]:
        """ç²å–A/Bæ¸¬è©¦çµæœ"""
        
        if test_id not in self.active_ab_tests:
            return None
        
        test_config = self.active_ab_tests[test_id]
        
        # çµ±è¨ˆå„çµ„çµæœ
        group_a_results = []
        group_b_results = []
        
        for assignment_key, group in self.ab_test_assignments.items():
            if assignment_key.endswith(f"_{test_id}"):
                user_id = assignment_key.replace(f"_{test_id}", "")
                
                # æŸ¥æ‰¾è©²ç”¨æˆ¶åœ¨æ¸¬è©¦æœŸé–“çš„ä½¿ç”¨è¨˜éŒ„
                user_records = [
                    r for r in self.cost_optimizer.usage_records
                    if (r.user_id == user_id and
                        test_config.start_date <= r.timestamp[:10] <= test_config.end_date)
                ]
                
                for record in user_records:
                    if group == "A" and record.model_name == test_config.model_a:
                        group_a_results.append(record)
                    elif group == "B" and record.model_name == test_config.model_b:
                        group_b_results.append(record)
        
        # è¨ˆç®—çµ±è¨ˆçµæœ
        def calculate_group_stats(records: List[UsageRecord]) -> Dict[str, Any]:
            if not records:
                return {"sample_size": 0}
            
            return {
                "sample_size": len(records),
                "avg_cost": statistics.mean(r.cost_usd for r in records),
                "avg_response_time": statistics.mean(r.response_time_ms for r in records),
                "success_rate": sum(1 for r in records if r.success) / len(records),
                "total_cost": sum(r.cost_usd for r in records)
            }
        
        group_a_stats = calculate_group_stats(group_a_results)
        group_b_stats = calculate_group_stats(group_b_results)
        
        # è¨ˆç®—çµ±è¨ˆé¡¯è‘—æ€§ï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼‰
        statistical_significance = self._calculate_significance(group_a_stats, group_b_stats, test_config.target_metric)
        
        return {
            "test_id": test_id,
            "test_name": test_config.name,
            "status": "completed" if datetime.now().strftime('%Y-%m-%d') > test_config.end_date else "running",
            "target_metric": test_config.target_metric.value,
            "group_a": {
                "model": test_config.model_a,
                "stats": group_a_stats
            },
            "group_b": {
                "model": test_config.model_b,
                "stats": group_b_stats
            },
            "statistical_significance": statistical_significance,
            "winner": self._determine_winner(group_a_stats, group_b_stats, test_config.target_metric),
            "confidence_level": statistical_significance.get("confidence", 0.0)
        }
    
    def _calculate_significance(self, group_a: Dict, group_b: Dict, metric: PerformanceMetric) -> Dict[str, Any]:
        """è¨ˆç®—çµ±è¨ˆé¡¯è‘—æ€§ï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼‰"""
        
        if group_a["sample_size"] < 10 or group_b["sample_size"] < 10:
            return {"significant": False, "confidence": 0.0, "reason": "insufficient_samples"}
        
        # ç°¡åŒ–çš„é¡¯è‘—æ€§æª¢é©—
        if metric == PerformanceMetric.COST_EFFICIENCY:
            metric_a = group_a.get("avg_cost", float('inf'))
            metric_b = group_b.get("avg_cost", float('inf'))
        elif metric == PerformanceMetric.RESPONSE_TIME:
            metric_a = group_a.get("avg_response_time", float('inf'))
            metric_b = group_b.get("avg_response_time", float('inf'))
        elif metric == PerformanceMetric.SUCCESS_RATE:
            metric_a = group_a.get("success_rate", 0.0)
            metric_b = group_b.get("success_rate", 0.0)
        else:
            return {"significant": False, "confidence": 0.0, "reason": "unsupported_metric"}
        
        # ç°¡åŒ–çš„æ•ˆæœé‡è¨ˆç®—
        effect_size = abs(metric_a - metric_b) / max(metric_a, metric_b, 0.001)
        
        # åŸºæ–¼æ¨£æœ¬æ•¸å’Œæ•ˆæœé‡çš„ä¿¡å¿ƒåº¦
        min_samples = min(group_a["sample_size"], group_b["sample_size"])
        confidence = min(0.95, effect_size * min_samples / 100)
        
        return {
            "significant": confidence > 0.8,
            "confidence": confidence,
            "effect_size": effect_size,
            "metric_a": metric_a,
            "metric_b": metric_b
        }
    
    def _determine_winner(self, group_a: Dict, group_b: Dict, metric: PerformanceMetric) -> str:
        """ç¢ºå®šç²å‹è€…"""
        
        if group_a["sample_size"] == 0 and group_b["sample_size"] == 0:
            return "inconclusive"
        elif group_a["sample_size"] == 0:
            return "group_b"
        elif group_b["sample_size"] == 0:
            return "group_a"
        
        if metric == PerformanceMetric.COST_EFFICIENCY:
            return "group_a" if group_a.get("avg_cost", float('inf')) < group_b.get("avg_cost", float('inf')) else "group_b"
        elif metric == PerformanceMetric.RESPONSE_TIME:
            return "group_a" if group_a.get("avg_response_time", float('inf')) < group_b.get("avg_response_time", float('inf')) else "group_b"
        elif metric == PerformanceMetric.SUCCESS_RATE:
            return "group_a" if group_a.get("success_rate", 0.0) > group_b.get("success_rate", 0.0) else "group_b"
        else:
            return "inconclusive"
    
    def get_selection_analytics(self, user_id: str = None) -> Dict[str, Any]:
        """ç²å–é¸æ“‡åˆ†æ"""
        
        # æ¨¡å‹ä½¿ç”¨çµ±è¨ˆ
        model_usage = {}
        total_requests = 0
        
        for record in self.cost_optimizer.usage_records:
            if user_id and record.user_id != user_id:
                continue
            
            model_name = record.model_name
            if model_name not in model_usage:
                model_usage[model_name] = {
                    "requests": 0,
                    "total_cost": 0.0,
                    "avg_response_time": 0.0,
                    "success_rate": 0.0
                }
            
            stats = model_usage[model_name]
            stats["requests"] += 1
            stats["total_cost"] += record.cost_usd
            stats["avg_response_time"] += record.response_time_ms
            stats["success_rate"] += 1 if record.success else 0
            total_requests += 1
        
        # è¨ˆç®—å¹³å‡å€¼å’Œä½¿ç”¨æ¯”ä¾‹
        for model_name, stats in model_usage.items():
            requests = stats["requests"]
            stats["usage_percentage"] = (requests / total_requests * 100) if total_requests > 0 else 0
            stats["avg_cost_per_request"] = stats["total_cost"] / requests
            stats["avg_response_time"] = stats["avg_response_time"] / requests
            stats["success_rate"] = (stats["success_rate"] / requests * 100) if requests > 0 else 0
        
        # æ€§èƒ½è¶¨å‹¢åˆ†æ
        performance_trends = {}
        for key, performance in self.model_performances.items():
            model_name = performance.model_name
            if model_name not in performance_trends:
                performance_trends[model_name] = []
            
            performance_trends[model_name].append({
                "task_type": performance.task_type,
                "avg_quality": performance.avg_quality,
                "sample_size": performance.sample_size,
                "confidence": performance.confidence_score
            })
        
        return {
            "model_usage": model_usage,
            "performance_trends": performance_trends,
            "active_ab_tests": len(self.active_ab_tests),
            "total_requests": total_requests,
            "learning_samples": sum(p.sample_size for p in self.model_performances.values()),
            "timestamp": datetime.now().isoformat()
        }

# ä¾¿åˆ©å‡½æ•¸
async def intelligent_model_selection(
    user_context: Dict[str, Any],
    task_type: str,
    estimated_tokens: int = 1000,
    strategy: SelectionStrategy = None
) -> Dict[str, Any]:
    """æ™ºèƒ½æ¨¡å‹é¸æ“‡"""
    
    selector = SmartModelSelector()
    
    selected_model, selection_info = await selector.select_model_intelligently(
        user_context, task_type, estimated_tokens, strategy
    )
    
    return {
        "selected_model": selected_model.model_name,
        "provider": selected_model.provider,
        "tier": selected_model.tier.value,
        "quality_score": selected_model.quality_score,
        "speed_score": selected_model.speed_score,
        "selection_info": selection_info,
        "estimated_cost": selection_info.get("estimated_cost", 0.0)
    }

if __name__ == "__main__":
    # æ¸¬è©¦è…³æœ¬
    import asyncio
    
    async def test_smart_selector():
        print("ğŸ§  æ¸¬è©¦æ™ºèƒ½æ¨¡å‹é¸æ“‡å™¨")
        
        selector = SmartModelSelector()
        
        # æ¸¬è©¦æ¨¡å‹é¸æ“‡
        user_context = {
            "user_id": "smart_test_user",
            "membership_tier": "GOLD"
        }
        
        selected_model, selection_info = await selector.select_model_intelligently(
            user_context, "technical_analysis", 1200
        )
        
        print(f"æ™ºèƒ½é¸æ“‡: {selected_model.model_name}")
        print(f"é¸æ“‡åŸå› : {selection_info.get('selection_reason')}")
        
        # æ¸¬è©¦A/Bæ¸¬è©¦
        test_id = selector.create_ab_test(
            "GPT-4 vs Gemini Pro",
            "gpt-4",
            "gemini-pro",
            PerformanceMetric.COST_EFFICIENCY,
            duration_days=3
        )
        
        print(f"å‰µå»ºA/Bæ¸¬è©¦: {test_id}")
        
        # æ¨¡æ“¬å­¸ç¿’
        await selector.learn_from_feedback(
            user_id="smart_test_user",
            model_name=selected_model.model_name,
            task_type="technical_analysis",
            actual_cost=0.025,
            actual_quality=8.5,
            actual_response_time=2800,
            success=True,
            user_rating=8.0
        )
        
        print("âœ… æ¸¬è©¦å®Œæˆ")
    
    asyncio.run(test_smart_selector())