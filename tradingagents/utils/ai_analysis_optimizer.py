#!/usr/bin/env python3
"""
AI Analysis Optimizer - AIåˆ†æå„ªåŒ–å™¨
å¤©å·¥ (TianGong) - A/Bæ¸¬è©¦é©…å‹•çš„AIåˆ†æå“è³ªå„ªåŒ–ç³»çµ±

æ­¤æ¨¡çµ„è² è²¬ï¼š
1. A/Bæ¸¬è©¦ç®¡ç†å’ŒåŸ·è¡Œ
2. åˆ†æå“è³ªè¿½è¹¤å’Œè©•ä¼°
3. è‡ªå‹•åŒ–å„ªåŒ–å»ºè­°
4. æ€§èƒ½æŒ‡æ¨™ç›£æ§
"""

import asyncio
import json
import logging
import time
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional, Tuple, Callable
from dataclasses import dataclass, asdict
from enum import Enum
import statistics
import secrets  # å®‰å…¨ä¿®å¾©ï¼šä½¿ç”¨åŠ å¯†å®‰å…¨çš„éš¨æ©Ÿæ•¸ç”Ÿæˆå™¨æ›¿æ›random
import hashlib

from .smart_model_selector import SmartModelSelector, PerformanceMetric
from .llm_cost_optimizer import LLMCostOptimizer

class OptimizationDimension(Enum):
    """å„ªåŒ–ç¶­åº¦"""
    MODEL_SELECTION = "model_selection"     # æ¨¡å‹é¸æ“‡
    PROMPT_ENGINEERING = "prompt_engineering"  # æç¤ºè©å·¥ç¨‹
    ANALYSIS_DEPTH = "analysis_depth"       # åˆ†ææ·±åº¦
    RESPONSE_FORMAT = "response_format"     # å›æ‡‰æ ¼å¼
    CACHE_STRATEGY = "cache_strategy"       # å¿«å–ç­–ç•¥

class TestType(Enum):
    """æ¸¬è©¦é¡å‹"""
    MODEL_COMPARISON = "model_comparison"   # æ¨¡å‹å°æ¯”
    PROMPT_VARIANT = "prompt_variant"       # æç¤ºè©è®Šé«”
    FEATURE_FLAG = "feature_flag"           # åŠŸèƒ½é–‹é—œ
    ALGORITHM_VARIANT = "algorithm_variant" # ç®—æ³•è®Šé«”

class QualityMetric(Enum):
    """å“è³ªæŒ‡æ¨™"""
    ACCURACY = "accuracy"                   # æº–ç¢ºæ€§
    RELEVANCE = "relevance"                 # ç›¸é—œæ€§
    COMPLETENESS = "completeness"           # å®Œæ•´æ€§
    ACTIONABILITY = "actionability"         # å¯æ“ä½œæ€§
    USER_SATISFACTION = "user_satisfaction" # ç”¨æˆ¶æ»¿æ„åº¦

@dataclass
class ABTestExperiment:
    """A/Bæ¸¬è©¦å¯¦é©—"""
    experiment_id: str
    name: str
    description: str
    test_type: TestType
    optimization_dimension: OptimizationDimension
    control_variant: Dict[str, Any]         # å°ç…§çµ„
    treatment_variant: Dict[str, Any]       # å¯¦é©—çµ„
    target_metrics: List[QualityMetric]
    traffic_allocation: float               # å¯¦é©—çµ„æµé‡æ¯”ä¾‹
    start_date: str
    end_date: str
    minimum_sample_size: int
    significance_threshold: float = 0.05    # çµ±è¨ˆé¡¯è‘—æ€§é–¾å€¼
    is_active: bool = True
    created_by: str = "ai_optimizer"

@dataclass
class AnalysisQualityRating:
    """åˆ†æå“è³ªè©•åˆ†"""
    analysis_id: str
    user_id: str
    experiment_id: Optional[str]
    variant_type: str                       # control/treatment
    quality_scores: Dict[QualityMetric, float]  # å„ç¶­åº¦è©•åˆ†
    overall_score: float
    feedback_text: Optional[str]
    rating_timestamp: str
    task_type: str
    model_used: str

@dataclass
class OptimizationInsight:
    """å„ªåŒ–æ´å¯Ÿ"""
    insight_id: str
    dimension: OptimizationDimension
    title: str
    description: str
    confidence_level: float
    potential_improvement: float
    recommendation: str
    supporting_data: Dict[str, Any]
    created_at: str

class AIAnalysisOptimizer:
    """AIåˆ†æå„ªåŒ–å™¨"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
        # ä¾è³´çµ„ä»¶
        self.model_selector = SmartModelSelector()
        self.cost_optimizer = LLMCostOptimizer()
        
        # å¯¦é©—ç®¡ç†
        self.active_experiments: Dict[str, ABTestExperiment] = {}
        self.experiment_assignments: Dict[str, str] = {}  # user_session -> variant
        
        # å“è³ªè¿½è¹¤
        self.quality_ratings: List[AnalysisQualityRating] = []
        self.quality_baselines: Dict[str, float] = {}
        
        # å„ªåŒ–å»ºè­°
        self.optimization_insights: List[OptimizationInsight] = []
        
        # é…ç½®
        self.auto_optimization_enabled = True
        self.min_confidence_for_action = 0.8
        
        # è¨­ç½®æ—¥èªŒ
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
            self.logger.setLevel(logging.INFO)
    
    def create_experiment(
        self,
        name: str,
        description: str,
        test_type: TestType,
        dimension: OptimizationDimension,
        control_config: Dict[str, Any],
        treatment_config: Dict[str, Any],
        target_metrics: List[QualityMetric],
        duration_days: int = 14,
        traffic_allocation: float = 0.5,
        min_sample_size: int = 100
    ) -> str:
        """å‰µå»ºA/Bæ¸¬è©¦å¯¦é©—"""
        
        experiment_id = f"exp_{int(time.time())}_{secrets.randbelow(8999) + 1000}"  # å®‰å…¨ä¿®å¾©ï¼šä½¿ç”¨secretsæ›¿æ›random
        start_date = datetime.now().strftime('%Y-%m-%d')
        end_date = (datetime.now() + timedelta(days=duration_days)).strftime('%Y-%m-%d')
        
        experiment = ABTestExperiment(
            experiment_id=experiment_id,
            name=name,
            description=description,
            test_type=test_type,
            optimization_dimension=dimension,
            control_variant=control_config,
            treatment_variant=treatment_config,
            target_metrics=target_metrics,
            traffic_allocation=traffic_allocation,
            start_date=start_date,
            end_date=end_date,
            minimum_sample_size=min_sample_size
        )
        
        self.active_experiments[experiment_id] = experiment
        
        self.logger.info(f"å‰µå»ºå¯¦é©—: {name} (ID: {experiment_id})")
        
        return experiment_id
    
    async def get_experiment_variant(
        self,
        user_session_id: str,
        task_type: str,
        user_context: Dict[str, Any]
    ) -> Tuple[str, Dict[str, Any]]:
        """ç²å–å¯¦é©—è®Šé«”é…ç½®"""
        
        # æŸ¥æ‰¾é©ç”¨çš„å¯¦é©—
        applicable_experiments = []
        
        for exp_id, experiment in self.active_experiments.items():
            if (experiment.is_active and 
                self._is_experiment_applicable(experiment, task_type, user_context)):
                applicable_experiments.append((exp_id, experiment))
        
        if not applicable_experiments:
            return "baseline", {}
        
        # é¸æ“‡å„ªå…ˆç´šæœ€é«˜çš„å¯¦é©—ï¼ˆé€™è£¡ç°¡åŒ–ç‚ºç¬¬ä¸€å€‹ï¼‰
        exp_id, experiment = applicable_experiments[0]
        
        # æª¢æŸ¥ç”¨æˆ¶åˆ†é…
        assignment_key = f"{user_session_id}_{exp_id}"
        
        if assignment_key not in self.experiment_assignments:
            # æ–°ç”¨æˆ¶ï¼Œé€²è¡Œåˆ†é…
            is_treatment = (secrets.randbelow(10000) / 10000.0) < experiment.traffic_allocation  # å®‰å…¨ä¿®å¾©ï¼šä½¿ç”¨secretsæ›¿æ›random
            self.experiment_assignments[assignment_key] = "treatment" if is_treatment else "control"
        
        variant_type = self.experiment_assignments[assignment_key]
        
        if variant_type == "treatment":
            return exp_id, experiment.treatment_variant
        else:
            return exp_id, experiment.control_variant
    
    def _is_experiment_applicable(
        self, 
        experiment: ABTestExperiment, 
        task_type: str, 
        user_context: Dict[str, Any]
    ) -> bool:
        """æª¢æŸ¥å¯¦é©—æ˜¯å¦é©ç”¨"""
        
        # æª¢æŸ¥æ™‚é–“ç¯„åœ
        current_date = datetime.now().strftime('%Y-%m-%d')
        if not (experiment.start_date <= current_date <= experiment.end_date):
            return False
        
        # æª¢æŸ¥ä»»å‹™é¡å‹åŒ¹é…ï¼ˆç°¡åŒ–å¯¦ç¾ï¼‰
        task_filter = experiment.control_variant.get('task_types')
        if task_filter and task_type not in task_filter:
            return False
        
        # æª¢æŸ¥ç”¨æˆ¶æ¢ä»¶ï¼ˆç°¡åŒ–å¯¦ç¾ï¼‰
        user_filter = experiment.control_variant.get('user_conditions')
        if user_filter:
            membership_tier = user_context.get('membership_tier', 'FREE')
            if 'required_tier' in user_filter and membership_tier not in user_filter['required_tier']:
                return False
        
        return True
    
    async def record_analysis_quality(
        self,
        analysis_id: str,
        user_id: str,
        task_type: str,
        model_used: str,
        quality_scores: Dict[str, float],
        overall_score: float,
        feedback_text: Optional[str] = None,
        experiment_context: Optional[Dict[str, Any]] = None
    ):
        """è¨˜éŒ„åˆ†æå“è³ªè©•åˆ†"""
        
        # è½‰æ›å“è³ªåˆ†æ•¸æ ¼å¼
        quality_metric_scores = {}
        for metric_name, score in quality_scores.items():
            try:
                metric = QualityMetric(metric_name)
                quality_metric_scores[metric] = score
            except ValueError:
                self.logger.warning(f"æœªçŸ¥å“è³ªæŒ‡æ¨™: {metric_name}")
        
        # ç¢ºå®šå¯¦é©—ä¸Šä¸‹æ–‡
        experiment_id = None
        variant_type = "baseline"
        
        if experiment_context:
            experiment_id = experiment_context.get('experiment_id')
            variant_type = experiment_context.get('variant_type', 'baseline')
        
        # å‰µå»ºå“è³ªè©•åˆ†è¨˜éŒ„
        quality_rating = AnalysisQualityRating(
            analysis_id=analysis_id,
            user_id=user_id,
            experiment_id=experiment_id,
            variant_type=variant_type,
            quality_scores=quality_metric_scores,
            overall_score=overall_score,
            feedback_text=feedback_text,
            rating_timestamp=datetime.now().isoformat(),
            task_type=task_type,
            model_used=model_used
        )
        
        self.quality_ratings.append(quality_rating)
        
        # è§¸ç™¼è‡ªå‹•å„ªåŒ–æª¢æŸ¥
        if self.auto_optimization_enabled:
            await self._check_auto_optimization_triggers(quality_rating)
        
        self.logger.info(f"è¨˜éŒ„å“è³ªè©•åˆ†: {analysis_id} - ç¸½åˆ†: {overall_score:.2f}")
    
    async def _check_auto_optimization_triggers(self, new_rating: AnalysisQualityRating):
        """æª¢æŸ¥è‡ªå‹•å„ªåŒ–è§¸ç™¼æ¢ä»¶"""
        
        # æª¢æŸ¥å“è³ªä¸‹é™
        task_type = new_rating.task_type
        recent_scores = [
            r.overall_score for r in self.quality_ratings[-50:]
            if r.task_type == task_type and r.experiment_id is None  # åªçœ‹åŸºç·šæ•¸æ“š
        ]
        
        if len(recent_scores) >= 10:
            recent_avg = statistics.mean(recent_scores[-10:])
            baseline_avg = self.quality_baselines.get(task_type, recent_avg)
            
            # å¦‚æœå“è³ªä¸‹é™è¶…é10%ï¼Œè§¸ç™¼å„ªåŒ–
            if recent_avg < baseline_avg * 0.9:
                await self._trigger_quality_improvement_experiment(task_type, baseline_avg, recent_avg)
    
    async def _trigger_quality_improvement_experiment(
        self, 
        task_type: str, 
        baseline_score: float, 
        current_score: float
    ):
        """è§¸ç™¼å“è³ªæ”¹å–„å¯¦é©—"""
        
        self.logger.warning(f"æª¢æ¸¬åˆ°å“è³ªä¸‹é™: {task_type} - {baseline_score:.2f} -> {current_score:.2f}")
        
        # è‡ªå‹•å‰µå»ºæ”¹å–„å¯¦é©—
        if task_type not in [exp.control_variant.get('task_types', []) for exp in self.active_experiments.values()]:
            
            # å‰µå»ºæ¨¡å‹å‡ç´šå¯¦é©—
            exp_id = self.create_experiment(
                name=f"Quality Recovery - {task_type}",
                description=f"è‡ªå‹•è§¸ç™¼çš„å“è³ªæ”¹å–„å¯¦é©—ï¼Œç›®æ¨™æ¢å¾© {task_type} åˆ†æå“è³ª",
                test_type=TestType.MODEL_COMPARISON,
                dimension=OptimizationDimension.MODEL_SELECTION,
                control_config={
                    "task_types": [task_type],
                    "model_strategy": "current_default"
                },
                treatment_config={
                    "task_types": [task_type],
                    "model_strategy": "quality_first"
                },
                target_metrics=[QualityMetric.ACCURACY, QualityMetric.OVERALL_QUALITY],
                duration_days=7,
                traffic_allocation=0.2,  # ä¿å®ˆçš„20%æµé‡
                min_sample_size=30
            )
            
            self.logger.info(f"è‡ªå‹•å‰µå»ºå“è³ªæ”¹å–„å¯¦é©—: {exp_id}")
    
    def analyze_experiment_results(self, experiment_id: str) -> Optional[Dict[str, Any]]:
        """åˆ†æå¯¦é©—çµæœ"""
        
        if experiment_id not in self.active_experiments:
            return None
        
        experiment = self.active_experiments[experiment_id]
        
        # æ”¶é›†å¯¦é©—æ•¸æ“š
        control_ratings = [
            r for r in self.quality_ratings
            if r.experiment_id == experiment_id and r.variant_type == "control"
        ]
        
        treatment_ratings = [
            r for r in self.quality_ratings
            if r.experiment_id == experiment_id and r.variant_type == "treatment"
        ]
        
        if not control_ratings or not treatment_ratings:
            return {
                "status": "insufficient_data",
                "control_samples": len(control_ratings),
                "treatment_samples": len(treatment_ratings),
                "minimum_required": experiment.minimum_sample_size
            }
        
        # è¨ˆç®—çµ±è¨ˆçµæœ
        results = {}
        
        for metric in experiment.target_metrics:
            control_scores = [r.quality_scores.get(metric, 0) for r in control_ratings if metric in r.quality_scores]
            treatment_scores = [r.quality_scores.get(metric, 0) for r in treatment_ratings if metric in r.quality_scores]
            
            if control_scores and treatment_scores:
                control_mean = statistics.mean(control_scores)
                treatment_mean = statistics.mean(treatment_scores)
                
                # ç°¡åŒ–çš„çµ±è¨ˆæª¢é©—
                improvement = (treatment_mean - control_mean) / control_mean if control_mean > 0 else 0
                
                # åŸºæ–¼æ¨£æœ¬å¤§å°å’Œæ•ˆæœé‡çš„ç°¡åŒ–ä¿¡å¿ƒåº¦è¨ˆç®—
                min_samples = min(len(control_scores), len(treatment_scores))
                effect_size = abs(improvement)
                confidence = min(0.99, effect_size * min_samples / 50)
                
                results[metric.value] = {
                    "control_mean": control_mean,
                    "treatment_mean": treatment_mean,
                    "improvement_percent": improvement * 100,
                    "confidence": confidence,
                    "significant": confidence > (1 - experiment.significance_threshold),
                    "control_samples": len(control_scores),
                    "treatment_samples": len(treatment_scores)
                }
        
        # æ•´é«”åˆ†æè©•åˆ†
        control_overall = [r.overall_score for r in control_ratings]
        treatment_overall = [r.overall_score for r in treatment_ratings]
        
        overall_analysis = {
            "control_mean": statistics.mean(control_overall),
            "treatment_mean": statistics.mean(treatment_overall),
            "improvement_percent": ((statistics.mean(treatment_overall) - statistics.mean(control_overall)) / statistics.mean(control_overall)) * 100,
            "control_samples": len(control_overall),
            "treatment_samples": len(treatment_overall)
        }
        
        # æ±ºç­–å»ºè­°
        significant_improvements = sum(
            1 for metric_result in results.values()
            if metric_result.get("significant", False) and metric_result.get("improvement_percent", 0) > 0
        )
        
        if significant_improvements >= len(experiment.target_metrics) / 2:
            recommendation = "adopt_treatment"
        elif any(r.get("improvement_percent", 0) < -5 for r in results.values()):
            recommendation = "reject_treatment"
        else:
            recommendation = "continue_testing"
        
        return {
            "experiment_id": experiment_id,
            "experiment_name": experiment.name,
            "status": "analysis_complete",
            "metric_results": results,
            "overall_analysis": overall_analysis,
            "recommendation": recommendation,
            "confidence_level": statistics.mean([r.get("confidence", 0) for r in results.values()]) if results else 0,
            "analysis_timestamp": datetime.now().isoformat()
        }
    
    async def generate_optimization_insights(self, days: int = 30) -> List[OptimizationInsight]:
        """ç”Ÿæˆå„ªåŒ–æ´å¯Ÿ"""
        
        insights = []
        
        # åˆ†ææœ€è¿‘çš„å“è³ªæ•¸æ“š
        recent_cutoff = datetime.now() - timedelta(days=days)
        recent_ratings = [
            r for r in self.quality_ratings
            if datetime.fromisoformat(r.rating_timestamp) >= recent_cutoff
        ]
        
        if not recent_ratings:
            return insights
        
        # 1. æ¨¡å‹æ€§èƒ½æ´å¯Ÿ
        model_insights = await self._analyze_model_performance(recent_ratings)
        insights.extend(model_insights)
        
        # 2. ä»»å‹™é¡å‹æ´å¯Ÿ
        task_insights = await self._analyze_task_performance(recent_ratings)
        insights.extend(task_insights)
        
        # 3. ç”¨æˆ¶æ»¿æ„åº¦æ´å¯Ÿ
        satisfaction_insights = await self._analyze_user_satisfaction(recent_ratings)
        insights.extend(satisfaction_insights)
        
        # 4. æˆæœ¬æ•ˆç›Šæ´å¯Ÿ
        cost_insights = await self._analyze_cost_effectiveness(recent_ratings)
        insights.extend(cost_insights)
        
        # ä¿å­˜æ´å¯Ÿ
        self.optimization_insights.extend(insights)
        
        return insights
    
    async def _analyze_model_performance(self, ratings: List[AnalysisQualityRating]) -> List[OptimizationInsight]:
        """åˆ†ææ¨¡å‹æ€§èƒ½"""
        insights = []
        
        # æŒ‰æ¨¡å‹åˆ†çµ„åˆ†æ
        model_performance = {}
        for rating in ratings:
            model = rating.model_used
            if model not in model_performance:
                model_performance[model] = {
                    "scores": [],
                    "count": 0
                }
            
            model_performance[model]["scores"].append(rating.overall_score)
            model_performance[model]["count"] += 1
        
        # æ‰¾å‡ºè¡¨ç¾æœ€å¥½å’Œæœ€å·®çš„æ¨¡å‹
        model_averages = {
            model: statistics.mean(data["scores"])
            for model, data in model_performance.items()
            if data["count"] >= 5  # è‡³å°‘5å€‹æ¨£æœ¬
        }
        
        if len(model_averages) >= 2:
            best_model = max(model_averages, key=model_averages.get)
            worst_model = min(model_averages, key=model_averages.get)
            
            performance_gap = model_averages[best_model] - model_averages[worst_model]
            
            if performance_gap > 1.0:  # é¡¯è‘—å·®ç•°
                insights.append(OptimizationInsight(
                    insight_id=f"model_perf_{int(time.time())}",
                    dimension=OptimizationDimension.MODEL_SELECTION,
                    title=f"æ¨¡å‹æ€§èƒ½å·®ç•°é¡¯è‘—",
                    description=f"{best_model} æ¯” {worst_model} è¡¨ç¾å¥½ {performance_gap:.1f} åˆ†",
                    confidence_level=0.8,
                    potential_improvement=performance_gap / 10.0,  # è½‰æ›ç‚ºç™¾åˆ†æ¯”
                    recommendation=f"è€ƒæ…®æ›´å¤šä½¿ç”¨ {best_model} æ¨¡å‹",
                    supporting_data={
                        "best_model": best_model,
                        "best_score": model_averages[best_model],
                        "worst_model": worst_model,
                        "worst_score": model_averages[worst_model],
                        "performance_gap": performance_gap
                    },
                    created_at=datetime.now().isoformat()
                ))
        
        return insights
    
    async def _analyze_task_performance(self, ratings: List[AnalysisQualityRating]) -> List[OptimizationInsight]:
        """åˆ†æä»»å‹™æ€§èƒ½"""
        insights = []
        
        # æŒ‰ä»»å‹™é¡å‹åˆ†æ
        task_performance = {}
        for rating in ratings:
            task_type = rating.task_type
            if task_type not in task_performance:
                task_performance[task_type] = []
            task_performance[task_type].append(rating.overall_score)
        
        # æ‰¾å‡ºéœ€è¦æ”¹å–„çš„ä»»å‹™é¡å‹
        for task_type, scores in task_performance.items():
            if len(scores) >= 10:  # è¶³å¤ æ¨£æœ¬
                avg_score = statistics.mean(scores)
                if avg_score < 7.0:  # ä½æ–¼7åˆ†éœ€è¦æ”¹å–„
                    insights.append(OptimizationInsight(
                        insight_id=f"task_perf_{task_type}_{int(time.time())}",
                        dimension=OptimizationDimension.ANALYSIS_DEPTH,
                        title=f"{task_type} åˆ†æå“è³ªéœ€è¦æ”¹å–„",
                        description=f"{task_type} å¹³å‡è©•åˆ†åƒ… {avg_score:.1f}ï¼Œä½æ–¼æœŸæœ›æ°´æº–",
                        confidence_level=0.7,
                        potential_improvement=(8.0 - avg_score) / 10.0,
                        recommendation=f"é‡å° {task_type} å„ªåŒ–åˆ†ææµç¨‹å’Œæç¤ºè©",
                        supporting_data={
                            "task_type": task_type,
                            "current_score": avg_score,
                            "target_score": 8.0,
                            "sample_size": len(scores)
                        },
                        created_at=datetime.now().isoformat()
                    ))
        
        return insights
    
    async def _analyze_user_satisfaction(self, ratings: List[AnalysisQualityRating]) -> List[OptimizationInsight]:
        """åˆ†æç”¨æˆ¶æ»¿æ„åº¦"""
        insights = []
        
        # åˆ†ææ»¿æ„åº¦è¶¨å‹¢
        satisfaction_scores = [
            r.quality_scores.get(QualityMetric.USER_SATISFACTION, r.overall_score)
            for r in ratings
            if QualityMetric.USER_SATISFACTION in r.quality_scores or r.overall_score > 0
        ]
        
        if len(satisfaction_scores) >= 20:
            recent_satisfaction = statistics.mean(satisfaction_scores[-10:])
            overall_satisfaction = statistics.mean(satisfaction_scores)
            
            if recent_satisfaction < overall_satisfaction * 0.9:
                insights.append(OptimizationInsight(
                    insight_id=f"satisfaction_trend_{int(time.time())}",
                    dimension=OptimizationDimension.RESPONSE_FORMAT,
                    title="ç”¨æˆ¶æ»¿æ„åº¦å‘ˆä¸‹é™è¶¨å‹¢",
                    description=f"æœ€è¿‘æ»¿æ„åº¦ {recent_satisfaction:.1f} ä½æ–¼æ•´é«”å¹³å‡ {overall_satisfaction:.1f}",
                    confidence_level=0.6,
                    potential_improvement=(overall_satisfaction - recent_satisfaction) / 10.0,
                    recommendation="æª¢æŸ¥å›æ‡‰æ ¼å¼å’Œå…§å®¹è³ªé‡ï¼Œå¯èƒ½éœ€è¦èª¿æ•´è¼¸å‡ºæ¨¡æ¿",
                    supporting_data={
                        "recent_satisfaction": recent_satisfaction,
                        "overall_satisfaction": overall_satisfaction,
                        "sample_size": len(satisfaction_scores)
                    },
                    created_at=datetime.now().isoformat()
                ))
        
        return insights
    
    async def _analyze_cost_effectiveness(self, ratings: List[AnalysisQualityRating]) -> List[OptimizationInsight]:
        """åˆ†ææˆæœ¬æ•ˆç›Š"""
        insights = []
        
        # ç²å–æˆæœ¬æ•¸æ“š
        cost_data = await self.cost_optimizer.get_cost_analytics(days=30)
        
        if cost_data.get("total_requests", 0) > 0:
            avg_cost = cost_data.get("average_cost_per_request", 0)
            avg_quality = statistics.mean([r.overall_score for r in ratings]) if ratings else 0
            
            # æˆæœ¬æ•ˆç›Šè©•åˆ†
            cost_effectiveness = avg_quality / max(avg_cost * 1000, 0.001)  # å“è³ª/åƒåˆ†ä¹‹æˆæœ¬
            
            if cost_effectiveness < 500:  # ç¶“é©—é–¾å€¼
                insights.append(OptimizationInsight(
                    insight_id=f"cost_effectiveness_{int(time.time())}",
                    dimension=OptimizationDimension.MODEL_SELECTION,
                    title="æˆæœ¬æ•ˆç›Šæœ‰æ”¹å–„ç©ºé–“",
                    description=f"ç•¶å‰æˆæœ¬æ•ˆç›Šè©•åˆ† {cost_effectiveness:.0f}ï¼Œå¯è€ƒæ…®å„ªåŒ–æ¨¡å‹é¸æ“‡",
                    confidence_level=0.5,
                    potential_improvement=0.2,
                    recommendation="è€ƒæ…®ä½¿ç”¨æ›´ç¶“æ¿Ÿçš„æ¨¡å‹é€²è¡Œç°¡å–®åˆ†æä»»å‹™",
                    supporting_data={
                        "cost_effectiveness_score": cost_effectiveness,
                        "avg_cost": avg_cost,
                        "avg_quality": avg_quality
                    },
                    created_at=datetime.now().isoformat()
                ))
        
        return insights
    
    def get_optimization_dashboard(self) -> Dict[str, Any]:
        """ç²å–å„ªåŒ–å„€è¡¨æ¿æ•¸æ“š"""
        
        # æ´»èºå¯¦é©—çµ±è¨ˆ
        active_count = len([exp for exp in self.active_experiments.values() if exp.is_active])
        
        # æœ€è¿‘å“è³ªè¶¨å‹¢
        recent_ratings = self.quality_ratings[-100:] if self.quality_ratings else []
        
        quality_trend = []
        if recent_ratings:
            # æŒ‰é€±è¨ˆç®—å¹³å‡å“è³ª
            for i in range(4):  # æœ€è¿‘4é€±
                week_start = datetime.now() - timedelta(weeks=i+1)
                week_end = datetime.now() - timedelta(weeks=i)
                
                week_ratings = [
                    r for r in recent_ratings
                    if week_start <= datetime.fromisoformat(r.rating_timestamp) < week_end
                ]
                
                if week_ratings:
                    week_avg = statistics.mean([r.overall_score for r in week_ratings])
                    quality_trend.append({
                        "week": f"Week {4-i}",
                        "avg_quality": week_avg,
                        "sample_size": len(week_ratings)
                    })
        
        # å¯¦é©—çµæœæ‘˜è¦
        experiment_results = []
        for exp_id in list(self.active_experiments.keys())[-5:]:  # æœ€è¿‘5å€‹å¯¦é©—
            result = self.analyze_experiment_results(exp_id)
            if result and result.get("status") != "insufficient_data":
                experiment_results.append({
                    "experiment_id": exp_id,
                    "name": self.active_experiments[exp_id].name,
                    "recommendation": result.get("recommendation"),
                    "confidence": result.get("confidence_level", 0)
                })
        
        # å„ªåŒ–æ´å¯Ÿæ‘˜è¦
        recent_insights = self.optimization_insights[-10:] if self.optimization_insights else []
        
        return {
            "summary": {
                "active_experiments": active_count,
                "total_quality_ratings": len(self.quality_ratings),
                "avg_quality_last_week": statistics.mean([
                    r.overall_score for r in recent_ratings[-50:]
                ]) if recent_ratings else 0,
                "optimization_insights": len(recent_insights)
            },
            "quality_trend": quality_trend,
            "experiment_results": experiment_results,
            "recent_insights": [asdict(insight) for insight in recent_insights],
            "timestamp": datetime.now().isoformat()
        }

# ä¾¿åˆ©å‡½æ•¸
async def create_model_comparison_experiment(
    model_a: str,
    model_b: str,
    task_types: List[str],
    duration_days: int = 14
) -> str:
    """å‰µå»ºæ¨¡å‹å°æ¯”å¯¦é©—"""
    
    optimizer = AIAnalysisOptimizer()
    
    return optimizer.create_experiment(
        name=f"Model Comparison: {model_a} vs {model_b}",
        description=f"æ¯”è¼ƒ {model_a} å’Œ {model_b} åœ¨ {', '.join(task_types)} ä»»å‹™ä¸Šçš„è¡¨ç¾",
        test_type=TestType.MODEL_COMPARISON,
        dimension=OptimizationDimension.MODEL_SELECTION,
        control_config={
            "model_name": model_a,
            "task_types": task_types
        },
        treatment_config={
            "model_name": model_b,
            "task_types": task_types
        },
        target_metrics=[QualityMetric.ACCURACY, QualityMetric.USER_SATISFACTION],
        duration_days=duration_days
    )

async def get_analysis_optimization_recommendations(task_type: str) -> List[Dict[str, Any]]:
    """ç²å–åˆ†æå„ªåŒ–å»ºè­°"""
    
    optimizer = AIAnalysisOptimizer()
    insights = await optimizer.generate_optimization_insights(days=30)
    
    # éæ¿¾ç›¸é—œå»ºè­°
    relevant_insights = [
        insight for insight in insights
        if task_type in insight.supporting_data.get("task_type", "")
    ]
    
    return [asdict(insight) for insight in relevant_insights]

if __name__ == "__main__":
    # æ¸¬è©¦è…³æœ¬
    import asyncio
    
    async def test_ai_optimizer():
        print("ğŸ”¬ æ¸¬è©¦AIåˆ†æå„ªåŒ–å™¨")
        
        optimizer = AIAnalysisOptimizer()
        
        # å‰µå»ºæ¸¬è©¦å¯¦é©—
        exp_id = optimizer.create_experiment(
            name="æ¸¬è©¦å¯¦é©—",
            description="GPT-4 vs Gemini Pro æŠ€è¡“åˆ†æå°æ¯”",
            test_type=TestType.MODEL_COMPARISON,
            dimension=OptimizationDimension.MODEL_SELECTION,
            control_config={"model_name": "gpt-4"},
            treatment_config={"model_name": "gemini-pro"},
            target_metrics=[QualityMetric.ACCURACY, QualityMetric.USER_SATISFACTION],
            duration_days=7
        )
        
        print(f"å‰µå»ºå¯¦é©—: {exp_id}")
        
        # æ¨¡æ“¬å“è³ªè©•åˆ†
        await optimizer.record_analysis_quality(
            analysis_id="test_001",
            user_id="test_user",
            task_type="technical_analysis",
            model_used="gpt-4",
            quality_scores={"accuracy": 8.5, "user_satisfaction": 7.8},
            overall_score=8.2,
            experiment_context={"experiment_id": exp_id, "variant_type": "control"}
        )
        
        # ç”Ÿæˆå„ªåŒ–æ´å¯Ÿ
        insights = await optimizer.generate_optimization_insights(days=7)
        print(f"ç”Ÿæˆæ´å¯Ÿ: {len(insights)} å€‹")
        
        # ç²å–å„€è¡¨æ¿
        dashboard = optimizer.get_optimization_dashboard()
        print(f"æ´»èºå¯¦é©—: {dashboard['summary']['active_experiments']}")
        
        print("âœ… æ¸¬è©¦å®Œæˆ")
    
    asyncio.run(test_ai_optimizer())