#!/usr/bin/env python3
"""
RULER Reward System - RULERçå‹µç”Ÿæˆç³»çµ±
å¤©å·¥ (TianGong) - é›¶æ¨£æœ¬çå‹µç”Ÿæˆç³»çµ±ï¼Œæ”¯æ´å¤šç¶­åº¦è©•ä¼°å’Œæœƒå“¡ç­‰ç´šå·®ç•°åŒ–

æ­¤æ¨¡çµ„æä¾›ï¼š
1. é›¶æ¨£æœ¬çå‹µç”Ÿæˆæ©Ÿåˆ¶ï¼Œç„¡éœ€äººå·¥æ¨™æ³¨
2. å¤šç¶­åº¦çå‹µå‡½æ•¸ï¼ˆç²åˆ©ç‡ã€é¢¨éšªèª¿æ•´æ”¶ç›Šã€å¤æ™®æ¯”ç‡ç­‰ï¼‰
3. æœƒå“¡ç­‰ç´šå·®ç•°åŒ–çš„çå‹µæ¬Šé‡ç³»çµ±
4. çå‹µä¿¡è™Ÿçš„æº–ç¢ºæ€§é©—è­‰æ©Ÿåˆ¶
5. å¯¦æ™‚å¸‚å ´è¡¨ç¾è¿½è¹¤
6. å‹•æ…‹çå‹µæ¨¡å‹èª¿å„ª
"""

from typing import Dict, Any, List, Optional, Tuple, Union, Callable
from dataclasses import dataclass, asdict, field
from datetime import datetime, timedelta
from enum import Enum
import json
import logging
import asyncio
import numpy as np
import pandas as pd
from pathlib import Path
import hashlib
import math
from concurrent.futures import ThreadPoolExecutor
import threading
from collections import defaultdict, deque
import statistics
from abc import ABC, abstractmethod

# Import trajectory components
try:
    from .trajectory_collector import AnalysisTrajectory, TrajectoryMetrics
    TRAJECTORY_COLLECTOR_AVAILABLE = True
except ImportError:
    TRAJECTORY_COLLECTOR_AVAILABLE = False

class RewardType(Enum):
    """çå‹µé¡å‹"""
    # æ ¸å¿ƒç¸¾æ•ˆçå‹µ
    ACCURACY_REWARD = "accuracy_reward"           # é æ¸¬æº–ç¢ºæ€§
    RETURN_PERFORMANCE = "return_performance"     # æŠ•è³‡å ±é…¬ç‡
    RISK_ADJUSTED_RETURN = "risk_adjusted_return" # é¢¨éšªèª¿æ•´å¾Œå ±é…¬
    
    # é¢¨éšªç®¡ç†çå‹µ  
    RISK_CONTROL = "risk_control"                 # é¢¨éšªæ§åˆ¶
    DRAWDOWN_MANAGEMENT = "drawdown_management"   # å›æ’¤ç®¡ç†
    VOLATILITY_MANAGEMENT = "volatility_management" # æ³¢å‹•æ€§ç®¡ç†
    
    # æ•ˆç‡çå‹µ
    TIMING_ACCURACY = "timing_accuracy"           # æ™‚æ©ŸæŠŠæ¡
    CONFIDENCE_CALIBRATION = "confidence_calibration" # ä¿¡å¿ƒåº¦æ ¡æº–
    REASONING_QUALITY = "reasoning_quality"       # æ¨ç†è³ªé‡
    
    # ç”¨æˆ¶é«”é©—çå‹µ
    USER_SATISFACTION = "user_satisfaction"       # ç”¨æˆ¶æ»¿æ„åº¦
    EXPLANATION_QUALITY = "explanation_quality"   # è§£é‡‹è³ªé‡
    ACTIONABILITY = "actionability"               # å¯åŸ·è¡Œæ€§
    
    # ç³»çµ±æ€§çå‹µ
    CONSISTENCY = "consistency"                   # ä¸€è‡´æ€§
    ROBUSTNESS = "robustness"                     # ç©©å¥æ€§
    ADAPTABILITY = "adaptability"                 # é©æ‡‰æ€§

class MembershipTier(Enum):
    """æœƒå“¡ç­‰ç´š"""
    FREE = "free"
    BASIC = "basic"
    PREMIUM = "premium"
    GOLD = "gold"
    PLATINUM = "platinum"

class RewardScope(Enum):
    """çå‹µç¯„åœ"""
    STEP_LEVEL = "step_level"           # æ­¥é©Ÿç´šåˆ¥
    TRAJECTORY_LEVEL = "trajectory_level" # è»Œè·¡ç´šåˆ¥
    SESSION_LEVEL = "session_level"     # æœƒè©±ç´šåˆ¥
    LONG_TERM = "long_term"            # é•·æœŸè¡¨ç¾

@dataclass
class MarketPerformanceData:
    """å¸‚å ´è¡¨ç¾æ•¸æ“š"""
    stock_id: str
    recommendation_date: str
    recommendation_type: str  # BUY, SELL, HOLD
    recommendation_price: float
    target_price: Optional[float] = None
    
    # è¿½è¹¤æ•¸æ“š
    current_price: float = 0.0
    price_change_1d: float = 0.0
    price_change_7d: float = 0.0
    price_change_30d: float = 0.0
    price_change_90d: float = 0.0
    
    # å¸‚å ´åŸºæº–
    market_change_1d: float = 0.0
    market_change_7d: float = 0.0
    market_change_30d: float = 0.0
    market_change_90d: float = 0.0
    
    # é¢¨éšªæŒ‡æ¨™
    volatility_30d: float = 0.0
    max_drawdown: float = 0.0
    sharpe_ratio: float = 0.0
    
    last_updated: str = field(default_factory=lambda: datetime.now().isoformat())
    
    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)

@dataclass
class RewardMetrics:
    """çå‹µæŒ‡æ¨™"""
    reward_id: str
    trajectory_id: str
    reward_type: RewardType
    reward_scope: RewardScope
    
    # æ ¸å¿ƒçå‹µå€¼
    raw_reward: float = 0.0              # åŸå§‹çå‹µå€¼ [-1, 1]
    weighted_reward: float = 0.0         # åŠ æ¬Šçå‹µå€¼
    final_reward: float = 0.0            # æœ€çµ‚çå‹µå€¼
    
    # ç½®ä¿¡åº¦å’Œè³ªé‡
    confidence: float = 0.0              # çå‹µç½®ä¿¡åº¦
    quality_score: float = 0.0           # çå‹µè³ªé‡åˆ†æ•¸
    reliability: float = 0.0             # å¯é æ€§åˆ†æ•¸
    
    # è¨ˆç®—ç´°ç¯€
    calculation_method: str = "default"
    data_sources: List[str] = field(default_factory=list)
    calculation_timestamp: str = field(default_factory=lambda: datetime.now().isoformat())
    
    # é©—è­‰ä¿¡æ¯
    is_validated: bool = False
    validation_score: float = 0.0
    validation_notes: List[str] = field(default_factory=list)
    
    def to_dict(self) -> Dict[str, Any]:
        result = asdict(self)
        result['reward_type'] = self.reward_type.value
        result['reward_scope'] = self.reward_scope.value
        return result

@dataclass
class RewardSignal:
    """çå‹µä¿¡è™Ÿ"""
    signal_id: str
    trajectory_id: str
    user_id: str
    stock_id: str
    analyst_type: str
    
    # çå‹µçµ„æˆ
    reward_components: Dict[RewardType, RewardMetrics] = field(default_factory=dict)
    
    # ç¶œåˆçå‹µ
    total_reward: float = 0.0
    weighted_total_reward: float = 0.0
    
    # æœƒå“¡ç­‰ç´šèª¿æ•´
    membership_tier: MembershipTier = MembershipTier.FREE
    membership_multiplier: float = 1.0
    
    # æ™‚é–“ä¿¡æ¯
    generated_at: str = field(default_factory=lambda: datetime.now().isoformat())
    market_performance_period: int = 30  # å¸‚å ´è¡¨ç¾è¿½è¹¤å¤©æ•¸
    
    # é©—è­‰ç‹€æ…‹
    is_final: bool = False
    requires_validation: bool = True
    
    def add_reward_component(self, reward_type: RewardType, metrics: RewardMetrics):
        """æ·»åŠ çå‹µçµ„æˆéƒ¨åˆ†"""
        self.reward_components[reward_type] = metrics
        self._recalculate_total_reward()
    
    def _recalculate_total_reward(self):
        """é‡æ–°è¨ˆç®—ç¸½çå‹µ"""
        if not self.reward_components:
            self.total_reward = 0.0
            return
        
        # è¨ˆç®—åŠ æ¬Šå¹³å‡
        total_weighted = 0.0
        total_weights = 0.0
        
        for reward_type, metrics in self.reward_components.items():
            weight = REWARD_TYPE_WEIGHTS.get(reward_type, 0.1)
            total_weighted += metrics.final_reward * weight
            total_weights += weight
        
        self.total_reward = total_weighted / total_weights if total_weights > 0 else 0.0
        self.weighted_total_reward = self.total_reward * self.membership_multiplier
    
    def to_dict(self) -> Dict[str, Any]:
        result = asdict(self)
        result['membership_tier'] = self.membership_tier.value
        result['reward_components'] = {
            reward_type.value: metrics.to_dict() 
            for reward_type, metrics in self.reward_components.items()
        }
        return result

# çå‹µæ¬Šé‡é…ç½®
REWARD_TYPE_WEIGHTS = {
    RewardType.ACCURACY_REWARD: 0.25,
    RewardType.RETURN_PERFORMANCE: 0.20,
    RewardType.RISK_ADJUSTED_RETURN: 0.15,
    RewardType.RISK_CONTROL: 0.10,
    RewardType.TIMING_ACCURACY: 0.08,
    RewardType.CONFIDENCE_CALIBRATION: 0.07,
    RewardType.REASONING_QUALITY: 0.05,
    RewardType.USER_SATISFACTION: 0.05,
    RewardType.CONSISTENCY: 0.03,
    RewardType.ROBUSTNESS: 0.02
}

# æœƒå“¡ç­‰ç´šçå‹µä¹˜æ•¸
MEMBERSHIP_MULTIPLIERS = {
    MembershipTier.FREE: 1.0,
    MembershipTier.BASIC: 1.1,
    MembershipTier.PREMIUM: 1.2,
    MembershipTier.GOLD: 1.3,
    MembershipTier.PLATINUM: 1.5
}

class RewardCalculator(ABC):
    """çå‹µè¨ˆç®—å™¨åŸºé¡"""
    
    @abstractmethod
    async def calculate_reward(self, 
                              trajectory: 'AnalysisTrajectory',
                              market_data: MarketPerformanceData,
                              user_context: Dict[str, Any] = None) -> RewardMetrics:
        """è¨ˆç®—çå‹µ"""
        pass
    
    @abstractmethod
    def get_reward_type(self) -> RewardType:
        """ç²å–çå‹µé¡å‹"""
        pass

class AccuracyRewardCalculator(RewardCalculator):
    """æº–ç¢ºæ€§çå‹µè¨ˆç®—å™¨"""
    
    async def calculate_reward(self, 
                              trajectory: 'AnalysisTrajectory',
                              market_data: MarketPerformanceData,
                              user_context: Dict[str, Any] = None) -> RewardMetrics:
        """è¨ˆç®—é æ¸¬æº–ç¢ºæ€§çå‹µ"""
        
        if not trajectory.final_recommendation or not market_data:
            return RewardMetrics(
                reward_id=f"accuracy_{trajectory.trajectory_id}",
                trajectory_id=trajectory.trajectory_id,
                reward_type=RewardType.ACCURACY_REWARD,
                reward_scope=RewardScope.TRAJECTORY_LEVEL,
                raw_reward=0.0,
                confidence=0.0
            )
        
        # æ ¹æ“šå»ºè­°é¡å‹å’Œå¯¦éš›è¡¨ç¾è¨ˆç®—æº–ç¢ºæ€§
        recommendation = trajectory.final_recommendation
        confidence = trajectory.final_confidence or 0.5
        
        # ä½¿ç”¨30å¤©è¡¨ç¾ä½œç‚ºä¸»è¦è©•ä¼°æ¨™æº–
        price_change = market_data.price_change_30d
        
        # è¨ˆç®—åŸºç¤æº–ç¢ºæ€§åˆ†æ•¸
        if recommendation == "BUY":
            accuracy_score = self._sigmoid(price_change * 10)  # ä¸Šæ¼²çå‹µ
        elif recommendation == "SELL":
            accuracy_score = self._sigmoid(-price_change * 10)  # ä¸‹è·Œçå‹µ
        else:  # HOLD
            accuracy_score = self._sigmoid(-abs(price_change) * 5)  # ç©©å®šçå‹µ
        
        # èª¿æ•´ä¿¡å¿ƒåº¦
        calibrated_score = accuracy_score * confidence
        
        # è€ƒæ…®æ™‚é–“è¡°æ¸›ï¼ˆé æ¸¬è¶Šä¹…å‰ï¼Œæ¬Šé‡è¶Šä½ï¼‰
        recommendation_date = datetime.fromisoformat(trajectory.start_time)
        current_date = datetime.now()
        days_passed = (current_date - recommendation_date).days
        time_decay = max(0.5, 1.0 - days_passed * 0.01)  # æ¯å¤©è¡°æ¸›1%
        
        final_reward = calibrated_score * time_decay
        
        return RewardMetrics(
            reward_id=f"accuracy_{trajectory.trajectory_id}",
            trajectory_id=trajectory.trajectory_id,
            reward_type=RewardType.ACCURACY_REWARD,
            reward_scope=RewardScope.TRAJECTORY_LEVEL,
            raw_reward=accuracy_score,
            weighted_reward=calibrated_score,
            final_reward=final_reward,
            confidence=0.8,  # å¸‚å ´æ•¸æ“šç›¸å°å¯é 
            quality_score=0.9 if abs(price_change) > 0.05 else 0.6,  # å¤§å¹…æ³¢å‹•æ›´æœ‰ä¿¡æ¯é‡
            calculation_method="sigmoid_with_confidence_and_time_decay",
            data_sources=["market_price_data", "trajectory_recommendation"]
        )
    
    def _sigmoid(self, x: float) -> float:
        """Sigmoidå‡½æ•¸ï¼Œå°‡å€¼æ˜ å°„åˆ°[-1, 1]"""
        return 2 / (1 + math.exp(-x)) - 1
    
    def get_reward_type(self) -> RewardType:
        return RewardType.ACCURACY_REWARD

class ReturnPerformanceCalculator(RewardCalculator):
    """æŠ•è³‡å ±é…¬ç‡çå‹µè¨ˆç®—å™¨"""
    
    async def calculate_reward(self, 
                              trajectory: 'AnalysisTrajectory',
                              market_data: MarketPerformanceData,
                              user_context: Dict[str, Any] = None) -> RewardMetrics:
        """è¨ˆç®—æŠ•è³‡å ±é…¬ç‡çå‹µ"""
        
        if not market_data or not trajectory.final_recommendation:
            return RewardMetrics(
                reward_id=f"return_{trajectory.trajectory_id}",
                trajectory_id=trajectory.trajectory_id,
                reward_type=RewardType.RETURN_PERFORMANCE,
                reward_scope=RewardScope.TRAJECTORY_LEVEL,
                raw_reward=0.0
            )
        
        # è¨ˆç®—çµ•å°å’Œç›¸å°å ±é…¬
        absolute_return = market_data.price_change_30d
        relative_return = absolute_return - market_data.market_change_30d  # è¶…é¡å ±é…¬
        
        # æ ¹æ“šå»ºè­°èª¿æ•´å ±é…¬æ–¹å‘
        if trajectory.final_recommendation == "SELL":
            absolute_return = -absolute_return
            relative_return = -relative_return
        elif trajectory.final_recommendation == "HOLD":
            # HOLDå»ºè­°é‡é»è€ƒæ…®æ³¢å‹•æ€§æ§åˆ¶
            absolute_return = -abs(absolute_return) * 0.5
            relative_return = -abs(relative_return) * 0.5
        
        # æ¨™æº–åŒ–å ±é…¬ç‡åˆ°[-1, 1]ç¯„åœ
        normalized_absolute = self._normalize_return(absolute_return)
        normalized_relative = self._normalize_return(relative_return)
        
        # ç¶œåˆè©•åˆ†ï¼ˆ70%ç›¸å°å ±é…¬ + 30%çµ•å°å ±é…¬ï¼‰
        final_reward = normalized_relative * 0.7 + normalized_absolute * 0.3
        
        return RewardMetrics(
            reward_id=f"return_{trajectory.trajectory_id}",
            trajectory_id=trajectory.trajectory_id,
            reward_type=RewardType.RETURN_PERFORMANCE,
            reward_scope=RewardScope.TRAJECTORY_LEVEL,
            raw_reward=absolute_return,
            weighted_reward=relative_return,
            final_reward=final_reward,
            confidence=0.85,
            quality_score=min(1.0, abs(relative_return) * 5),
            calculation_method="relative_return_weighted",
            data_sources=["market_price_data", "benchmark_data"]
        )
    
    def _normalize_return(self, return_rate: float) -> float:
        """æ¨™æº–åŒ–å ±é…¬ç‡"""
        # ä½¿ç”¨tanhå‡½æ•¸å°‡å ±é…¬ç‡æ˜ å°„åˆ°[-1, 1]
        return math.tanh(return_rate * 10)
    
    def get_reward_type(self) -> RewardType:
        return RewardType.RETURN_PERFORMANCE

class RiskAdjustedReturnCalculator(RewardCalculator):
    """é¢¨éšªèª¿æ•´å¾Œå ±é…¬è¨ˆç®—å™¨"""
    
    async def calculate_reward(self, 
                              trajectory: 'AnalysisTrajectory',
                              market_data: MarketPerformanceData,
                              user_context: Dict[str, Any] = None) -> RewardMetrics:
        """è¨ˆç®—é¢¨éšªèª¿æ•´å¾Œå ±é…¬çå‹µ"""
        
        if not market_data:
            return RewardMetrics(
                reward_id=f"risk_adj_return_{trajectory.trajectory_id}",
                trajectory_id=trajectory.trajectory_id,
                reward_type=RewardType.RISK_ADJUSTED_RETURN,
                reward_scope=RewardScope.TRAJECTORY_LEVEL,
                raw_reward=0.0
            )
        
        # è¨ˆç®—åŸºç¤å ±é…¬
        base_return = market_data.price_change_30d
        if trajectory.final_recommendation == "SELL":
            base_return = -base_return
        
        # é¢¨éšªèª¿æ•´
        volatility = max(0.01, market_data.volatility_30d)  # é¿å…é™¤é›¶
        risk_adjusted_return = base_return / volatility
        
        # è€ƒæ…®æœ€å¤§å›æ’¤
        max_drawdown = market_data.max_drawdown
        drawdown_penalty = max(0, 1 - abs(max_drawdown) * 2)  # å›æ’¤æ‡²ç½°
        
        # å¤æ™®æ¯”ç‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        sharpe_bonus = 0.0
        if market_data.sharpe_ratio > 0:
            sharpe_bonus = min(0.5, market_data.sharpe_ratio * 0.1)
        
        # æœ€çµ‚é¢¨éšªèª¿æ•´å¾Œçå‹µ
        final_reward = (risk_adjusted_return * drawdown_penalty + sharpe_bonus) / 2
        final_reward = max(-1.0, min(1.0, final_reward))
        
        return RewardMetrics(
            reward_id=f"risk_adj_return_{trajectory.trajectory_id}",
            trajectory_id=trajectory.trajectory_id,
            reward_type=RewardType.RISK_ADJUSTED_RETURN,
            reward_scope=RewardScope.TRAJECTORY_LEVEL,
            raw_reward=risk_adjusted_return,
            weighted_reward=risk_adjusted_return * drawdown_penalty,
            final_reward=final_reward,
            confidence=0.75,
            quality_score=0.8,
            calculation_method="volatility_adjusted_with_drawdown",
            data_sources=["market_price_data", "volatility_data", "drawdown_data"]
        )
    
    def get_reward_type(self) -> RewardType:
        return RewardType.RISK_ADJUSTED_RETURN

class ReasoningQualityCalculator(RewardCalculator):
    """æ¨ç†è³ªé‡çå‹µè¨ˆç®—å™¨"""
    
    async def calculate_reward(self, 
                              trajectory: 'AnalysisTrajectory',
                              market_data: MarketPerformanceData,
                              user_context: Dict[str, Any] = None) -> RewardMetrics:
        """è¨ˆç®—æ¨ç†è³ªé‡çå‹µ"""
        
        if not trajectory.decision_steps:
            return RewardMetrics(
                reward_id=f"reasoning_{trajectory.trajectory_id}",
                trajectory_id=trajectory.trajectory_id,
                reward_type=RewardType.REASONING_QUALITY,
                reward_scope=RewardScope.TRAJECTORY_LEVEL,
                raw_reward=0.0
            )
        
        # è©•ä¼°æ¨ç†è³ªé‡çš„å¤šå€‹ç¶­åº¦
        depth_score = self._evaluate_reasoning_depth(trajectory)
        consistency_score = self._evaluate_consistency(trajectory)
        completeness_score = self._evaluate_completeness(trajectory)
        logical_flow_score = self._evaluate_logical_flow(trajectory)
        
        # åŠ æ¬Šå¹³å‡
        quality_score = (
            depth_score * 0.3 +
            consistency_score * 0.25 +
            completeness_score * 0.25 +
            logical_flow_score * 0.2
        )
        
        return RewardMetrics(
            reward_id=f"reasoning_{trajectory.trajectory_id}",
            trajectory_id=trajectory.trajectory_id,
            reward_type=RewardType.REASONING_QUALITY,
            reward_scope=RewardScope.TRAJECTORY_LEVEL,
            raw_reward=quality_score,
            final_reward=quality_score,
            confidence=0.7,  # æ¨ç†è³ªé‡è©•ä¼°ç›¸å°ä¸»è§€
            quality_score=quality_score,
            calculation_method="multi_dimensional_reasoning_assessment",
            data_sources=["trajectory_reasoning_steps", "decision_logic"]
        )
    
    def _evaluate_reasoning_depth(self, trajectory: 'AnalysisTrajectory') -> float:
        """è©•ä¼°æ¨ç†æ·±åº¦"""
        if not trajectory.decision_steps:
            return 0.0
        
        total_reasoning = 0
        for step in trajectory.decision_steps:
            reasoning_count = len(step.reasoning_process)
            avg_length = sum(len(r) for r in step.reasoning_process) / max(1, reasoning_count)
            total_reasoning += reasoning_count * (avg_length / 50)  # æ¨™æº–åŒ–é•·åº¦
        
        depth_score = total_reasoning / len(trajectory.decision_steps)
        return min(1.0, depth_score / 5.0)
    
    def _evaluate_consistency(self, trajectory: 'AnalysisTrajectory') -> float:
        """è©•ä¼°æ¨ç†ä¸€è‡´æ€§"""
        if len(trajectory.decision_steps) < 2:
            return 1.0
        
        # æª¢æŸ¥ä¿¡å¿ƒåº¦ä¸€è‡´æ€§
        confidences = [step.confidence_score for step in trajectory.decision_steps if step.confidence_score > 0]
        if not confidences:
            return 0.5
        
        consistency = 1.0 - (statistics.stdev(confidences) / max(0.1, statistics.mean(confidences)))
        return max(0.0, min(1.0, consistency))
    
    def _evaluate_completeness(self, trajectory: 'AnalysisTrajectory') -> float:
        """è©•ä¼°æ¨ç†å®Œæ•´æ€§"""
        expected_step_types = {
            'data_collection', 'financial_analysis', 'risk_assessment', 
            'recommendation_logic'
        }
        
        actual_step_types = {step.trajectory_type.value for step in trajectory.decision_steps}
        coverage = len(actual_step_types.intersection(expected_step_types))
        return coverage / len(expected_step_types)
    
    def _evaluate_logical_flow(self, trajectory: 'AnalysisTrajectory') -> float:
        """è©•ä¼°é‚è¼¯æµç¨‹"""
        if len(trajectory.decision_steps) < 2:
            return 1.0
        
        # ç°¡åŒ–çš„é‚è¼¯æµç¨‹è©•ä¼°
        # æª¢æŸ¥æ­¥é©Ÿé–“çš„ä¾è³´é—œä¿‚æ˜¯å¦åˆç†
        flow_score = 0.8  # åŸºç¤åˆ†æ•¸
        
        # æª¢æŸ¥æ˜¯å¦æœ‰æœ€çµ‚æ¨ç†æ­¥é©Ÿ
        has_final_reasoning = any(
            'recommendation' in step.trajectory_type.value 
            for step in trajectory.decision_steps
        )
        
        if has_final_reasoning:
            flow_score += 0.2
        
        return min(1.0, flow_score)
    
    def get_reward_type(self) -> RewardType:
        return RewardType.REASONING_QUALITY

class RULERRewardSystem:
    """RULERçå‹µç³»çµ± - é›¶æ¨£æœ¬çå‹µç”Ÿæˆ"""
    
    def __init__(self, 
                 storage_path: str = None,
                 market_data_provider: Optional[Callable] = None,
                 enable_real_time_tracking: bool = True):
        
        # å­˜å„²é…ç½®
        self.storage_path = Path(storage_path) if storage_path else Path("./art_data/rewards")
        self.storage_path.mkdir(parents=True, exist_ok=True)
        
        # çå‹µè¨ˆç®—å™¨
        self.calculators: Dict[RewardType, RewardCalculator] = {
            RewardType.ACCURACY_REWARD: AccuracyRewardCalculator(),
            RewardType.RETURN_PERFORMANCE: ReturnPerformanceCalculator(),
            RewardType.RISK_ADJUSTED_RETURN: RiskAdjustedReturnCalculator(),
            RewardType.REASONING_QUALITY: ReasoningQualityCalculator(),
        }
        
        # å¸‚å ´æ•¸æ“šè¿½è¹¤
        self.market_data_provider = market_data_provider
        self.enable_real_time_tracking = enable_real_time_tracking
        self.market_performance_cache: Dict[str, MarketPerformanceData] = {}
        
        # çå‹µå­˜å„²
        self.active_rewards: Dict[str, RewardSignal] = {}
        self.completed_rewards: Dict[str, RewardSignal] = {}
        
        # é…ç½®
        self.config = {
            'reward_calculation_interval_hours': 24,
            'market_tracking_period_days': 90,
            'min_confidence_threshold': 0.3,
            'reward_aggregation_method': 'weighted_average',
            'enable_dynamic_weights': True
        }
        
        # æ—¥èªŒè¨˜éŒ„
        self.logger = logging.getLogger(__name__)
        self._setup_logging()
        
        # ç·šç¨‹æ± 
        self.thread_pool = ThreadPoolExecutor(max_workers=4, thread_name_prefix="RULER-Reward")
        
        # çµ±è¨ˆä¿¡æ¯
        self.reward_stats = defaultdict(int)
        self._stats_lock = threading.Lock()
        
        # å‹•æ…‹æ¬Šé‡ç³»çµ±
        self.dynamic_weights = REWARD_TYPE_WEIGHTS.copy()
        self.weight_adjustment_history = deque(maxlen=1000)
        
        self.logger.info(f"RULERRewardSystem initialized with {len(self.calculators)} calculators")
    
    def _setup_logging(self):
        """è¨­ç½®æ—¥èªŒè¨˜éŒ„"""
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - RULER-RewardSystem - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
            self.logger.setLevel(logging.INFO)
    
    async def generate_reward_signal(self,
                                   trajectory: 'AnalysisTrajectory',
                                   user_context: Dict[str, Any] = None,
                                   immediate_evaluation: bool = False,
                                   custom_weights: Dict[RewardType, float] = None) -> RewardSignal:
        """ç”Ÿæˆçå‹µä¿¡è™Ÿ"""
        
        # å‰µå»ºçå‹µä¿¡è™Ÿ
        signal_id = f"reward_{trajectory.trajectory_id}_{int(datetime.now().timestamp())}"
        
        # ç²å–ç”¨æˆ¶æœƒå“¡ç­‰ç´š
        membership_tier = self._determine_membership_tier(user_context)
        membership_multiplier = MEMBERSHIP_MULTIPLIERS.get(membership_tier, 1.0)
        
        reward_signal = RewardSignal(
            signal_id=signal_id,
            trajectory_id=trajectory.trajectory_id,
            user_id=trajectory.user_id,
            stock_id=trajectory.stock_id,
            analyst_type=trajectory.analyst_type,
            membership_tier=membership_tier,
            membership_multiplier=membership_multiplier,
            is_final=immediate_evaluation
        )
        
        # ç²å–å¸‚å ´æ•¸æ“š
        market_data = await self._get_market_performance_data(
            trajectory.stock_id,
            trajectory.start_time,
            immediate=immediate_evaluation
        )
        
        # è¨ˆç®—å„ç¨®çå‹µçµ„æˆ
        reward_weights = custom_weights or self.dynamic_weights
        
        for reward_type, calculator in self.calculators.items():
            if reward_weights.get(reward_type, 0) > 0:
                try:
                    reward_metrics = await calculator.calculate_reward(
                        trajectory, market_data, user_context
                    )
                    
                    # æ‡‰ç”¨å‹•æ…‹æ¬Šé‡
                    weight = reward_weights[reward_type]
                    reward_metrics.weighted_reward = reward_metrics.final_reward * weight
                    
                    reward_signal.add_reward_component(reward_type, reward_metrics)
                    
                except Exception as e:
                    self.logger.error(f"Failed to calculate {reward_type.value} reward: {e}")
        
        # å­˜å„²çå‹µä¿¡è™Ÿ
        if immediate_evaluation:
            self.completed_rewards[signal_id] = reward_signal
        else:
            self.active_rewards[signal_id] = reward_signal
        
        # ä¿å­˜åˆ°å­˜å„²
        await self._save_reward_signal(reward_signal)
        
        # æ›´æ–°çµ±è¨ˆ
        with self._stats_lock:
            self.reward_stats['signals_generated'] += 1
            self.reward_stats[f'{reward_signal.membership_tier.value}_rewards'] += 1
        
        self.logger.info(f"Generated reward signal: {signal_id} (total: {reward_signal.weighted_total_reward:.3f})")
        return reward_signal
    
    def _determine_membership_tier(self, user_context: Dict[str, Any] = None) -> MembershipTier:
        """ç¢ºå®šç”¨æˆ¶æœƒå“¡ç­‰ç´š"""
        if not user_context:
            return MembershipTier.FREE
        
        tier_str = user_context.get('membership_tier', 'FREE').upper()
        
        try:
            return MembershipTier(tier_str.lower())
        except ValueError:
            self.logger.warning(f"Unknown membership tier: {tier_str}, defaulting to FREE")
            return MembershipTier.FREE
    
    async def _get_market_performance_data(self,
                                          stock_id: str,
                                          start_date: str,
                                          immediate: bool = False) -> MarketPerformanceData:
        """ç²å–å¸‚å ´è¡¨ç¾æ•¸æ“š"""
        
        cache_key = f"{stock_id}_{start_date}"
        
        # æª¢æŸ¥ç·©å­˜
        if cache_key in self.market_performance_cache and not immediate:
            cached_data = self.market_performance_cache[cache_key]
            # æª¢æŸ¥ç·©å­˜æ˜¯å¦éæœŸï¼ˆ1å°æ™‚ï¼‰
            last_update = datetime.fromisoformat(cached_data.last_updated)
            if datetime.now() - last_update < timedelta(hours=1):
                return cached_data
        
        # ç²å–æ–°æ•¸æ“š
        if self.market_data_provider:
            try:
                market_data = await self.market_data_provider(stock_id, start_date)
                self.market_performance_cache[cache_key] = market_data
                return market_data
            except Exception as e:
                self.logger.error(f"Failed to get market data for {stock_id}: {e}")
        
        # å‰µå»ºæ¨¡æ“¬æ•¸æ“š
        return await self._create_mock_market_data(stock_id, start_date)
    
    async def _create_mock_market_data(self, stock_id: str, start_date: str) -> MarketPerformanceData:
        """å‰µå»ºæ¨¡æ“¬å¸‚å ´æ•¸æ“š"""
        
        # æ¨¡æ“¬ä¸åŒè‚¡ç¥¨çš„ä¸åŒè¡¨ç¾ç‰¹æ€§
        if stock_id in ['2330', '2454']:  # åŠå°é«”è‚¡
            base_change = np.random.normal(0.02, 0.15)
            volatility = np.random.uniform(0.15, 0.25)
        elif stock_id in ['2881', '2882']:  # é‡‘èè‚¡
            base_change = np.random.normal(0.005, 0.08)
            volatility = np.random.uniform(0.08, 0.15)
        else:
            base_change = np.random.normal(0.01, 0.12)
            volatility = np.random.uniform(0.10, 0.20)
        
        # è¨ˆç®—ä¸åŒæœŸé–“çš„è¡¨ç¾
        price_change_1d = np.random.normal(base_change * 0.05, volatility * 0.3)
        price_change_7d = np.random.normal(base_change * 0.3, volatility * 0.7)
        price_change_30d = np.random.normal(base_change, volatility)
        price_change_90d = np.random.normal(base_change * 2.5, volatility * 1.5)
        
        # å¸‚å ´åŸºæº–ï¼ˆç¨å¾®ä¿å®ˆä¸€äº›ï¼‰
        market_change_1d = price_change_1d * 0.8 + np.random.normal(0, 0.01)
        market_change_7d = price_change_7d * 0.8 + np.random.normal(0, 0.02)
        market_change_30d = price_change_30d * 0.8 + np.random.normal(0, 0.03)
        market_change_90d = price_change_90d * 0.8 + np.random.normal(0, 0.05)
        
        # é¢¨éšªæŒ‡æ¨™
        max_drawdown = -abs(np.random.normal(0.05, 0.03))
        sharpe_ratio = (base_change - 0.01) / volatility  # å‡è¨­ç„¡é¢¨éšªåˆ©ç‡1%
        
        return MarketPerformanceData(
            stock_id=stock_id,
            recommendation_date=start_date,
            recommendation_type="BUY",  # é»˜èª
            recommendation_price=np.random.uniform(50, 800),
            current_price=np.random.uniform(50, 800),
            price_change_1d=price_change_1d,
            price_change_7d=price_change_7d,
            price_change_30d=price_change_30d,
            price_change_90d=price_change_90d,
            market_change_1d=market_change_1d,
            market_change_7d=market_change_7d,
            market_change_30d=market_change_30d,
            market_change_90d=market_change_90d,
            volatility_30d=volatility,
            max_drawdown=max_drawdown,
            sharpe_ratio=sharpe_ratio
        )
    
    async def update_reward_signals(self, trajectory_ids: List[str] = None) -> Dict[str, RewardSignal]:
        """æ›´æ–°çå‹µä¿¡è™Ÿ"""
        
        if trajectory_ids is None:
            trajectory_ids = list(self.active_rewards.keys())
        
        updated_signals = {}
        
        for signal_id in trajectory_ids:
            if signal_id not in self.active_rewards:
                continue
            
            try:
                reward_signal = self.active_rewards[signal_id]
                
                # é‡æ–°ç²å–å¸‚å ´æ•¸æ“š
                market_data = await self._get_market_performance_data(
                    reward_signal.stock_id,
                    reward_signal.generated_at,
                    immediate=True
                )
                
                # é‡æ–°è¨ˆç®—çå‹µ
                # é€™è£¡éœ€è¦trajectoryæ•¸æ“šï¼Œå¯¦éš›å¯¦ç¾ä¸­éœ€è¦å¾è»Œè·¡æ”¶é›†å™¨ç²å–
                # ç‚ºäº†æ¼”ç¤ºï¼Œè·³éé‡æ–°è¨ˆç®—
                
                updated_signals[signal_id] = reward_signal
                
            except Exception as e:
                self.logger.error(f"Failed to update reward signal {signal_id}: {e}")
        
        return updated_signals
    
    async def finalize_reward_signal(self, signal_id: str) -> Optional[RewardSignal]:
        """æœ€çµ‚ç¢ºå®šçå‹µä¿¡è™Ÿ"""
        
        if signal_id not in self.active_rewards:
            return None
        
        reward_signal = self.active_rewards[signal_id]
        
        # æ¨™è¨˜ç‚ºæœ€çµ‚
        reward_signal.is_final = True
        reward_signal.requires_validation = False
        
        # ç§»å‹•åˆ°å®Œæˆçå‹µ
        self.completed_rewards[signal_id] = reward_signal
        del self.active_rewards[signal_id]
        
        # ä¿å­˜
        await self._save_reward_signal(reward_signal)
        
        # æ›´æ–°å‹•æ…‹æ¬Šé‡
        await self._update_dynamic_weights(reward_signal)
        
        return reward_signal
    
    async def _update_dynamic_weights(self, reward_signal: RewardSignal):
        """æ›´æ–°å‹•æ…‹æ¬Šé‡"""
        
        if not self.config['enable_dynamic_weights']:
            return
        
        # åŸºæ–¼çå‹µä¿¡è™Ÿçš„è¡¨ç¾èª¿æ•´æ¬Šé‡
        # é€™æ˜¯ä¸€å€‹ç°¡åŒ–çš„å¯¦ç¾ï¼Œå¯¦éš›ä¸­éœ€è¦æ›´è¤‡é›œçš„å­¸ç¿’ç®—æ³•
        
        total_reward = reward_signal.weighted_total_reward
        
        for reward_type, metrics in reward_signal.reward_components.items():
            if metrics.quality_score > 0.7 and total_reward > 0.5:
                # è¡¨ç¾å¥½çš„çå‹µé¡å‹å¢åŠ æ¬Šé‡
                adjustment = 0.001  # å°å¹…èª¿æ•´
                self.dynamic_weights[reward_type] = min(
                    1.0, 
                    self.dynamic_weights[reward_type] + adjustment
                )
            elif metrics.quality_score < 0.3 or total_reward < -0.3:
                # è¡¨ç¾å·®çš„çå‹µé¡å‹æ¸›å°‘æ¬Šé‡
                adjustment = 0.001
                self.dynamic_weights[reward_type] = max(
                    0.01, 
                    self.dynamic_weights[reward_type] - adjustment
                )
        
        # è¨˜éŒ„æ¬Šé‡èª¿æ•´æ­·å²
        self.weight_adjustment_history.append({
            'timestamp': datetime.now().isoformat(),
            'weights': self.dynamic_weights.copy(),
            'trigger_signal': signal_id
        })
    
    async def get_user_reward_summary(self, 
                                    user_id: str,
                                    time_range_days: int = 30) -> Dict[str, Any]:
        """ç²å–ç”¨æˆ¶çå‹µæ‘˜è¦"""
        
        cutoff_date = datetime.now() - timedelta(days=time_range_days)
        
        user_rewards = []
        total_reward = 0.0
        reward_type_breakdown = defaultdict(float)
        
        # æœé›†ç”¨æˆ¶çš„çå‹µä¿¡è™Ÿ
        for reward_signal in self.completed_rewards.values():
            if reward_signal.user_id == user_id:
                signal_date = datetime.fromisoformat(reward_signal.generated_at)
                if signal_date >= cutoff_date:
                    user_rewards.append(reward_signal)
                    total_reward += reward_signal.weighted_total_reward
                    
                    for reward_type, metrics in reward_signal.reward_components.items():
                        reward_type_breakdown[reward_type.value] += metrics.final_reward
        
        return {
            'user_id': user_id,
            'time_range_days': time_range_days,
            'total_signals': len(user_rewards),
            'total_reward': total_reward,
            'average_reward': total_reward / len(user_rewards) if user_rewards else 0.0,
            'reward_breakdown': dict(reward_type_breakdown),
            'best_performance': max([s.weighted_total_reward for s in user_rewards]) if user_rewards else 0.0,
            'worst_performance': min([s.weighted_total_reward for s in user_rewards]) if user_rewards else 0.0,
            'consistency_score': self._calculate_consistency_score(user_rewards)
        }
    
    def _calculate_consistency_score(self, reward_signals: List[RewardSignal]) -> float:
        """è¨ˆç®—ä¸€è‡´æ€§åˆ†æ•¸"""
        if len(reward_signals) < 2:
            return 1.0
        
        rewards = [signal.weighted_total_reward for signal in reward_signals]
        mean_reward = statistics.mean(rewards)
        std_dev = statistics.stdev(rewards)
        
        # ä¸€è‡´æ€§åˆ†æ•¸ï¼šæ¨™æº–å·®è¶Šå°ï¼Œä¸€è‡´æ€§è¶Šé«˜
        consistency = max(0.0, 1.0 - (std_dev / max(0.1, abs(mean_reward))))
        return consistency
    
    async def _save_reward_signal(self, reward_signal: RewardSignal):
        """ä¿å­˜çå‹µä¿¡è™Ÿ"""
        try:
            reward_file = self.storage_path / f"reward_{reward_signal.signal_id}.json"
            
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(
                self.thread_pool,
                self._sync_save_reward_signal,
                reward_file,
                reward_signal.to_dict()
            )
            
        except Exception as e:
            self.logger.error(f"Failed to save reward signal {reward_signal.signal_id}: {e}")
    
    def _sync_save_reward_signal(self, file_path: Path, reward_data: Dict[str, Any]):
        """åŒæ­¥ä¿å­˜çå‹µä¿¡è™Ÿ"""
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(reward_data, f, ensure_ascii=False, indent=2)
    
    def get_system_statistics(self) -> Dict[str, Any]:
        """ç²å–ç³»çµ±çµ±è¨ˆä¿¡æ¯"""
        with self._stats_lock:
            stats = self.reward_stats.copy()
        
        stats.update({
            'active_rewards': len(self.active_rewards),
            'completed_rewards': len(self.completed_rewards),
            'available_calculators': list(self.calculators.keys()),
            'current_weights': self.dynamic_weights,
            'market_cache_size': len(self.market_performance_cache)
        })
        
        return stats
    
    async def cleanup(self):
        """æ¸…ç†è³‡æº"""
        # æœ€çµ‚ç¢ºå®šæ‰€æœ‰æ´»èºçå‹µ
        for signal_id in list(self.active_rewards.keys()):
            await self.finalize_reward_signal(signal_id)
        
        # é—œé–‰ç·šç¨‹æ± 
        self.thread_pool.shutdown(wait=True)
        
        self.logger.info("RULERRewardSystem cleanup completed")

# å·¥å» å‡½æ•¸
async def create_ruler_reward_system(
    storage_path: str = None,
    market_data_provider: Optional[Callable] = None,
    enable_real_time_tracking: bool = True,
    custom_calculators: Dict[RewardType, RewardCalculator] = None
) -> RULERRewardSystem:
    """å‰µå»ºRULERçå‹µç³»çµ±å¯¦ä¾‹"""
    
    system = RULERRewardSystem(
        storage_path=storage_path,
        market_data_provider=market_data_provider,
        enable_real_time_tracking=enable_real_time_tracking
    )
    
    # æ·»åŠ è‡ªå®šç¾©è¨ˆç®—å™¨
    if custom_calculators:
        system.calculators.update(custom_calculators)
    
    return system


# è‡ªå‹•æª¢æ¸¬å’Œåˆ‡æ›åˆ° TradingAgents ç›®éŒ„
def ensure_tradingagents_directory():
    """ç¢ºä¿ç•¶å‰å·¥ä½œç›®éŒ„åœ¨ TradingAgents/ ä¸‹ï¼Œä»¥æ­£ç¢ºè¨ªå•é…ç½®æ–‡ä»¶"""
    current_dir = Path.cwd()
    
    # å¦‚æœç•¶å‰ç›®éŒ„æ˜¯ TradingAgents çš„çˆ¶ç›®éŒ„ï¼Œåˆ‡æ›åˆ° TradingAgents
    if (current_dir / 'TradingAgents').exists():
        os.chdir(current_dir / 'TradingAgents')
        print(f"[DIR] è‡ªå‹•åˆ‡æ›å·¥ä½œç›®éŒ„åˆ°: {Path.cwd()}")
    
    # é©—è­‰å¿…è¦çš„ç›®éŒ„å­˜åœ¨
    required_dirs = ['configs', 'training', 'tradingagents']
    missing_dirs = [d for d in required_dirs if not Path(d).exists()]
    
    if missing_dirs:
        raise FileNotFoundError(f"ç¼ºå°‘å¿…è¦ç›®éŒ„: {missing_dirs}. è«‹ç¢ºä¿å¾ TradingAgents/ ç›®éŒ„åŸ·è¡Œæ­¤è…³æœ¬")

# ç›®éŒ„æª¢æŸ¥å‡½æ•¸å·²æº–å‚™å¥½ï¼Œä½†ä¸åœ¨æ¨¡çµ„å°å…¥æ™‚è‡ªå‹•åŸ·è¡Œ
# åªåœ¨éœ€è¦æ™‚æ‰‹å‹•èª¿ç”¨ ensure_tradingagents_directory()

if __name__ == "__main__":
    # æ¸¬è©¦è…³æœ¬
    import asyncio
    
    async def test_ruler_reward_system():
        print("ğŸ¯ æ¸¬è©¦RULERçå‹µç³»çµ±")
        
        # å‰µå»ºçå‹µç³»çµ±
        reward_system = await create_ruler_reward_system("./test_rewards")
        
        # å‰µå»ºæ¨¡æ“¬è»Œè·¡
        class MockTrajectory:
            def __init__(self):
                self.trajectory_id = "test_trajectory_123"
                self.stock_id = "2330"
                self.user_id = "test_user"
                self.analyst_type = "fundamentals_analyst"
                self.start_time = datetime.now().isoformat()
                self.final_recommendation = "BUY"
                self.final_confidence = 0.85
                self.decision_steps = []
        
        trajectory = MockTrajectory()
        
        # ç”Ÿæˆçå‹µä¿¡è™Ÿ
        user_context = {
            'user_id': 'test_user',
            'membership_tier': 'GOLD'
        }
        
        reward_signal = await reward_system.generate_reward_signal(
            trajectory=trajectory,
            user_context=user_context,
            immediate_evaluation=True
        )
        
        print(f"âœ… ç”Ÿæˆçå‹µä¿¡è™Ÿ: {reward_signal.signal_id}")
        print(f"ğŸ“Š ç¸½çå‹µ: {reward_signal.weighted_total_reward:.3f}")
        print(f"ğŸ… æœƒå“¡ç­‰ç´š: {reward_signal.membership_tier.value}")
        
        # çå‹µçµ„æˆåˆ†æ
        print(f"\nğŸ” çå‹µçµ„æˆ:")
        for reward_type, metrics in reward_signal.reward_components.items():
            print(f"  â€¢ {reward_type.value}: {metrics.final_reward:.3f} (ä¿¡å¿ƒ: {metrics.confidence:.2f})")
        
        # ç²å–ç”¨æˆ¶æ‘˜è¦
        summary = await reward_system.get_user_reward_summary('test_user')
        print(f"\nğŸ“ˆ ç”¨æˆ¶æ‘˜è¦: ç¸½çå‹µ {summary['total_reward']:.3f}")
        
        # ç³»çµ±çµ±è¨ˆ
        stats = reward_system.get_system_statistics()
        print(f"\nğŸ“Š ç³»çµ±çµ±è¨ˆ: {stats['completed_rewards']} å€‹å·²å®Œæˆçå‹µ")
        
        # æ¸…ç†
        await reward_system.cleanup()
        
        print("âœ… RULERçå‹µç³»çµ±æ¸¬è©¦å®Œæˆ")
    
    asyncio.run(test_ruler_reward_system())