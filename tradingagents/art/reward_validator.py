#!/usr/bin/env python3
"""
RewardValidator - çå‹µé©—è­‰å™¨
å¤©å·¥ (TianGong) - çå‹µä¿¡è™Ÿé©—è­‰èˆ‡å„ªåŒ–ç³»çµ±

æ­¤æ¨¡çµ„æä¾›ï¼š
1. å¯¦æ™‚çå‹µä¿¡è™Ÿé©—è­‰
2. çå‹µå‡½æ•¸è‡ªå‹•èª¿å„ªæ©Ÿåˆ¶
3. A/Bæ¸¬è©¦æ¡†æ¶æ”¯æ´
4. æ­·å²ç¸¾æ•ˆè¿½è¹¤å’Œåˆ†æ
5. çå‹µæ¨¡å‹å“è³ªè©•ä¼°
6. ç•°å¸¸æª¢æ¸¬å’Œä¿®æ­£æ©Ÿåˆ¶
"""

from typing import Dict, Any, List, Optional, Tuple, Union, Callable
from dataclasses import dataclass, asdict, field
from datetime import datetime, timedelta
from enum import Enum
import json
import logging
import asyncio
import numpy as np
import pandas as pd
from pathlib import Path
import hashlib
import math
import statistics
from concurrent.futures import ThreadPoolExecutor
import threading
from collections import defaultdict, deque
from abc import ABC, abstractmethod
import warnings
import random

# Import reward system components
try:
    from .ruler_reward_system import RewardSignal, RewardMetrics, RewardType, MembershipTier
    REWARD_SYSTEM_AVAILABLE = True
except ImportError:
    REWARD_SYSTEM_AVAILABLE = False

class ValidationType(Enum):
    """é©—è­‰é¡å‹"""
    RANGE_CHECK = "range_check"                 # ç¯„åœæª¢æŸ¥
    CONSISTENCY_CHECK = "consistency_check"     # ä¸€è‡´æ€§æª¢æŸ¥
    OUTLIER_DETECTION = "outlier_detection"     # ç•°å¸¸å€¼æª¢æ¸¬
    CORRELATION_ANALYSIS = "correlation_analysis" # ç›¸é—œæ€§åˆ†æ
    TEMPORAL_VALIDATION = "temporal_validation"  # æ™‚é–“åºåˆ—é©—è­‰
    CROSS_VALIDATION = "cross_validation"       # äº¤å‰é©—è­‰
    PERFORMANCE_VALIDATION = "performance_validation" # ç¸¾æ•ˆé©—è­‰
    BIAS_DETECTION = "bias_detection"           # åå·®æª¢æ¸¬

class ValidationStatus(Enum):
    """é©—è­‰ç‹€æ…‹"""
    PENDING = "pending"       # ç­‰å¾…é©—è­‰
    PASSED = "passed"         # é©—è­‰é€šé
    FAILED = "failed"         # é©—è­‰å¤±æ•—
    WARNING = "warning"       # è­¦å‘Š
    NEEDS_REVIEW = "needs_review" # éœ€è¦å¯©æŸ¥

class OptimizationStrategy(Enum):
    """å„ªåŒ–ç­–ç•¥"""
    GRADIENT_BASED = "gradient_based"           # åŸºæ–¼æ¢¯åº¦çš„å„ªåŒ–
    EVOLUTIONARY = "evolutionary"               # æ¼”åŒ–ç®—æ³•
    BAYESIAN = "bayesian"                       # è²è‘‰æ–¯å„ªåŒ–
    REINFORCEMENT_LEARNING = "reinforcement_learning" # å¼·åŒ–å­¸ç¿’
    A_B_TESTING = "a_b_testing"                # A/Bæ¸¬è©¦
    ENSEMBLE = "ensemble"                       # é›†æˆæ–¹æ³•

@dataclass
class ValidationRule:
    """é©—è­‰è¦å‰‡"""
    rule_id: str
    validation_type: ValidationType
    rule_description: str
    
    # è¦å‰‡åƒæ•¸
    min_value: Optional[float] = None
    max_value: Optional[float] = None
    threshold: Optional[float] = None
    tolerance: Optional[float] = None
    
    # åš´é‡æ€§å’Œæ¬Šé‡
    severity: str = "medium"  # low, medium, high, critical
    weight: float = 1.0
    
    # é©ç”¨æ¢ä»¶
    applicable_reward_types: List[RewardType] = field(default_factory=list)
    applicable_membership_tiers: List[MembershipTier] = field(default_factory=list)
    
    # è‡ªå‹•ä¿®æ­£
    auto_correction_enabled: bool = False
    correction_function: Optional[str] = None
    
    def is_applicable(self, reward_signal: 'RewardSignal') -> bool:
        """æª¢æŸ¥è¦å‰‡æ˜¯å¦é©ç”¨æ–¼çµ¦å®šçš„çå‹µä¿¡è™Ÿ"""
        # æª¢æŸ¥çå‹µé¡å‹
        if self.applicable_reward_types:
            signal_reward_types = set(reward_signal.reward_components.keys())
            rule_reward_types = set(self.applicable_reward_types)
            if not signal_reward_types.intersection(rule_reward_types):
                return False
        
        # æª¢æŸ¥æœƒå“¡ç­‰ç´š
        if self.applicable_membership_tiers:
            if reward_signal.membership_tier not in self.applicable_membership_tiers:
                return False
        
        return True
    
    def to_dict(self) -> Dict[str, Any]:
        result = asdict(self)
        result['validation_type'] = self.validation_type.value
        result['applicable_reward_types'] = [rt.value for rt in self.applicable_reward_types]
        result['applicable_membership_tiers'] = [mt.value for mt in self.applicable_membership_tiers]
        return result

@dataclass
class ValidationResult:
    """é©—è­‰çµæœ"""
    result_id: str
    signal_id: str
    rule_id: str
    validation_type: ValidationType
    
    # é©—è­‰ç‹€æ…‹
    status: ValidationStatus = ValidationStatus.PENDING
    confidence: float = 0.0
    
    # çµæœè©³æƒ…
    original_value: Any = None
    expected_value: Any = None
    corrected_value: Any = None
    deviation: float = 0.0
    
    # è©³ç´°ä¿¡æ¯
    validation_details: Dict[str, Any] = field(default_factory=dict)
    error_message: Optional[str] = None
    recommendation: Optional[str] = None
    
    # æ™‚é–“ä¿¡æ¯
    validation_timestamp: str = field(default_factory=lambda: datetime.now().isoformat())
    
    def to_dict(self) -> Dict[str, Any]:
        result = asdict(self)
        result['validation_type'] = self.validation_type.value
        result['status'] = self.status.value
        return result

@dataclass
class ValidationMetrics:
    """é©—è­‰æŒ‡æ¨™"""
    validator_id: str
    
    # åŸºæœ¬çµ±è¨ˆ
    total_validations: int = 0
    passed_validations: int = 0
    failed_validations: int = 0
    warning_validations: int = 0
    
    # æº–ç¢ºç‡æŒ‡æ¨™
    validation_accuracy: float = 0.0
    false_positive_rate: float = 0.0
    false_negative_rate: float = 0.0
    
    # æ•ˆç‡æŒ‡æ¨™
    avg_validation_time_ms: float = 0.0
    validation_throughput: float = 0.0
    
    # è³ªé‡æŒ‡æ¨™
    avg_confidence: float = 0.0
    consistency_score: float = 0.0
    
    # æ›´æ–°æ™‚é–“
    last_updated: str = field(default_factory=lambda: datetime.now().isoformat())
    
    def calculate_pass_rate(self) -> float:
        """è¨ˆç®—é€šéç‡"""
        if self.total_validations == 0:
            return 0.0
        return self.passed_validations / self.total_validations
    
    def calculate_quality_score(self) -> float:
        """è¨ˆç®—è³ªé‡åˆ†æ•¸"""
        accuracy_weight = 0.4
        consistency_weight = 0.3
        confidence_weight = 0.3
        
        return (
            self.validation_accuracy * accuracy_weight +
            self.consistency_score * consistency_weight +
            self.avg_confidence * confidence_weight
        )
    
    def to_dict(self) -> Dict[str, Any]:
        result = asdict(self)
        result['pass_rate'] = self.calculate_pass_rate()
        result['quality_score'] = self.calculate_quality_score()
        return result

@dataclass
class RewardModelConfig:
    """çå‹µæ¨¡å‹é…ç½®"""
    config_id: str
    model_version: str
    
    # æ¬Šé‡é…ç½®
    reward_weights: Dict[str, float] = field(default_factory=dict)
    membership_multipliers: Dict[str, float] = field(default_factory=dict)
    
    # é©—è­‰é…ç½®
    validation_rules: List[ValidationRule] = field(default_factory=list)
    
    # å„ªåŒ–é…ç½®
    optimization_strategy: OptimizationStrategy = OptimizationStrategy.A_B_TESTING
    optimization_target: str = "total_reward_quality"
    
    # A/Bæ¸¬è©¦é…ç½®
    ab_test_enabled: bool = False
    ab_test_split_ratio: float = 0.5
    ab_test_duration_days: int = 30
    
    # æ€§èƒ½åŸºæº–
    performance_baseline: Dict[str, float] = field(default_factory=dict)
    
    # å…ƒæ•¸æ“š
    created_at: str = field(default_factory=lambda: datetime.now().isoformat())
    created_by: str = "system"
    description: str = ""
    
    def to_dict(self) -> Dict[str, Any]:
        result = asdict(self)
        result['optimization_strategy'] = self.optimization_strategy.value
        result['validation_rules'] = [rule.to_dict() for rule in self.validation_rules]
        return result

class ValidationEngine(ABC):
    """é©—è­‰å¼•æ“åŸºé¡"""
    
    @abstractmethod
    async def validate(self, 
                      reward_signal: 'RewardSignal',
                      validation_rule: ValidationRule,
                      historical_data: List['RewardSignal'] = None) -> ValidationResult:
        """åŸ·è¡Œé©—è­‰"""
        pass
    
    @abstractmethod
    def get_validation_type(self) -> ValidationType:
        """ç²å–é©—è­‰é¡å‹"""
        pass

class RangeCheckEngine(ValidationEngine):
    """ç¯„åœæª¢æŸ¥å¼•æ“"""
    
    async def validate(self, 
                      reward_signal: 'RewardSignal',
                      validation_rule: ValidationRule,
                      historical_data: List['RewardSignal'] = None) -> ValidationResult:
        """åŸ·è¡Œç¯„åœæª¢æŸ¥"""
        
        result_id = f"range_check_{reward_signal.signal_id}_{validation_rule.rule_id}"
        
        # ç²å–è¦æª¢æŸ¥çš„å€¼
        value_to_check = reward_signal.weighted_total_reward
        
        # åŸ·è¡Œç¯„åœæª¢æŸ¥
        is_valid = True
        deviation = 0.0
        error_message = None
        
        if validation_rule.min_value is not None and value_to_check < validation_rule.min_value:
            is_valid = False
            deviation = abs(value_to_check - validation_rule.min_value)
            error_message = f"Value {value_to_check:.3f} is below minimum {validation_rule.min_value:.3f}"
        
        if validation_rule.max_value is not None and value_to_check > validation_rule.max_value:
            is_valid = False
            deviation = abs(value_to_check - validation_rule.max_value)
            error_message = f"Value {value_to_check:.3f} is above maximum {validation_rule.max_value:.3f}"
        
        # ç¢ºå®šç‹€æ…‹
        if is_valid:
            status = ValidationStatus.PASSED
            confidence = 0.9
        else:
            # æ ¹æ“šåå·®ç¨‹åº¦æ±ºå®šç‹€æ…‹
            if deviation > (validation_rule.tolerance or 0.1):
                status = ValidationStatus.FAILED
                confidence = 0.8
            else:
                status = ValidationStatus.WARNING
                confidence = 0.6
        
        return ValidationResult(
            result_id=result_id,
            signal_id=reward_signal.signal_id,
            rule_id=validation_rule.rule_id,
            validation_type=self.get_validation_type(),
            status=status,
            confidence=confidence,
            original_value=value_to_check,
            expected_value=(validation_rule.min_value, validation_rule.max_value),
            deviation=deviation,
            error_message=error_message,
            validation_details={
                'checked_field': 'weighted_total_reward',
                'min_threshold': validation_rule.min_value,
                'max_threshold': validation_rule.max_value,
                'tolerance': validation_rule.tolerance
            }
        )
    
    def get_validation_type(self) -> ValidationType:
        return ValidationType.RANGE_CHECK

class OutlierDetectionEngine(ValidationEngine):
    """ç•°å¸¸å€¼æª¢æ¸¬å¼•æ“"""
    
    async def validate(self, 
                      reward_signal: 'RewardSignal',
                      validation_rule: ValidationRule,
                      historical_data: List['RewardSignal'] = None) -> ValidationResult:
        """åŸ·è¡Œç•°å¸¸å€¼æª¢æ¸¬"""
        
        result_id = f"outlier_detection_{reward_signal.signal_id}_{validation_rule.rule_id}"
        
        if not historical_data or len(historical_data) < 10:
            return ValidationResult(
                result_id=result_id,
                signal_id=reward_signal.signal_id,
                rule_id=validation_rule.rule_id,
                validation_type=self.get_validation_type(),
                status=ValidationStatus.WARNING,
                confidence=0.3,
                error_message="Insufficient historical data for outlier detection"
            )
        
        # æå–æ­·å²çå‹µå€¼
        historical_rewards = [signal.weighted_total_reward for signal in historical_data]
        current_reward = reward_signal.weighted_total_reward
        
        # è¨ˆç®—çµ±è¨ˆæŒ‡æ¨™
        mean_reward = statistics.mean(historical_rewards)
        std_dev = statistics.stdev(historical_rewards) if len(historical_rewards) > 1 else 0.1
        
        # Z-scoreæª¢æ¸¬
        z_score = abs(current_reward - mean_reward) / std_dev if std_dev > 0 else 0
        outlier_threshold = validation_rule.threshold or 2.5
        
        # IQRæª¢æ¸¬
        q1 = np.percentile(historical_rewards, 25)
        q3 = np.percentile(historical_rewards, 75)
        iqr = q3 - q1
        lower_bound = q1 - 1.5 * iqr
        upper_bound = q3 + 1.5 * iqr
        
        is_iqr_outlier = current_reward < lower_bound or current_reward > upper_bound
        is_zscore_outlier = z_score > outlier_threshold
        
        # ç¶œåˆåˆ¤å®š
        if is_zscore_outlier and is_iqr_outlier:
            status = ValidationStatus.FAILED
            confidence = 0.9
            error_message = f"Strong outlier detected (Z-score: {z_score:.2f}, IQR outlier: {is_iqr_outlier})"
        elif is_zscore_outlier or is_iqr_outlier:
            status = ValidationStatus.WARNING
            confidence = 0.7
            error_message = f"Potential outlier detected (Z-score: {z_score:.2f})"
        else:
            status = ValidationStatus.PASSED
            confidence = 0.8
            error_message = None
        
        return ValidationResult(
            result_id=result_id,
            signal_id=reward_signal.signal_id,
            rule_id=validation_rule.rule_id,
            validation_type=self.get_validation_type(),
            status=status,
            confidence=confidence,
            original_value=current_reward,
            expected_value=(lower_bound, upper_bound),
            deviation=z_score,
            error_message=error_message,
            validation_details={
                'z_score': z_score,
                'historical_mean': mean_reward,
                'historical_std': std_dev,
                'iqr_bounds': (lower_bound, upper_bound),
                'outlier_threshold': outlier_threshold,
                'sample_size': len(historical_rewards)
            }
        )
    
    def get_validation_type(self) -> ValidationType:
        return ValidationType.OUTLIER_DETECTION

class ConsistencyCheckEngine(ValidationEngine):
    """ä¸€è‡´æ€§æª¢æŸ¥å¼•æ“"""
    
    async def validate(self, 
                      reward_signal: 'RewardSignal',
                      validation_rule: ValidationRule,
                      historical_data: List['RewardSignal'] = None) -> ValidationResult:
        """åŸ·è¡Œä¸€è‡´æ€§æª¢æŸ¥"""
        
        result_id = f"consistency_check_{reward_signal.signal_id}_{validation_rule.rule_id}"
        
        if not historical_data:
            return ValidationResult(
                result_id=result_id,
                signal_id=reward_signal.signal_id,
                rule_id=validation_rule.rule_id,
                validation_type=self.get_validation_type(),
                status=ValidationStatus.WARNING,
                confidence=0.5,
                error_message="No historical data available for consistency check"
            )
        
        # æª¢æŸ¥åŒé¡å‹çå‹µçš„ä¸€è‡´æ€§
        consistency_scores = []
        
        for reward_type in reward_signal.reward_components.keys():
            current_metrics = reward_signal.reward_components[reward_type]
            
            # æ”¶é›†æ­·å²åŒé¡å‹çå‹µ
            historical_values = []
            for historical_signal in historical_data:
                if reward_type in historical_signal.reward_components:
                    historical_values.append(
                        historical_signal.reward_components[reward_type].final_reward
                    )
            
            if len(historical_values) >= 3:
                # è¨ˆç®—ä¸€è‡´æ€§åˆ†æ•¸
                historical_mean = statistics.mean(historical_values)
                historical_std = statistics.stdev(historical_values)
                current_value = current_metrics.final_reward
                
                if historical_std > 0:
                    consistency_score = max(0, 1 - abs(current_value - historical_mean) / historical_std)
                else:
                    consistency_score = 1.0 if abs(current_value - historical_mean) < 0.01 else 0.0
                
                consistency_scores.append(consistency_score)
        
        if not consistency_scores:
            return ValidationResult(
                result_id=result_id,
                signal_id=reward_signal.signal_id,
                rule_id=validation_rule.rule_id,
                validation_type=self.get_validation_type(),
                status=ValidationStatus.WARNING,
                confidence=0.3,
                error_message="Insufficient data for consistency analysis"
            )
        
        # ç¶œåˆä¸€è‡´æ€§è©•åˆ†
        avg_consistency = statistics.mean(consistency_scores)
        consistency_threshold = validation_rule.threshold or 0.7
        
        if avg_consistency >= consistency_threshold:
            status = ValidationStatus.PASSED
            confidence = 0.8
            error_message = None
        elif avg_consistency >= consistency_threshold * 0.7:
            status = ValidationStatus.WARNING
            confidence = 0.6
            error_message = f"Moderate consistency concern (score: {avg_consistency:.2f})"
        else:
            status = ValidationStatus.FAILED
            confidence = 0.9
            error_message = f"Low consistency detected (score: {avg_consistency:.2f})"
        
        return ValidationResult(
            result_id=result_id,
            signal_id=reward_signal.signal_id,
            rule_id=validation_rule.rule_id,
            validation_type=self.get_validation_type(),
            status=status,
            confidence=confidence,
            original_value=avg_consistency,
            expected_value=consistency_threshold,
            deviation=abs(avg_consistency - consistency_threshold),
            error_message=error_message,
            validation_details={
                'consistency_scores': consistency_scores,
                'average_consistency': avg_consistency,
                'threshold': consistency_threshold,
                'analyzed_reward_types': len(consistency_scores)
            }
        )
    
    def get_validation_type(self) -> ValidationType:
        return ValidationType.CONSISTENCY_CHECK

class RewardValidator:
    """çå‹µé©—è­‰å™¨ - ä¸»è¦æ§åˆ¶å™¨"""
    
    def __init__(self, 
                 storage_path: str = None,
                 model_config: RewardModelConfig = None,
                 enable_auto_correction: bool = True,
                 enable_ab_testing: bool = True):
        
        # å­˜å„²é…ç½®
        self.storage_path = Path(storage_path) if storage_path else Path("./art_data/validation")
        self.storage_path.mkdir(parents=True, exist_ok=True)
        
        # æ¨¡å‹é…ç½®
        self.model_config = model_config or self._create_default_config()
        
        # é©—è­‰å¼•æ“
        self.validation_engines: Dict[ValidationType, ValidationEngine] = {
            ValidationType.RANGE_CHECK: RangeCheckEngine(),
            ValidationType.OUTLIER_DETECTION: OutlierDetectionEngine(),
            ValidationType.CONSISTENCY_CHECK: ConsistencyCheckEngine()
        }
        
        # é©—è­‰æ­·å²
        self.validation_history: Dict[str, List[ValidationResult]] = defaultdict(list)
        
        # A/Bæ¸¬è©¦
        self.enable_ab_testing = enable_ab_testing
        self.ab_test_groups: Dict[str, str] = {}  # signal_id -> group_name
        self.ab_test_results: Dict[str, Dict[str, Any]] = defaultdict(dict)
        
        # è‡ªå‹•ä¿®æ­£
        self.enable_auto_correction = enable_auto_correction
        self.correction_functions: Dict[str, Callable] = {}
        
        # æ€§èƒ½è¿½è¹¤
        self.validation_metrics = ValidationMetrics(validator_id="main_validator")
        
        # æ—¥èªŒè¨˜éŒ„
        self.logger = logging.getLogger(__name__)
        self._setup_logging()
        
        # ç·šç¨‹æ± 
        self.thread_pool = ThreadPoolExecutor(max_workers=4, thread_name_prefix="Reward-Validator")
        
        # çµ±è¨ˆä¿¡æ¯
        self.validation_stats = defaultdict(int)
        self._stats_lock = threading.Lock()
        
        self.logger.info(f"RewardValidator initialized with {len(self.validation_engines)} engines")
    
    def _setup_logging(self):
        """è¨­ç½®æ—¥èªŒè¨˜éŒ„"""
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - RewardValidator - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
            self.logger.setLevel(logging.INFO)
    
    def _create_default_config(self) -> RewardModelConfig:
        """å‰µå»ºé»˜èªé…ç½®"""
        
        # å‰µå»ºé»˜èªé©—è­‰è¦å‰‡
        validation_rules = [
            # ç¯„åœæª¢æŸ¥è¦å‰‡
            ValidationRule(
                rule_id="reward_range_check",
                validation_type=ValidationType.RANGE_CHECK,
                rule_description="Check if reward is within acceptable range",
                min_value=-1.0,
                max_value=1.0,
                tolerance=0.05,
                severity="high",
                weight=0.9,
                auto_correction_enabled=True
            ),
            
            # ç•°å¸¸å€¼æª¢æ¸¬è¦å‰‡
            ValidationRule(
                rule_id="outlier_detection",
                validation_type=ValidationType.OUTLIER_DETECTION,
                rule_description="Detect reward outliers using statistical methods",
                threshold=2.5,
                tolerance=0.5,
                severity="medium",
                weight=0.7
            ),
            
            # ä¸€è‡´æ€§æª¢æŸ¥è¦å‰‡
            ValidationRule(
                rule_id="consistency_check",
                validation_type=ValidationType.CONSISTENCY_CHECK,
                rule_description="Check reward consistency across similar scenarios",
                threshold=0.7,
                tolerance=0.1,
                severity="medium",
                weight=0.6
            )
        ]
        
        return RewardModelConfig(
            config_id="default_config_v1.0",
            model_version="1.0.0",
            validation_rules=validation_rules,
            optimization_strategy=OptimizationStrategy.A_B_TESTING,
            ab_test_enabled=True,
            ab_test_split_ratio=0.5,
            ab_test_duration_days=30,
            description="Default reward validation configuration"
        )
    
    async def validate_reward_signal(self,
                                   reward_signal: 'RewardSignal',
                                   historical_context: List['RewardSignal'] = None,
                                   custom_rules: List[ValidationRule] = None) -> List[ValidationResult]:
        """é©—è­‰çå‹µä¿¡è™Ÿ"""
        
        validation_results = []
        
        # ç²å–é©ç”¨çš„é©—è­‰è¦å‰‡
        rules_to_apply = custom_rules or self.model_config.validation_rules
        applicable_rules = [
            rule for rule in rules_to_apply 
            if rule.is_applicable(reward_signal)
        ]
        
        # ä¸¦è¡ŒåŸ·è¡Œé©—è­‰
        validation_tasks = []
        
        for rule in applicable_rules:
            if rule.validation_type in self.validation_engines:
                engine = self.validation_engines[rule.validation_type]
                task = engine.validate(reward_signal, rule, historical_context)
                validation_tasks.append(task)
        
        if validation_tasks:
            results = await asyncio.gather(*validation_tasks, return_exceptions=True)
            
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    self.logger.error(f"Validation failed for rule {applicable_rules[i].rule_id}: {result}")
                    # å‰µå»ºå¤±æ•—çµæœ
                    failed_result = ValidationResult(
                        result_id=f"failed_{reward_signal.signal_id}_{applicable_rules[i].rule_id}",
                        signal_id=reward_signal.signal_id,
                        rule_id=applicable_rules[i].rule_id,
                        validation_type=applicable_rules[i].validation_type,
                        status=ValidationStatus.FAILED,
                        confidence=0.0,
                        error_message=str(result)
                    )
                    validation_results.append(failed_result)
                else:
                    validation_results.append(result)
        
        # è¨˜éŒ„é©—è­‰æ­·å²
        self.validation_history[reward_signal.signal_id] = validation_results
        
        # æ›´æ–°çµ±è¨ˆ
        self._update_validation_statistics(validation_results)
        
        # è‡ªå‹•ä¿®æ­£ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
        if self.enable_auto_correction:
            corrected_signal = await self._apply_auto_corrections(reward_signal, validation_results)
            if corrected_signal != reward_signal:
                self.logger.info(f"Applied auto-corrections to signal {reward_signal.signal_id}")
        
        # A/Bæ¸¬è©¦åˆ†çµ„ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
        if self.enable_ab_testing and self.model_config.ab_test_enabled:
            await self._assign_ab_test_group(reward_signal, validation_results)
        
        self.logger.info(f"Validated signal {reward_signal.signal_id}: {len(validation_results)} checks completed")
        return validation_results
    
    def _update_validation_statistics(self, results: List[ValidationResult]):
        """æ›´æ–°é©—è­‰çµ±è¨ˆ"""
        with self._stats_lock:
            for result in results:
                self.validation_stats['total_validations'] += 1
                
                if result.status == ValidationStatus.PASSED:
                    self.validation_stats['passed_validations'] += 1
                elif result.status == ValidationStatus.FAILED:
                    self.validation_stats['failed_validations'] += 1
                elif result.status == ValidationStatus.WARNING:
                    self.validation_stats['warning_validations'] += 1
        
        # æ›´æ–°ä¸»è¦æŒ‡æ¨™
        self.validation_metrics.total_validations = self.validation_stats['total_validations']
        self.validation_metrics.passed_validations = self.validation_stats['passed_validations']
        self.validation_metrics.failed_validations = self.validation_stats['failed_validations']
        self.validation_metrics.warning_validations = self.validation_stats['warning_validations']
        
        # è¨ˆç®—æº–ç¢ºç‡
        if self.validation_metrics.total_validations > 0:
            self.validation_metrics.validation_accuracy = (
                self.validation_metrics.passed_validations / 
                self.validation_metrics.total_validations
            )
    
    async def _apply_auto_corrections(self,
                                    reward_signal: 'RewardSignal',
                                    validation_results: List[ValidationResult]) -> 'RewardSignal':
        """æ‡‰ç”¨è‡ªå‹•ä¿®æ­£"""
        
        corrected_signal = reward_signal  # å‰µå»ºå‰¯æœ¬æœƒæ›´å¥½ï¼Œä½†ç‚ºäº†ç°¡åŒ–...
        
        for result in validation_results:
            if result.status == ValidationStatus.FAILED and result.corrected_value is not None:
                # æ‡‰ç”¨ä¿®æ­£ï¼ˆé€™è£¡éœ€è¦æ ¹æ“šå…·é«”æƒ…æ³å¯¦ç¾ï¼‰
                if result.validation_type == ValidationType.RANGE_CHECK:
                    # ç¯„åœä¿®æ­£
                    if result.original_value < -1.0:
                        corrected_signal.weighted_total_reward = -1.0
                    elif result.original_value > 1.0:
                        corrected_signal.weighted_total_reward = 1.0
                    
                    self.logger.info(f"Applied range correction to signal {reward_signal.signal_id}")
        
        return corrected_signal
    
    async def _assign_ab_test_group(self,
                                   reward_signal: 'RewardSignal',
                                   validation_results: List[ValidationResult]):
        """åˆ†é…A/Bæ¸¬è©¦çµ„"""
        
        # ç°¡å–®çš„éš¨æ©Ÿåˆ†çµ„
        if random.random() < self.model_config.ab_test_split_ratio:
            group = "control"
        else:
            group = "treatment"
        
        self.ab_test_groups[reward_signal.signal_id] = group
        
        # è¨˜éŒ„A/Bæ¸¬è©¦æ•¸æ“š
        group_key = f"ab_test_{group}"
        if group_key not in self.ab_test_results:
            self.ab_test_results[group_key] = {
                'total_signals': 0,
                'avg_reward': 0.0,
                'validation_pass_rate': 0.0,
                'signals': []
            }
        
        self.ab_test_results[group_key]['total_signals'] += 1
        self.ab_test_results[group_key]['signals'].append(reward_signal.signal_id)
        
        # æ›´æ–°å¹³å‡çå‹µ
        current_avg = self.ab_test_results[group_key]['avg_reward']
        current_count = self.ab_test_results[group_key]['total_signals']
        new_avg = ((current_avg * (current_count - 1)) + reward_signal.weighted_total_reward) / current_count
        self.ab_test_results[group_key]['avg_reward'] = new_avg
        
        # æ›´æ–°é©—è­‰é€šéç‡
        passed_validations = sum(1 for r in validation_results if r.status == ValidationStatus.PASSED)
        total_validations = len(validation_results)
        if total_validations > 0:
            validation_pass_rate = passed_validations / total_validations
            current_pass_rate = self.ab_test_results[group_key]['validation_pass_rate']
            new_pass_rate = ((current_pass_rate * (current_count - 1)) + validation_pass_rate) / current_count
            self.ab_test_results[group_key]['validation_pass_rate'] = new_pass_rate
    
    async def optimize_reward_model(self,
                                  optimization_target: str = "validation_accuracy",
                                  historical_data: List['RewardSignal'] = None) -> Dict[str, Any]:
        """å„ªåŒ–çå‹µæ¨¡å‹"""
        
        optimization_results = {
            'optimization_target': optimization_target,
            'strategy_used': self.model_config.optimization_strategy.value,
            'improvements': {},
            'new_config': None,
            'performance_gain': 0.0
        }
        
        if self.model_config.optimization_strategy == OptimizationStrategy.A_B_TESTING:
            # A/Bæ¸¬è©¦å„ªåŒ–
            ab_results = await self._analyze_ab_test_results()
            optimization_results['improvements'] = ab_results
            
            if ab_results.get('significant_difference', False):
                winning_group = ab_results.get('winning_group')
                if winning_group:
                    self.logger.info(f"A/B test shows {winning_group} group performs better")
                    # åœ¨å¯¦éš›å¯¦ç¾ä¸­ï¼Œé€™è£¡æœƒæ›´æ–°æ¨¡å‹é…ç½®
        
        # è¦å‰‡æ¬Šé‡å„ªåŒ–
        if historical_data and len(historical_data) > 100:
            weight_optimization = await self._optimize_rule_weights(historical_data)
            optimization_results['improvements']['weight_optimization'] = weight_optimization
        
        return optimization_results
    
    async def _analyze_ab_test_results(self) -> Dict[str, Any]:
        """åˆ†æA/Bæ¸¬è©¦çµæœ"""
        
        if 'ab_test_control' not in self.ab_test_results or 'ab_test_treatment' not in self.ab_test_results:
            return {'error': 'Insufficient A/B test data'}
        
        control_data = self.ab_test_results['ab_test_control']
        treatment_data = self.ab_test_results['ab_test_treatment']
        
        # è¨ˆç®—çµ±è¨ˆé¡¯è‘—æ€§ï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼‰
        control_avg = control_data['avg_reward']
        treatment_avg = treatment_data['avg_reward']
        
        # æ•ˆæœå¤§å°
        effect_size = abs(treatment_avg - control_avg)
        relative_improvement = (treatment_avg - control_avg) / abs(control_avg) if control_avg != 0 else 0
        
        # ç°¡å–®çš„é¡¯è‘—æ€§æª¢é©—
        min_sample_size = 30
        significant_difference = (
            control_data['total_signals'] >= min_sample_size and
            treatment_data['total_signals'] >= min_sample_size and
            effect_size > 0.05  # 5%çš„æœ€å°æœ‰æ„ç¾©å·®ç•°
        )
        
        winning_group = "treatment" if treatment_avg > control_avg else "control"
        
        return {
            'control_group': {
                'avg_reward': control_avg,
                'sample_size': control_data['total_signals'],
                'validation_pass_rate': control_data['validation_pass_rate']
            },
            'treatment_group': {
                'avg_reward': treatment_avg,
                'sample_size': treatment_data['total_signals'],
                'validation_pass_rate': treatment_data['validation_pass_rate']
            },
            'effect_size': effect_size,
            'relative_improvement': relative_improvement,
            'significant_difference': significant_difference,
            'winning_group': winning_group if significant_difference else None,
            'confidence_level': 0.8 if significant_difference else 0.3
        }
    
    async def _optimize_rule_weights(self, historical_data: List['RewardSignal']) -> Dict[str, Any]:
        """å„ªåŒ–è¦å‰‡æ¬Šé‡"""
        
        # é€™æ˜¯ä¸€å€‹ç°¡åŒ–çš„æ¬Šé‡å„ªåŒ–å¯¦ç¾
        # å¯¦éš›ä¸­å¯èƒ½éœ€è¦æ›´è¤‡é›œçš„æ©Ÿå™¨å­¸ç¿’ç®—æ³•
        
        current_weights = {rule.rule_id: rule.weight for rule in self.model_config.validation_rules}
        
        # åˆ†ææ¯å€‹è¦å‰‡çš„æœ‰æ•ˆæ€§
        rule_effectiveness = {}
        
        for rule in self.model_config.validation_rules:
            rule_id = rule.rule_id
            
            # è¨ˆç®—è©²è¦å‰‡çš„æº–ç¢ºç‡
            total_applications = 0
            correct_predictions = 0
            
            for signal in historical_data:
                if rule.is_applicable(signal):
                    total_applications += 1
                    
                    # é€™è£¡éœ€è¦å¯¦éš›çš„é©—è­‰é‚è¼¯ä¾†åˆ¤æ–·æ­£ç¢ºæ€§
                    # ç°¡åŒ–ç‚ºå‡è¨­é«˜çå‹µä¿¡è™Ÿæ›´å¯èƒ½æ˜¯æ­£ç¢ºçš„
                    if signal.weighted_total_reward > 0.5:
                        correct_predictions += 1
            
            if total_applications > 0:
                effectiveness = correct_predictions / total_applications
                rule_effectiveness[rule_id] = effectiveness
        
        # èª¿æ•´æ¬Šé‡
        optimized_weights = current_weights.copy()
        
        for rule_id, effectiveness in rule_effectiveness.items():
            if effectiveness > 0.8:  # é«˜æ•ˆè¦å‰‡
                optimized_weights[rule_id] = min(1.0, current_weights[rule_id] * 1.1)
            elif effectiveness < 0.6:  # ä½æ•ˆè¦å‰‡
                optimized_weights[rule_id] = max(0.1, current_weights[rule_id] * 0.9)
        
        return {
            'current_weights': current_weights,
            'optimized_weights': optimized_weights,
            'rule_effectiveness': rule_effectiveness,
            'optimization_applied': True
        }
    
    async def get_validation_report(self, 
                                  signal_id: str = None,
                                  time_range_days: int = 30) -> Dict[str, Any]:
        """ç²å–é©—è­‰å ±å‘Š"""
        
        cutoff_date = datetime.now() - timedelta(days=time_range_days)
        
        if signal_id:
            # å–®å€‹ä¿¡è™Ÿçš„å ±å‘Š
            if signal_id in self.validation_history:
                results = self.validation_history[signal_id]
                return self._create_signal_report(signal_id, results)
            else:
                return {'error': f'No validation history found for signal {signal_id}'}
        
        # æ•´é«”å ±å‘Š
        all_results = []
        for results_list in self.validation_history.values():
            for result in results_list:
                result_date = datetime.fromisoformat(result.validation_timestamp)
                if result_date >= cutoff_date:
                    all_results.append(result)
        
        return self._create_summary_report(all_results, time_range_days)
    
    def _create_signal_report(self, signal_id: str, results: List[ValidationResult]) -> Dict[str, Any]:
        """å‰µå»ºå–®å€‹ä¿¡è™Ÿçš„é©—è­‰å ±å‘Š"""
        
        passed_count = sum(1 for r in results if r.status == ValidationStatus.PASSED)
        failed_count = sum(1 for r in results if r.status == ValidationStatus.FAILED)
        warning_count = sum(1 for r in results if r.status == ValidationStatus.WARNING)
        
        avg_confidence = statistics.mean([r.confidence for r in results]) if results else 0.0
        
        return {
            'signal_id': signal_id,
            'total_validations': len(results),
            'passed_validations': passed_count,
            'failed_validations': failed_count,
            'warning_validations': warning_count,
            'pass_rate': passed_count / len(results) if results else 0.0,
            'average_confidence': avg_confidence,
            'validation_details': [result.to_dict() for result in results],
            'overall_status': 'PASSED' if failed_count == 0 else 'FAILED' if failed_count > len(results) / 2 else 'WARNING'
        }
    
    def _create_summary_report(self, results: List[ValidationResult], time_range_days: int) -> Dict[str, Any]:
        """å‰µå»ºæ‘˜è¦å ±å‘Š"""
        
        if not results:
            return {
                'time_range_days': time_range_days,
                'total_validations': 0,
                'message': 'No validation data available for the specified time range'
            }
        
        # åŸºæœ¬çµ±è¨ˆ
        passed_count = sum(1 for r in results if r.status == ValidationStatus.PASSED)
        failed_count = sum(1 for r in results if r.status == ValidationStatus.FAILED)
        warning_count = sum(1 for r in results if r.status == ValidationStatus.WARNING)
        
        # æŒ‰é©—è­‰é¡å‹åˆ†çµ„
        type_breakdown = defaultdict(lambda: {'passed': 0, 'failed': 0, 'warning': 0})
        for result in results:
            type_key = result.validation_type.value
            if result.status == ValidationStatus.PASSED:
                type_breakdown[type_key]['passed'] += 1
            elif result.status == ValidationStatus.FAILED:
                type_breakdown[type_key]['failed'] += 1
            elif result.status == ValidationStatus.WARNING:
                type_breakdown[type_key]['warning'] += 1
        
        # å¹³å‡ç½®ä¿¡åº¦
        confidences = [r.confidence for r in results if r.confidence > 0]
        avg_confidence = statistics.mean(confidences) if confidences else 0.0
        
        # A/Bæ¸¬è©¦æ‘˜è¦
        ab_test_summary = None
        if self.enable_ab_testing and self.ab_test_results:
            ab_test_summary = {
                'control_group_size': self.ab_test_results.get('ab_test_control', {}).get('total_signals', 0),
                'treatment_group_size': self.ab_test_results.get('ab_test_treatment', {}).get('total_signals', 0),
                'control_avg_reward': self.ab_test_results.get('ab_test_control', {}).get('avg_reward', 0.0),
                'treatment_avg_reward': self.ab_test_results.get('ab_test_treatment', {}).get('avg_reward', 0.0)
            }
        
        return {
            'time_range_days': time_range_days,
            'total_validations': len(results),
            'passed_validations': passed_count,
            'failed_validations': failed_count,
            'warning_validations': warning_count,
            'overall_pass_rate': passed_count / len(results),
            'average_confidence': avg_confidence,
            'validation_type_breakdown': dict(type_breakdown),
            'ab_test_summary': ab_test_summary,
            'system_metrics': self.validation_metrics.to_dict(),
            'report_generated_at': datetime.now().isoformat()
        }
    
    def get_system_status(self) -> Dict[str, Any]:
        """ç²å–ç³»çµ±ç‹€æ…‹"""
        return {
            'validator_status': 'active',
            'validation_engines': list(self.validation_engines.keys()),
            'active_rules': len(self.model_config.validation_rules),
            'ab_testing_enabled': self.enable_ab_testing,
            'auto_correction_enabled': self.enable_auto_correction,
            'validation_history_size': len(self.validation_history),
            'metrics': self.validation_metrics.to_dict(),
            'storage_path': str(self.storage_path),
            'last_optimization': 'not_implemented',
            'system_health': 'good'
        }
    
    async def cleanup(self):
        """æ¸…ç†è³‡æº"""
        # ä¿å­˜é©—è­‰æ­·å²
        await self._save_validation_history()
        
        # é—œé–‰ç·šç¨‹æ± 
        self.thread_pool.shutdown(wait=True)
        
        self.logger.info("RewardValidator cleanup completed")
    
    async def _save_validation_history(self):
        """ä¿å­˜é©—è­‰æ­·å²"""
        try:
            history_file = self.storage_path / "validation_history.json"
            
            # è½‰æ›ç‚ºå¯åºåˆ—åŒ–æ ¼å¼
            serializable_history = {}
            for signal_id, results in self.validation_history.items():
                serializable_history[signal_id] = [result.to_dict() for result in results]
            
            with open(history_file, 'w', encoding='utf-8') as f:
                json.dump(serializable_history, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"Saved validation history to {history_file}")
            
        except Exception as e:
            self.logger.error(f"Failed to save validation history: {e}")

# å·¥å» å‡½æ•¸
async def create_reward_validator(
    storage_path: str = None,
    model_config: RewardModelConfig = None,
    enable_auto_correction: bool = True,
    enable_ab_testing: bool = True,
    custom_engines: Dict[ValidationType, ValidationEngine] = None
) -> RewardValidator:
    """å‰µå»ºçå‹µé©—è­‰å™¨å¯¦ä¾‹"""
    
    validator = RewardValidator(
        storage_path=storage_path,
        model_config=model_config,
        enable_auto_correction=enable_auto_correction,
        enable_ab_testing=enable_ab_testing
    )
    
    # æ·»åŠ è‡ªå®šç¾©é©—è­‰å¼•æ“
    if custom_engines:
        validator.validation_engines.update(custom_engines)
    
    return validator


# è‡ªå‹•æª¢æ¸¬å’Œåˆ‡æ›åˆ° TradingAgents ç›®éŒ„
def ensure_tradingagents_directory():
    """ç¢ºä¿ç•¶å‰å·¥ä½œç›®éŒ„åœ¨ TradingAgents/ ä¸‹ï¼Œä»¥æ­£ç¢ºè¨ªå•é…ç½®æ–‡ä»¶"""
    current_dir = Path.cwd()
    
    # å¦‚æœç•¶å‰ç›®éŒ„æ˜¯ TradingAgents çš„çˆ¶ç›®éŒ„ï¼Œåˆ‡æ›åˆ° TradingAgents
    if (current_dir / 'TradingAgents').exists():
        os.chdir(current_dir / 'TradingAgents')
        print(f"[DIR] è‡ªå‹•åˆ‡æ›å·¥ä½œç›®éŒ„åˆ°: {Path.cwd()}")
    
    # é©—è­‰å¿…è¦çš„ç›®éŒ„å­˜åœ¨
    required_dirs = ['configs', 'training', 'tradingagents']
    missing_dirs = [d for d in required_dirs if not Path(d).exists()]
    
    if missing_dirs:
        raise FileNotFoundError(f"ç¼ºå°‘å¿…è¦ç›®éŒ„: {missing_dirs}. è«‹ç¢ºä¿å¾ TradingAgents/ ç›®éŒ„åŸ·è¡Œæ­¤è…³æœ¬")

# ç›®éŒ„æª¢æŸ¥å‡½æ•¸å·²æº–å‚™å¥½ï¼Œä½†ä¸åœ¨æ¨¡çµ„å°å…¥æ™‚è‡ªå‹•åŸ·è¡Œ
# åªåœ¨éœ€è¦æ™‚æ‰‹å‹•èª¿ç”¨ ensure_tradingagents_directory()

if __name__ == "__main__":
    # æ¸¬è©¦è…³æœ¬
    import asyncio
    
    async def test_reward_validator():
        print("ğŸ” æ¸¬è©¦RewardValidator")
        
        # å‰µå»ºé©—è­‰å™¨
        validator = await create_reward_validator("./test_validation")
        
        # å‰µå»ºæ¨¡æ“¬çå‹µä¿¡è™Ÿ
        class MockRewardSignal:
            def __init__(self, signal_id: str, reward_value: float):
                self.signal_id = signal_id
                self.weighted_total_reward = reward_value
                self.reward_components = {}
                self.membership_tier = MembershipTier.GOLD
        
        # æ¸¬è©¦ä¸åŒå ´æ™¯çš„çå‹µä¿¡è™Ÿ
        test_signals = [
            MockRewardSignal("normal_signal", 0.75),      # æ­£å¸¸
            MockRewardSignal("high_reward", 1.2),         # è¶…å‡ºç¯„åœ
            MockRewardSignal("negative_signal", -0.8),    # è² çå‹µ
            MockRewardSignal("extreme_outlier", 2.5)      # æ¥µç«¯ç•°å¸¸å€¼
        ]
        
        for signal in test_signals:
            print(f"\nğŸ¯ é©—è­‰ä¿¡è™Ÿ: {signal.signal_id} (çå‹µ: {signal.weighted_total_reward})")
            
            validation_results = await validator.validate_reward_signal(signal)
            
            for result in validation_results:
                status_emoji = {
                    ValidationStatus.PASSED: 'âœ…',
                    ValidationStatus.WARNING: 'âš ï¸',
                    ValidationStatus.FAILED: 'âŒ'
                }.get(result.status, 'â“')
                
                print(f"  {status_emoji} {result.validation_type.value}: {result.status.value} (ä¿¡å¿ƒ: {result.confidence:.2f})")
                
                if result.error_message:
                    print(f"      è©³æƒ…: {result.error_message}")
        
        # ç²å–é©—è­‰å ±å‘Š
        print(f"\nğŸ“Š é©—è­‰å ±å‘Š:")
        report = await validator.get_validation_report()
        print(f"  ç¸½é©—è­‰æ¬¡æ•¸: {report['total_validations']}")
        print(f"  é€šéç‡: {report['overall_pass_rate']:.2%}")
        print(f"  å¹³å‡ä¿¡å¿ƒåº¦: {report['average_confidence']:.2f}")
        
        # ç³»çµ±ç‹€æ…‹
        status = validator.get_system_status()
        print(f"  ç³»çµ±ç‹€æ…‹: {status['validator_status']}")
        print(f"  å•Ÿç”¨å¼•æ“: {len(status['validation_engines'])}")
        
        # æ¸…ç†
        await validator.cleanup()
        
        print("âœ… RewardValidatoræ¸¬è©¦å®Œæˆ")
    
    asyncio.run(test_reward_validator())