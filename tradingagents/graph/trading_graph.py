#!/usr/bin/env python3
"""
TradingAgentsGraph - äº¤æ˜“ä»£ç†äººå·¥ä½œæµå¼•æ“
å¤©å·¥ (TianGong) - å¤šä»£ç†äººå”ä½œçš„æ™ºèƒ½åˆ†æç³»çµ±æ ¸å¿ƒ

æ­¤å·¥ä½œæµå¼•æ“å”èª¿å¤šå€‹å°ˆæ¥­åˆ†æå¸«ï¼Œå¯¦ç¾ï¼š
1. ä¸¦è¡Œæ•¸æ“šæ”¶é›†å’Œåˆæ­¥åˆ†æ
2. å¤šéšæ®µå°ˆæ¥­åˆ†æå¸«åŸ·è¡Œ
3. çµæœè¾¯è«–å’Œå…±è­˜å»ºç«‹
4. æœ€çµ‚æŠ•è³‡æ±ºç­–æ•´åˆ
5. å³æ™‚ç‹€æ…‹æ¨é€å’Œç›£æ§
"""

import asyncio
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional, Union, Callable
from dataclasses import dataclass, field, asdict
from enum import Enum
import uuid
import time
from collections import defaultdict

from ..agents.analysts.base_analyst import BaseAnalyst, AnalysisResult, AnalysisState, AnalysisType
from ..agents.analysts.workflow_orchestrator import WorkflowOrchestrator, ExecutionStrategy, get_global_orchestrator
from ..utils.user_context import UserContext
from ..dataflows.data_orchestrator import DataOrchestrator, DataRequest, DataType, DataSource
from ..utils.error_handler import handle_error, get_user_friendly_message
from ..utils.logging_config import get_analysis_logger
from ..utils.performance_monitor import log_performance
from ..default_config import DEFAULT_CONFIG
from .production_optimizations import ProductionOptimizer, create_production_optimizer, optimize_for_production
from ..routing.ai_task_router import AITaskRouter, RoutingDecisionRequest, RoutingStrategy
from ..database.task_metadata_db import TaskMetadataDB
from ..database.model_capability_db import ModelCapabilityDB

class AnalysisPhase(Enum):
    """åˆ†æéšæ®µ"""
    INITIALIZATION = "initialization"           # åˆå§‹åŒ–éšæ®µ
    DATA_COLLECTION = "data_collection"         # æ•¸æ“šæ”¶é›†éšæ®µ
    PARALLEL_ANALYSIS = "parallel_analysis"     # ä¸¦è¡Œåˆ†æéšæ®µ
    DEBATE_CONSENSUS = "debate_consensus"       # è¾¯è«–å…±è­˜éšæ®µ
    FINAL_INTEGRATION = "final_integration"     # æœ€çµ‚æ•´åˆéšæ®µ
    COMPLETED = "completed"                     # å®Œæˆéšæ®µ
    ERROR = "error"                            # éŒ¯èª¤éšæ®µ

class AnalysisStatus(Enum):
    """åˆ†æç‹€æ…‹"""
    PENDING = "pending"                         # ç­‰å¾…ä¸­
    RUNNING = "running"                         # åŸ·è¡Œä¸­
    COMPLETED = "completed"                     # å·²å®Œæˆ
    FAILED = "failed"                          # å¤±æ•—
    CANCELLED = "cancelled"                     # å·²å–æ¶ˆ

@dataclass
class AnalysistExecution:
    """åˆ†æå¸«åŸ·è¡Œç‹€æ…‹"""
    analyst_id: str
    analyst_type: AnalysisType
    status: AnalysisStatus = AnalysisStatus.PENDING
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    result: Optional[AnalysisResult] = None
    error: Optional[str] = None
    execution_time: float = 0.0
    
    def to_dict(self) -> Dict[str, Any]:
        result_dict = asdict(self)
        result_dict['analyst_type'] = self.analyst_type.value
        result_dict['status'] = self.status.value
        if self.start_time:
            result_dict['start_time'] = self.start_time.isoformat()
        if self.end_time:
            result_dict['end_time'] = self.end_time.isoformat()
        if self.result:
            result_dict['result'] = {
                'recommendation': self.result.recommendation,
                'confidence': self.result.confidence,
                'target_price': self.result.target_price,
                'reasoning': self.result.reasoning,
                'risk_factors': self.result.risk_factors
            }
        return result_dict

@dataclass
class WorkflowState:
    """å·¥ä½œæµç‹€æ…‹"""
    session_id: str
    stock_id: str
    user_context: UserContext
    current_phase: AnalysisPhase = AnalysisPhase.INITIALIZATION
    overall_status: AnalysisStatus = AnalysisStatus.PENDING
    start_time: datetime = field(default_factory=datetime.now)
    end_time: Optional[datetime] = None
    progress_percentage: float = 0.0
    
    # æ•¸æ“šæ”¶é›†çµæœ
    collected_data: Dict[str, Any] = field(default_factory=dict)
    data_collection_errors: List[str] = field(default_factory=list)
    
    # åˆ†æå¸«åŸ·è¡Œç‹€æ…‹
    analyst_executions: Dict[str, AnalysistExecution] = field(default_factory=dict)
    
    # è¾¯è«–å’Œå…±è­˜
    debate_rounds: List[Dict[str, Any]] = field(default_factory=list)
    consensus_result: Optional[Dict[str, Any]] = None
    
    # æœ€çµ‚çµæœ
    final_result: Optional[AnalysisResult] = None
    integration_metadata: Dict[str, Any] = field(default_factory=dict)
    
    # éŒ¯èª¤å’Œç›£æ§
    errors: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)
    performance_metrics: Dict[str, Any] = field(default_factory=dict)
    
    def add_error(self, error: str):
        """æ·»åŠ éŒ¯èª¤"""
        self.errors.append(f"[{datetime.now().isoformat()}] {error}")
    
    def add_warning(self, warning: str):
        """æ·»åŠ è­¦å‘Š"""
        self.warnings.append(f"[{datetime.now().isoformat()}] {warning}")
    
    def to_dict(self) -> Dict[str, Any]:
        """è½‰æ›ç‚ºå­—å…¸"""
        result = asdict(self)
        result['current_phase'] = self.current_phase.value
        result['overall_status'] = self.overall_status.value
        result['start_time'] = self.start_time.isoformat()
        if self.end_time:
            result['end_time'] = self.end_time.isoformat()
        if self.user_context:
            result['user_context'] = {
                'user_id': self.user_context.user_id,
                'membership_tier': self.user_context.membership_tier.value,
                'permissions': asdict(self.user_context.permissions)
            }
        
        # è½‰æ›åˆ†æå¸«åŸ·è¡Œç‹€æ…‹
        result['analyst_executions'] = {
            analyst_id: execution.to_dict() 
            for analyst_id, execution in self.analyst_executions.items()
        }
        
        # è™•ç†æœ€çµ‚çµæœ
        if self.final_result:
            result['final_result'] = {
                'analyst_id': self.final_result.analyst_id,
                'recommendation': self.final_result.recommendation,
                'confidence': self.final_result.confidence,
                'target_price': self.final_result.target_price,
                'reasoning': self.final_result.reasoning,
                'risk_factors': self.final_result.risk_factors,
                'timestamp': self.final_result.timestamp
            }
        
        return result

class TradingAgentsGraph:
    """äº¤æ˜“ä»£ç†äººå·¥ä½œæµå¼•æ“"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        åˆå§‹åŒ–å·¥ä½œæµå¼•æ“
        
        Args:
            config: é…ç½®åƒæ•¸
        """
        self.config = config or DEFAULT_CONFIG
        self.logger = get_analysis_logger("trading_graph")
        
        # å·¥ä½œæµé…ç½® (å¿…é ˆå…ˆè¨­å®š)
        self.workflow_config = self.config.get('workflow', {
            'max_concurrent_sessions': 10,
            'session_timeout': 1800,  # 30åˆ†é˜
            'enable_debate': True,
            'min_consensus_threshold': 0.6,
            'max_debate_rounds': 3,
            'max_parallelism': 4,
            'enable_conflict_resolution': True,
            'retry_attempts': 2,
            'enable_intelligent_routing': True  # é»˜è®¤å¯ç”¨æ™ºèƒ½è·¯ç”±
        })
        
        # åˆå§‹åŒ–çµ„ä»¶
        self.data_orchestrator = None
        self.analysts = {}
        self.active_sessions = {}
        
        # GPT-OSSæ™ºèƒ½è·¯ç”±ç³»ç»Ÿ (æ–°å¢)
        self.ai_task_router = None
        self.enable_intelligent_routing = self.workflow_config.get('enable_intelligent_routing', True)
        self.routing_transparency = self.config.get('routing_transparency', {
            'enabled': True,
            'include_decision_details': True,
            'log_routing_decisions': True
        })
        
        # å·¥ä½œæµç·¨æ’å™¨
        self.workflow_orchestrator = WorkflowOrchestrator({
            'max_parallelism': self.workflow_config.get('max_parallelism', 4),
            'execution_timeout': self.workflow_config.get('session_timeout', 1800),
            'conflict_resolution_enabled': self.workflow_config.get('enable_conflict_resolution', True),
            'retry_attempts': self.workflow_config.get('retry_attempts', 2)
        })
        
        # ç‹€æ…‹æ¨é€å›èª¿
        self.status_callbacks: List[Callable] = []
        
        # æ€§èƒ½ç›£æ§
        self.session_metrics = defaultdict(dict)
        
        # ç”Ÿç”¢ç’°å¢ƒå„ªåŒ–å™¨
        self.production_optimizer = None
        self.enable_production_optimization = self.config.get('enable_production_optimization', True)
        
        if self.enable_production_optimization:
            self.production_optimizer = create_production_optimizer(
                self.config.get('production_optimization', {})
            )
        
        self.logger.info("TradingAgentsGraph å·¥ä½œæµå¼•æ“åˆå§‹åŒ–å®Œæˆ", extra={
            'production_optimization_enabled': self.enable_production_optimization,
            'intelligent_routing_enabled': self.enable_intelligent_routing,
            'max_concurrent_sessions': self.workflow_config['max_concurrent_sessions']
        })
    
    async def initialize(self):
        """ç•°æ­¥åˆå§‹åŒ–"""
        try:
            # åˆå§‹åŒ–æ•¸æ“šç·¨æ’å™¨
            self.data_orchestrator = DataOrchestrator(self.config.get('data_orchestrator', {}))
            await self.data_orchestrator.initialize()
            
            # åˆå§‹åŒ–æ™ºèƒ½è·¯ç”±å™¨ (GPT-OSSæ•´åˆ)
            if self.enable_intelligent_routing:
                await self._initialize_intelligent_routing()
            
            # åˆå§‹åŒ–åˆ†æå¸«
            await self._initialize_analysts()
            
            # å•Ÿå‹•ç”Ÿç”¢ç’°å¢ƒå„ªåŒ–å™¨
            if self.production_optimizer:
                await self.production_optimizer.start()
                self.logger.info("ç”Ÿç”¢ç’°å¢ƒå„ªåŒ–å™¨å·²å•Ÿå‹•")
            
            self.logger.info("å·¥ä½œæµå¼•æ“ç•°æ­¥åˆå§‹åŒ–å®Œæˆ", extra={
                'analysts_count': len(self.analysts),
                'production_optimizer_running': self.production_optimizer is not None,
                'intelligent_routing_active': self.ai_task_router is not None
            })
            
        except Exception as e:
            self.logger.error(f"å·¥ä½œæµå¼•æ“åˆå§‹åŒ–å¤±æ•—: {str(e)}")
            raise
    
    async def _initialize_intelligent_routing(self):
        """åˆå§‹åŒ–æ™ºèƒ½è·¯ç”±å™¨"""
        try:
            self.logger.info("ğŸš€ åˆå§‹åŒ–GPT-OSSæ™ºèƒ½è·¯ç”±å™¨...")
            
            # è·å–è·¯ç”±å™¨é…ç½®
            router_config = self.config.get('ai_task_router', {})
            
            # åˆå§‹åŒ–æ•°æ®åº“ç»„ä»¶
            task_db = TaskMetadataDB()
            model_db = ModelCapabilityDB() 
            
            # è·å–æ€§èƒ½ç›‘æ§å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            performance_monitor = None
            if hasattr(self, 'production_optimizer') and self.production_optimizer:
                performance_monitor = getattr(self.production_optimizer, 'performance_monitor', None)
            
            # åˆ›å»ºè·¯ç”±å™¨å®ä¾‹
            self.ai_task_router = AITaskRouter(
                task_db=task_db,
                model_db=model_db,
                performance_monitor=performance_monitor,
                config=router_config
            )
            
            # åˆå§‹åŒ–è·¯ç”±å™¨
            router_ready = await self.ai_task_router.initialize()
            
            if router_ready:
                self.logger.info("âœ… GPT-OSSæ™ºèƒ½è·¯ç”±å™¨åˆå§‹åŒ–æˆåŠŸ")
                
                # è®°å½•è·¯ç”±å™¨çŠ¶æ€
                if self.routing_transparency.get('log_routing_decisions', True):
                    router_stats = self.ai_task_router.get_routing_statistics()
                    self.logger.info("ğŸ“Š è·¯ç”±å™¨ç»Ÿè®¡", extra={'router_stats': router_stats})
            else:
                self.logger.warning("âš ï¸ æ™ºèƒ½è·¯ç”±å™¨åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿè·¯ç”±")
                self.ai_task_router = None
                self.enable_intelligent_routing = False
            
        except Exception as e:
            self.logger.error(f"âŒ æ™ºèƒ½è·¯ç”±å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            self.ai_task_router = None
            self.enable_intelligent_routing = False
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå…è®¸ç³»ç»Ÿç»§ç»­ä½¿ç”¨ä¼ ç»Ÿè·¯ç”±
    
    async def _initialize_analysts(self):
        """åˆå§‹åŒ–æ‰€æœ‰åˆ†æå¸«"""
        analyst_configs = self.config.get('analysts_config', {})
        
        # å‹•æ…‹å°å…¥å’Œåˆå§‹åŒ–åˆ†æå¸«
        try:
            # å°è‚¡å¸‚å ´åˆ†æå¸«
            from ..agents.analysts.taiwan_market_analyst import TaiwanMarketAnalyst
            self.analysts['taiwan_market_analyst'] = TaiwanMarketAnalyst(
                analyst_configs.get('taiwan_market_analyst', {})
            )
            
            # é¢¨éšªåˆ†æå¸«
            from ..agents.analysts.risk_analyst import RiskAnalyst
            self.analysts['risk_analyst'] = RiskAnalyst(
                analyst_configs.get('risk_analyst', {})
            )
            
            # æŠ•è³‡è¦åŠƒå¸«
            from ..agents.analysts.investment_planner import InvestmentPlanner
            self.analysts['investment_planner'] = InvestmentPlanner(
                analyst_configs.get('investment_planner', {})
            )
            
            # åŸºæœ¬é¢åˆ†æå¸« (å¦‚æœå­˜åœ¨)
            try:
                from ..agents.analysts.fundamentals_analyst import FundamentalsAnalyst
                self.analysts['fundamentals_analyst'] = FundamentalsAnalyst(
                    analyst_configs.get('fundamentals_analyst', {})
                )
            except ImportError:
                self.logger.warning("åŸºæœ¬é¢åˆ†æå¸«æœªå¯¦ç¾ï¼Œè·³éåˆå§‹åŒ–")
            
            # æ–°èåˆ†æå¸« (å¦‚æœå­˜åœ¨)
            try:
                from ..agents.analysts.news_analyst import NewsAnalyst
                self.analysts['news_analyst'] = NewsAnalyst(
                    analyst_configs.get('news_analyst', {})
                )
            except ImportError:
                self.logger.warning("æ–°èåˆ†æå¸«æœªå¯¦ç¾ï¼Œè·³éåˆå§‹åŒ–")
            
            # æƒ…ç·’åˆ†æå¸« (å¦‚æœå­˜åœ¨)
            try:
                from ..agents.analysts.sentiment_analyst import SentimentAnalyst
                self.analysts['sentiment_analyst'] = SentimentAnalyst(
                    analyst_configs.get('sentiment_analyst', {})
                )
            except ImportError:
                self.logger.warning("æƒ…ç·’åˆ†æå¸«æœªå¯¦ç¾ï¼Œè·³éåˆå§‹åŒ–")
            
            # å°‡åˆ†æå¸«è¨»å†Šåˆ°å·¥ä½œæµç·¨æ’å™¨
            for analyst_id, analyst in self.analysts.items():
                self.workflow_orchestrator.register_analyst(analyst)
                
                # ä¸ºåˆ†æå¸ˆæ³¨å…¥æ™ºèƒ½è·¯ç”±å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if self.ai_task_router and hasattr(analyst, '_initialize_intelligent_routing_client'):
                    try:
                        await analyst._initialize_intelligent_routing_client(self.ai_task_router)
                        self.logger.debug(f"âœ… åˆ†æå¸ˆ {analyst_id} å·²è¿æ¥æ™ºèƒ½è·¯ç”±å™¨")
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ åˆ†æå¸ˆ {analyst_id} æ™ºèƒ½è·¯ç”±è¿æ¥å¤±è´¥: {e}")
            
            self.logger.info(f"æˆåŠŸåˆå§‹åŒ– {len(self.analysts)} å€‹åˆ†æå¸«ï¼Œå·²è¨»å†Šåˆ°å·¥ä½œæµç·¨æ’å™¨", extra={
                'intelligent_routing_injected': self.ai_task_router is not None,
                'analysts': list(self.analysts.keys())
            })
            
        except Exception as e:
            self.logger.error(f"åˆ†æå¸«åˆå§‹åŒ–å¤±æ•—: {str(e)}")
            raise
    
    def add_status_callback(self, callback: Callable[[WorkflowState], None]):
        """æ·»åŠ ç‹€æ…‹æ¨é€å›èª¿"""
        self.status_callbacks.append(callback)
    
    def remove_status_callback(self, callback: Callable[[WorkflowState], None]):
        """ç§»é™¤ç‹€æ…‹æ¨é€å›èª¿"""
        if callback in self.status_callbacks:
            self.status_callbacks.remove(callback)
    
    async def _notify_status_change(self, state: WorkflowState):
        """é€šçŸ¥ç‹€æ…‹è®ŠåŒ–"""
        for callback in self.status_callbacks:
            try:
                if asyncio.iscoroutinefunction(callback):
                    await callback(state)
                else:
                    callback(state)
            except Exception as e:
                self.logger.error(f"ç‹€æ…‹æ¨é€å›èª¿å¤±æ•—: {str(e)}")
    
    @optimize_for_production
    @log_performance()
    async def analyze_stock(
        self,
        stock_id: str,
        user_context: UserContext,
        preferred_analysts: Optional[List[str]] = None,
        enable_debate: Optional[bool] = None
    ) -> str:
        """
        é–‹å§‹è‚¡ç¥¨åˆ†æå·¥ä½œæµ
        
        Args:
            stock_id: è‚¡ç¥¨ä»£è™Ÿ
            user_context: ç”¨æˆ¶ä¸Šä¸‹æ–‡
            preferred_analysts: æŒ‡å®šçš„åˆ†æå¸«åˆ—è¡¨
            enable_debate: æ˜¯å¦å•Ÿç”¨è¾¯è«–æ©Ÿåˆ¶
            
        Returns:
            æœƒè©±ID
        """
        # ç²å–æœƒè©±æ§½ä½ï¼ˆå¦‚æœå•Ÿç”¨ç”Ÿç”¢å„ªåŒ–ï¼‰
        if self.production_optimizer:
            slot_acquired = await self.production_optimizer.concurrency_optimizer.acquire_session_slot()
            if not slot_acquired:
                raise RuntimeError("ç„¡æ³•ç²å–æœƒè©±æ§½ä½ï¼Œç³»çµ±ç¹å¿™ï¼Œè«‹ç¨å¾Œå†è©¦")
        
        try:
            # å‰µå»ºæœƒè©±
            session_id = str(uuid.uuid4())
            
            # æª¢æŸ¥æœƒè©±æ•¸é‡é™åˆ¶
            current_sessions = len(self.active_sessions)
            max_sessions = self.workflow_config['max_concurrent_sessions']
            
            if current_sessions >= max_sessions:
                self.logger.warning(f"æœƒè©±æ•¸é‡é”åˆ°é™åˆ¶: {current_sessions}/{max_sessions}")
                raise RuntimeError("è¶…éæœ€å¤§ä¸¦ç™¼æœƒè©±æ•¸é‡é™åˆ¶")
            
            # å‰µå»ºå·¥ä½œæµç‹€æ…‹
            state = WorkflowState(
                session_id=session_id,
                stock_id=stock_id,
                user_context=user_context
            )
            
            self.active_sessions[session_id] = state
            
            # è¨˜éŒ„æœƒè©±é–‹å§‹
            self.logger.info(f"é–‹å§‹è‚¡ç¥¨åˆ†æå·¥ä½œæµ: {stock_id}, æœƒè©±ID: {session_id}", extra={
                'session_id': session_id,
                'stock_symbol': stock_id,
                'user_id': user_context.user_id,
                'membership_tier': user_context.membership_tier.value,
                'preferred_analysts': preferred_analysts,
                'enable_debate': enable_debate,
                'current_sessions': current_sessions + 1,
                'max_sessions': max_sessions
            })
            
            # é–‹å§‹ç•°æ­¥åŸ·è¡Œå·¥ä½œæµ
            asyncio.create_task(self._execute_workflow_with_cleanup(
                state, preferred_analysts, enable_debate
            ))
            
            return session_id
            
        except Exception:
            # é‡‹æ”¾æœƒè©±æ§½ä½
            if self.production_optimizer:
                self.production_optimizer.concurrency_optimizer.release_session_slot()
            raise
    
    async def _execute_workflow_with_cleanup(
        self,
        state: WorkflowState,
        preferred_analysts: Optional[List[str]] = None,
        enable_debate: Optional[bool] = None
    ):
        """åŸ·è¡Œå·¥ä½œæµä¸¦è™•ç†æ¸…ç†"""
        try:
            await self._execute_workflow(state, preferred_analysts, enable_debate)
        except Exception as e:
            # è™•ç†éŒ¯èª¤
            error_info = await handle_error(e, {
                'session_id': state.session_id,
                'stock_symbol': state.stock_id,
                'user_id': state.user_context.user_id,
                'current_phase': state.current_phase.value,
                'component': 'trading_graph_workflow'
            }, user_id=state.user_context.user_id, session_id=state.session_id)
            
            # æ›´æ–°ç‹€æ…‹
            state.overall_status = AnalysisStatus.FAILED
            state.current_phase = AnalysisPhase.ERROR
            state.add_error(f"å·¥ä½œæµåŸ·è¡Œå¤±æ•—: {str(e)}")
            state.end_time = datetime.now()
            
            self.logger.error(f"å·¥ä½œæµåŸ·è¡Œå¤±æ•—: {state.session_id}", extra={
                'session_id': state.session_id,
                'stock_symbol': state.stock_id,
                'error_id': error_info.error_id,
                'error_message': str(e)
            })
            
            await self._notify_status_change(state)
        
        finally:
            # é‡‹æ”¾æœƒè©±æ§½ä½
            if self.production_optimizer:
                self.production_optimizer.concurrency_optimizer.release_session_slot()
    
    @optimize_for_production
    async def _execute_workflow(
        self,
        state: WorkflowState,
        preferred_analysts: Optional[List[str]] = None,
        enable_debate: Optional[bool] = None
    ):
        """åŸ·è¡Œå®Œæ•´çš„åˆ†æå·¥ä½œæµ"""
        try:
            # éšæ®µ 1: æ•¸æ“šæ”¶é›†
            await self._phase_data_collection(state)
            
            # éšæ®µ 2: ä¸¦è¡Œåˆ†æ
            await self._phase_parallel_analysis(state, preferred_analysts)
            
            # éšæ®µ 3: è¾¯è«–å’Œå…±è­˜ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
            if enable_debate or (enable_debate is None and self.workflow_config.get('enable_debate', True)):
                await self._phase_debate_consensus(state)
            
            # éšæ®µ 4: æœ€çµ‚æ•´åˆ
            await self._phase_final_integration(state)
            
            # æ¨™è¨˜å®Œæˆ
            state.current_phase = AnalysisPhase.COMPLETED
            state.overall_status = AnalysisStatus.COMPLETED
            state.end_time = datetime.now()
            state.progress_percentage = 100.0
            
            # è¨ˆç®—ç¸½åŸ·è¡Œæ™‚é–“
            total_time = (state.end_time - state.start_time).total_seconds()
            state.performance_metrics['total_execution_time'] = total_time
            
            await self._notify_status_change(state)
            
            self.logger.info(f"å·¥ä½œæµå®Œæˆ: {state.session_id}, åŸ·è¡Œæ™‚é–“: {total_time:.2f}ç§’")
            
        except Exception as e:
            # è™•ç†å·¥ä½œæµéŒ¯èª¤
            state.current_phase = AnalysisPhase.ERROR
            state.overall_status = AnalysisStatus.FAILED
            state.end_time = datetime.now()
            state.add_error(f"å·¥ä½œæµåŸ·è¡Œå¤±æ•—: {str(e)}")
            
            await self._notify_status_change(state)
            
            self.logger.error(f"å·¥ä½œæµåŸ·è¡Œå¤±æ•—: {state.session_id}, éŒ¯èª¤: {str(e)}")
        
        finally:
            # è¨˜éŒ„æœƒè©±æŒ‡æ¨™
            self._record_session_metrics(state)
    
    async def _phase_data_collection(self, state: WorkflowState):
        """éšæ®µ 1: æ•¸æ“šæ”¶é›†"""
        state.current_phase = AnalysisPhase.DATA_COLLECTION
        state.progress_percentage = 10.0
        await self._notify_status_change(state)
        
        self.logger.info(f"é–‹å§‹æ•¸æ“šæ”¶é›†éšæ®µ: {state.stock_id}")
        
        # ä¸¦è¡Œæ”¶é›†ä¸åŒé¡å‹çš„æ•¸æ“š
        data_tasks = []
        
        # è‚¡åƒ¹æ•¸æ“š
        data_tasks.append(self._collect_data(
            state, DataType.STOCK_PRICE, "è‚¡åƒ¹æ•¸æ“š"
        ))
        
        # å…¬å¸è³‡æ–™
        data_tasks.append(self._collect_data(
            state, DataType.COMPANY_PROFILE, "å…¬å¸è³‡æ–™"
        ))
        
        # è²¡å‹™æ•¸æ“š
        data_tasks.append(self._collect_data(
            state, DataType.FINANCIAL_DATA, "è²¡å‹™æ•¸æ“š"
        ))
        
        # æ–°èæ•¸æ“š
        data_tasks.append(self._collect_data(
            state, DataType.COMPANY_NEWS, "æ–°èæ•¸æ“š"
        ))
        
        # ç­‰å¾…æ‰€æœ‰æ•¸æ“šæ”¶é›†å®Œæˆ
        await asyncio.gather(*data_tasks, return_exceptions=True)
        
        state.progress_percentage = 25.0
        await self._notify_status_change(state)
        
        self.logger.info(f"æ•¸æ“šæ”¶é›†éšæ®µå®Œæˆ: {state.stock_id}")
    
    async def _collect_data(self, state: WorkflowState, data_type: DataType, description: str):
        """æ”¶é›†ç‰¹å®šé¡å‹çš„æ•¸æ“š"""
        try:
            request = DataRequest(
                symbol=state.stock_id,
                data_type=data_type,
                user_context=state.user_context
            )
            
            response = await self.data_orchestrator.get_data(request)
            
            if response.success:
                state.collected_data[data_type.value] = response.data
                self.logger.debug(f"{description}æ”¶é›†æˆåŠŸ: {state.stock_id}")
            else:
                error_msg = f"{description}æ”¶é›†å¤±æ•—: {response.error}"
                state.data_collection_errors.append(error_msg)
                self.logger.warning(error_msg)
                
        except Exception as e:
            error_msg = f"{description}æ”¶é›†ç•°å¸¸: {str(e)}"
            state.data_collection_errors.append(error_msg)
            self.logger.error(error_msg)
    
    async def _phase_parallel_analysis(self, state: WorkflowState, preferred_analysts: Optional[List[str]]):
        """éšæ®µ 2: ä¸¦è¡Œåˆ†æ - ä½¿ç”¨WorkflowOrchestrator"""
        state.current_phase = AnalysisPhase.PARALLEL_ANALYSIS
        state.progress_percentage = 30.0
        await self._notify_status_change(state)
        
        self.logger.info(f"é–‹å§‹ä¸¦è¡Œåˆ†æéšæ®µ: {state.stock_id}")
        
        # ç¢ºå®šè¦åŸ·è¡Œçš„åˆ†æå¸«
        analysts_to_run = self._select_analysts(state.user_context, preferred_analysts)
        
        # å‰µå»ºåˆ†æå¸«åŸ·è¡Œç‹€æ…‹ï¼ˆç”¨æ–¼ç‹€æ…‹è¿½è¹¤ï¼‰
        for analyst_id in analysts_to_run:
            if analyst_id in self.analysts:
                analyst = self.analysts[analyst_id]
                state.analyst_executions[analyst_id] = AnalysistExecution(
                    analyst_id=analyst_id,
                    analyst_type=analyst.get_analysis_type()
                )
        
        # å‰µå»ºåˆ†æç‹€æ…‹
        analysis_state = AnalysisState(
            stock_id=state.stock_id,
            analysis_date=datetime.now().strftime('%Y-%m-%d'),
            user_context=asdict(state.user_context) if state.user_context else None,
            stock_data=state.collected_data.get('stock_price'),
            financial_data=state.collected_data.get('financial_data'),
            news_data=state.collected_data.get('company_news'),
            market_data=state.collected_data.get('market_data')
        )
        
        # ä½¿ç”¨å·¥ä½œæµç·¨æ’å™¨åŸ·è¡Œåˆ†æ
        try:
            execution_result = await self.workflow_orchestrator.execute_workflow(
                state=analysis_state,
                selected_analysts=analysts_to_run
            )
            
            # å°‡ç·¨æ’å™¨çµæœè½‰æ›ç‚ºå·¥ä½œæµç‹€æ…‹
            for analyst_id, analysis_result in execution_result.analyst_results.items():
                if analyst_id in state.analyst_executions:
                    execution = state.analyst_executions[analyst_id]
                    execution.status = AnalysisStatus.COMPLETED
                    execution.result = analysis_result
                    execution.start_time = datetime.now() - timedelta(
                        seconds=execution_result.performance_metrics.get(
                            f"{analyst_id}_execution_time", 1.0
                        )
                    )
                    execution.end_time = datetime.now()
                    execution.execution_time = execution_result.performance_metrics.get(
                        f"{analyst_id}_execution_time", 1.0
                    )
            
            # è¨˜éŒ„å¤±æ•—çš„åˆ†æå¸«
            for analyst_id in analysts_to_run:
                if (analyst_id not in execution_result.analyst_results and 
                    analyst_id in state.analyst_executions):
                    execution = state.analyst_executions[analyst_id]
                    execution.status = AnalysisStatus.FAILED
                    execution.error = execution_result.performance_metrics.get(
                        f"{analyst_id}_error", "æœªçŸ¥éŒ¯èª¤"
                    )
            
            # è¨˜éŒ„è¡çªè§£æ±ºä¿¡æ¯
            if execution_result.conflict_resolutions:
                for conflict in execution_result.conflict_resolutions:
                    state.add_warning(f"è§£æ±ºå»ºè­°è¡çª: {conflict['conflict_type']} -> {conflict.get('resolved_recommendation')}")
            
            self.logger.info(f"å·¥ä½œæµç·¨æ’å™¨åŸ·è¡Œå®Œæˆ", extra={
                'session_id': state.session_id,
                'successful_analyses': len(execution_result.analyst_results),
                'conflicts_resolved': len(execution_result.conflict_resolutions),
                'orchestrator_success': execution_result.success
            })
            
        except Exception as e:
            # ç·¨æ’å™¨åŸ·è¡Œå¤±æ•—ï¼Œæ¨™è¨˜æ‰€æœ‰åˆ†æå¸«ç‚ºå¤±æ•—
            for analyst_id in analysts_to_run:
                if analyst_id in state.analyst_executions:
                    execution = state.analyst_executions[analyst_id]
                    execution.status = AnalysisStatus.FAILED
                    execution.error = f"å·¥ä½œæµç·¨æ’å™¨åŸ·è¡Œå¤±æ•—: {str(e)}"
            
            self.logger.error(f"å·¥ä½œæµç·¨æ’å™¨åŸ·è¡Œå¤±æ•—: {str(e)}", extra={
                'session_id': state.session_id,
                'analysts_to_run': analysts_to_run
            })
        
        state.progress_percentage = 70.0
        await self._notify_status_change(state)
        
        completed_count = sum(1 for exec in state.analyst_executions.values() 
                             if exec.status == AnalysisStatus.COMPLETED)
        
        self.logger.info(f"ä¸¦è¡Œåˆ†æéšæ®µå®Œæˆ: {state.stock_id}, å®Œæˆ {completed_count}/{len(analysts_to_run)} å€‹åˆ†æå¸«")
    
    def _select_analysts(self, user_context: UserContext, preferred_analysts: Optional[List[str]]) -> List[str]:
        """æ ¹æ“šç”¨æˆ¶æ¬Šé™å’Œåå¥½é¸æ“‡åˆ†æå¸«"""
        available_analysts = list(self.analysts.keys())
        
        # å¦‚æœæŒ‡å®šäº†åå¥½åˆ†æå¸«ï¼Œéæ¿¾å¯ç”¨çš„
        if preferred_analysts:
            analysts_to_run = [a for a in preferred_analysts if a in available_analysts]
        else:
            # æ ¹æ“šæœƒå“¡ç­‰ç´šé¸æ“‡åˆ†æå¸«
            if user_context.membership_tier.value == 'FREE':
                # å…è²»æœƒå“¡ï¼šåŸºç¤åˆ†æå¸«
                analysts_to_run = ['taiwan_market_analyst', 'risk_analyst']
            elif user_context.membership_tier.value == 'GOLD':
                # é»ƒé‡‘æœƒå“¡ï¼šå¢åŠ åŸºæœ¬é¢å’Œæ–°èåˆ†æ
                analysts_to_run = ['taiwan_market_analyst', 'risk_analyst', 'fundamentals_analyst', 'news_analyst']
            else:  # DIAMOND
                # é‘½çŸ³æœƒå“¡ï¼šæ‰€æœ‰åˆ†æå¸«
                analysts_to_run = available_analysts
        
        # éæ¿¾å¯¦éš›å­˜åœ¨çš„åˆ†æå¸«
        analysts_to_run = [a for a in analysts_to_run if a in self.analysts]
        
        # æŠ•è³‡è¦åŠƒå¸«ç¸½æ˜¯æœ€å¾ŒåŸ·è¡Œ
        if 'investment_planner' in analysts_to_run:
            analysts_to_run.remove('investment_planner')
            analysts_to_run.append('investment_planner')
        
        return analysts_to_run
    
    async def _execute_analyst(self, state: WorkflowState, analyst_id: str):
        """åŸ·è¡Œå–®å€‹åˆ†æå¸«"""
        execution = state.analyst_executions[analyst_id]
        analyst = self.analysts[analyst_id]
        
        try:
            execution.status = AnalysisStatus.RUNNING
            execution.start_time = datetime.now()
            
            # å‰µå»ºåˆ†æç‹€æ…‹
            analysis_state = AnalysisState(
                stock_id=state.stock_id,
                analysis_date=datetime.now().strftime('%Y-%m-%d'),
                user_context=asdict(state.user_context) if state.user_context else None,
                stock_data=state.collected_data.get('stock_price'),
                financial_data=state.collected_data.get('financial_data'),
                news_data=state.collected_data.get('company_news'),
                market_data=state.collected_data.get('market_data')
            )
            
            # åŸ·è¡Œåˆ†æ
            result = await analyst.analyze(analysis_state)
            
            execution.end_time = datetime.now()
            execution.execution_time = (execution.end_time - execution.start_time).total_seconds()
            execution.result = result
            execution.status = AnalysisStatus.COMPLETED
            
            self.logger.info(f"åˆ†æå¸« {analyst_id} åŸ·è¡Œå®Œæˆï¼Œè€—æ™‚ {execution.execution_time:.2f}ç§’")
            
        except Exception as e:
            execution.end_time = datetime.now()
            execution.execution_time = (execution.end_time - execution.start_time).total_seconds()
            execution.status = AnalysisStatus.FAILED
            execution.error = str(e)
            
            state.add_error(f"åˆ†æå¸« {analyst_id} åŸ·è¡Œå¤±æ•—: {str(e)}")
            self.logger.error(f"åˆ†æå¸« {analyst_id} åŸ·è¡Œå¤±æ•—: {str(e)}")
        
        # é€šçŸ¥ç‹€æ…‹è®ŠåŒ–
        await self._notify_status_change(state)
    
    async def _phase_debate_consensus(self, state: WorkflowState):
        """éšæ®µ 3: è¾¯è«–å’Œå…±è­˜"""
        state.current_phase = AnalysisPhase.DEBATE_CONSENSUS
        state.progress_percentage = 75.0
        await self._notify_status_change(state)
        
        self.logger.info(f"é–‹å§‹è¾¯è«–å…±è­˜éšæ®µ: {state.stock_id}")
        
        # æ”¶é›†æˆåŠŸçš„åˆ†æçµæœ
        successful_results = []
        for execution in state.analyst_executions.values():
            if execution.status == AnalysisStatus.COMPLETED and execution.result:
                successful_results.append(execution.result)
        
        if len(successful_results) < 2:
            state.add_warning("åˆ†æçµæœä¸è¶³ï¼Œè·³éè¾¯è«–éšæ®µ")
            return
        
        # æª¢æŸ¥æ˜¯å¦å­˜åœ¨åˆ†æ­§
        consensus_score = self._calculate_consensus_score(successful_results)
        
        if consensus_score >= self.workflow_config.get('min_consensus_threshold', 0.6):
            # å…±è­˜åº¦è¶³å¤ ï¼Œç„¡éœ€è¾¯è«–
            state.consensus_result = {
                'consensus_achieved': True,
                'consensus_score': consensus_score,
                'debate_rounds': 0,
                'final_recommendation': self._get_majority_recommendation(successful_results)
            }
        else:
            # éœ€è¦é€²è¡Œè¾¯è«–
            await self._conduct_debate(state, successful_results)
        
        state.progress_percentage = 85.0
        await self._notify_status_change(state)
        
        self.logger.info(f"è¾¯è«–å…±è­˜éšæ®µå®Œæˆ: {state.stock_id}")
    
    def _calculate_consensus_score(self, results: List[AnalysisResult]) -> float:
        """è¨ˆç®—å…±è­˜åˆ†æ•¸"""
        if not results:
            return 0.0
        
        recommendations = [r.recommendation for r in results]
        recommendation_counts = {}
        
        for rec in recommendations:
            recommendation_counts[rec] = recommendation_counts.get(rec, 0) + 1
        
        # æœ€å¤šçš„å»ºè­°æ•¸é‡ / ç¸½å»ºè­°æ•¸é‡
        max_count = max(recommendation_counts.values())
        return max_count / len(recommendations)
    
    def _get_majority_recommendation(self, results: List[AnalysisResult]) -> str:
        """ç²å–å¤šæ•¸æ´¾å»ºè­°"""
        recommendations = [r.recommendation for r in results]
        recommendation_counts = {}
        
        for rec in recommendations:
            recommendation_counts[rec] = recommendation_counts.get(rec, 0) + 1
        
        return max(recommendation_counts.items(), key=lambda x: x[1])[0]
    
    async def _conduct_debate(self, state: WorkflowState, results: List[AnalysisResult]):
        """é€²è¡Œè¾¯è«–éç¨‹"""
        max_rounds = self.workflow_config.get('max_debate_rounds', 3)
        
        for round_num in range(1, max_rounds + 1):
            debate_round = {
                'round': round_num,
                'timestamp': datetime.now().isoformat(),
                'participants': [],
                'consensus_score': 0.0
            }
            
            # æ¨¡æ“¬è¾¯è«–éç¨‹
            # åœ¨å¯¦éš›å¯¦ç¾ä¸­ï¼Œé€™è£¡æœƒè®“åˆ†æå¸«äº’ç›¸è¾¯è«–
            await asyncio.sleep(0.5)  # æ¨¡æ“¬è¾¯è«–æ™‚é–“
            
            # é‡æ–°è¨ˆç®—å…±è­˜
            consensus_score = self._calculate_consensus_score(results)
            debate_round['consensus_score'] = consensus_score
            
            state.debate_rounds.append(debate_round)
            
            if consensus_score >= self.workflow_config.get('min_consensus_threshold', 0.6):
                # é”æˆå…±è­˜
                state.consensus_result = {
                    'consensus_achieved': True,
                    'consensus_score': consensus_score,
                    'debate_rounds': round_num,
                    'final_recommendation': self._get_majority_recommendation(results)
                }
                break
        
        # å¦‚æœç¶“éæ‰€æœ‰è¼ªæ¬¡ä»æœªé”æˆå…±è­˜
        if not state.consensus_result:
            state.consensus_result = {
                'consensus_achieved': False,
                'consensus_score': consensus_score,
                'debate_rounds': max_rounds,
                'final_recommendation': self._get_majority_recommendation(results)
            }
    
    async def _phase_final_integration(self, state: WorkflowState):
        """éšæ®µ 4: æœ€çµ‚æ•´åˆ"""
        state.current_phase = AnalysisPhase.FINAL_INTEGRATION
        state.progress_percentage = 90.0
        await self._notify_status_change(state)
        
        self.logger.info(f"é–‹å§‹æœ€çµ‚æ•´åˆéšæ®µ: {state.stock_id}")
        
        # å¦‚æœæœ‰æŠ•è³‡è¦åŠƒå¸«çµæœï¼Œç›´æ¥ä½¿ç”¨
        investment_planner_result = None
        for execution in state.analyst_executions.values():
            if (execution.analyst_id == 'investment_planner' and 
                execution.status == AnalysisStatus.COMPLETED):
                investment_planner_result = execution.result
                break
        
        if investment_planner_result:
            # ä½¿ç”¨æŠ•è³‡è¦åŠƒå¸«çš„ç¶œåˆçµæœ
            state.final_result = investment_planner_result
        else:
            # æ‰‹å‹•æ•´åˆæ‰€æœ‰åˆ†æå¸«çµæœ
            state.final_result = await self._integrate_results(state)
        
        # æ·»åŠ æ•´åˆå…ƒæ•¸æ“š
        state.integration_metadata = {
            'integration_method': 'investment_planner' if investment_planner_result else 'manual_integration',
            'analysts_used': list(state.analyst_executions.keys()),
            'successful_analyses': sum(1 for exec in state.analyst_executions.values() 
                                     if exec.status == AnalysisStatus.COMPLETED),
            'consensus_info': state.consensus_result,
            'data_sources_used': list(state.collected_data.keys())
        }
        
        state.progress_percentage = 95.0
        await self._notify_status_change(state)
        
        self.logger.info(f"æœ€çµ‚æ•´åˆéšæ®µå®Œæˆ: {state.stock_id}")
    
    async def _integrate_results(self, state: WorkflowState) -> AnalysisResult:
        """æ‰‹å‹•æ•´åˆåˆ†æçµæœ"""
        successful_results = []
        for execution in state.analyst_executions.values():
            if execution.status == AnalysisStatus.COMPLETED and execution.result:
                successful_results.append(execution.result)
        
        if not successful_results:
            # æ²’æœ‰æˆåŠŸçš„åˆ†æçµæœ
            from ..agents.analysts.base_analyst import AnalysisResult, AnalysisConfidenceLevel
            return AnalysisResult(
                analyst_id='trading_graph_integration',
                stock_id=state.stock_id,
                analysis_date=datetime.now().strftime('%Y-%m-%d'),
                analysis_type=AnalysisType.INVESTMENT_PLANNING,
                recommendation='HOLD',
                confidence=0.0,
                confidence_level=AnalysisConfidenceLevel.VERY_LOW,
                reasoning=['æ‰€æœ‰åˆ†æå¸«åŸ·è¡Œå¤±æ•—ï¼Œç„¡æ³•æä¾›å»ºè­°'],
                risk_factors=['ç³»çµ±åˆ†æå¤±æ•—é¢¨éšª']
            )
        
        # ç°¡å–®çš„æŠ•ç¥¨æ©Ÿåˆ¶
        recommendations = [r.recommendation for r in successful_results]
        recommendation_counts = {}
        for rec in recommendations:
            recommendation_counts[rec] = recommendation_counts.get(rec, 0) + 1
        
        final_recommendation = max(recommendation_counts.items(), key=lambda x: x[1])[0]
        
        # è¨ˆç®—å¹³å‡ä¿¡å¿ƒåº¦
        avg_confidence = sum(r.confidence for r in successful_results) / len(successful_results)
        
        # æ”¶é›†æ‰€æœ‰ç†ç”±
        all_reasoning = []
        for result in successful_results:
            all_reasoning.extend([f"[{result.analyst_id}] {reason}" for reason in result.reasoning or []])
        
        # æ”¶é›†æ‰€æœ‰é¢¨éšªå› å­
        all_risk_factors = []
        for result in successful_results:
            if result.risk_factors:
                all_risk_factors.extend([f"[{result.analyst_id}] {risk}" for risk in result.risk_factors])
        
        # è¨ˆç®—ç›®æ¨™åƒ¹
        target_prices = [r.target_price for r in successful_results if r.target_price]
        avg_target_price = sum(target_prices) / len(target_prices) if target_prices else None
        
        from ..agents.analysts.base_analyst import AnalysisResult, AnalysisConfidenceLevel
        return AnalysisResult(
            analyst_id='trading_graph_integration',
            stock_id=state.stock_id,
            analysis_date=datetime.now().strftime('%Y-%m-%d'),
            analysis_type=AnalysisType.INVESTMENT_PLANNING,
            recommendation=final_recommendation,
            confidence=avg_confidence,
            confidence_level=AnalysisConfidenceLevel.HIGH if avg_confidence > 0.7 else AnalysisConfidenceLevel.MODERATE,
            target_price=avg_target_price,
            reasoning=all_reasoning[:10],  # é™åˆ¶ç†ç”±æ•¸é‡
            risk_factors=all_risk_factors[:8]  # é™åˆ¶é¢¨éšªå› å­æ•¸é‡
        )
    
    def _record_session_metrics(self, state: WorkflowState):
        """è¨˜éŒ„æœƒè©±æŒ‡æ¨™"""
        metrics = {
            'session_id': state.session_id,
            'stock_id': state.stock_id,
            'user_tier': state.user_context.membership_tier.value,
            'total_execution_time': state.performance_metrics.get('total_execution_time', 0),
            'analysts_executed': len(state.analyst_executions),
            'successful_analyses': sum(1 for exec in state.analyst_executions.values() 
                                     if exec.status == AnalysisStatus.COMPLETED),
            'data_collection_errors': len(state.data_collection_errors),
            'final_status': state.overall_status.value,
            'timestamp': datetime.now().isoformat()
        }
        
        self.session_metrics[state.session_id] = metrics
    
    # ==================== å¤–éƒ¨æ¥å£æ–¹æ³• ====================
    
    def get_session_status(self, session_id: str) -> Optional[Dict[str, Any]]:
        """ç²å–æœƒè©±ç‹€æ…‹"""
        if session_id in self.active_sessions:
            return self.active_sessions[session_id].to_dict()
        return None
    
    def get_all_active_sessions(self) -> Dict[str, Dict[str, Any]]:
        """ç²å–æ‰€æœ‰æ´»èºæœƒè©±"""
        return {
            session_id: state.to_dict()
            for session_id, state in self.active_sessions.items()
        }
    
    def cancel_session(self, session_id: str) -> bool:
        """å–æ¶ˆæœƒè©±"""
        if session_id in self.active_sessions:
            state = self.active_sessions[session_id]
            state.overall_status = AnalysisStatus.CANCELLED
            state.end_time = datetime.now()
            state.add_warning("æœƒè©±è¢«ç”¨æˆ¶å–æ¶ˆ")
            return True
        return False
    
    def cleanup_completed_sessions(self, max_age_hours: int = 24):
        """æ¸…ç†å·²å®Œæˆçš„æœƒè©±"""
        current_time = datetime.now()
        sessions_to_remove = []
        
        for session_id, state in self.active_sessions.items():
            if state.overall_status in [AnalysisStatus.COMPLETED, AnalysisStatus.FAILED, AnalysisStatus.CANCELLED]:
                if state.end_time and (current_time - state.end_time).total_seconds() > max_age_hours * 3600:
                    sessions_to_remove.append(session_id)
        
        for session_id in sessions_to_remove:
            del self.active_sessions[session_id]
            self.logger.info(f"æ¸…ç†å·²å®Œæˆæœƒè©±: {session_id}")
        
        return len(sessions_to_remove)
    
    async def get_routing_decision_history(
        self, 
        limit: int = 50,
        task_type: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """è·å–è·¯ç”±å†³ç­–å†å²ï¼ˆé€æ˜åŒ–åŠŸèƒ½ï¼‰"""
        if not self.ai_task_router:
            return []
        
        try:
            return self.ai_task_router.get_decision_history(
                limit=limit, 
                task_type=task_type
            )
        except Exception as e:
            self.logger.error(f"è·å–è·¯ç”±å†³ç­–å†å²å¤±è´¥: {e}")
            return []
    
    async def force_routing_health_check(self) -> Dict[str, Any]:
        """å¼ºåˆ¶æ‰§è¡Œè·¯ç”±å™¨å¥åº·æ£€æŸ¥"""
        if not self.ai_task_router:
            return {'error': 'æ™ºèƒ½è·¯ç”±å™¨æœªå¯ç”¨'}
        
        try:
            return await self.ai_task_router.health_check()
        except Exception as e:
            self.logger.error(f"è·¯ç”±å™¨å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def update_routing_strategy(
        self, 
        strategy: RoutingStrategy,
        custom_weights: Optional[Dict[str, float]] = None
    ) -> bool:
        """åŠ¨æ€æ›´æ–°è·¯ç”±ç­–ç•¥"""
        if not self.ai_task_router:
            self.logger.warning("æ™ºèƒ½è·¯ç”±å™¨æœªå¯ç”¨ï¼Œæ— æ³•æ›´æ–°ç­–ç•¥")
            return False
        
        try:
            if custom_weights:
                from ..routing.ai_task_router import RoutingWeights
                weights = RoutingWeights(**custom_weights)
                return self.ai_task_router.update_strategy_weights(strategy, weights)
            return True
        except Exception as e:
            self.logger.error(f"æ›´æ–°è·¯ç”±ç­–ç•¥å¤±è´¥: {e}")
            return False
    
    def get_system_metrics(self) -> Dict[str, Any]:
        """ç²å–ç³»çµ±æŒ‡æ¨™"""
        active_count = len(self.active_sessions)
        completed_count = sum(1 for metrics in self.session_metrics.values() 
                             if metrics['final_status'] == 'completed')
        
        avg_execution_time = 0
        if self.session_metrics:
            total_time = sum(metrics.get('total_execution_time', 0) 
                           for metrics in self.session_metrics.values())
            avg_execution_time = total_time / len(self.session_metrics)
        
        # ç²å–å·¥ä½œæµç·¨æ’å™¨æŒ‡æ¨™
        orchestrator_info = {
            'registered_analysts': self.workflow_orchestrator.get_registered_analysts(),
            'execution_history': self.workflow_orchestrator.get_execution_history(5)
        }
        
        # è·å–æ™ºèƒ½è·¯ç”±å™¨ç»Ÿè®¡ä¿¡æ¯
        routing_info = {}
        if self.ai_task_router:
            try:
                routing_info = self.ai_task_router.get_routing_statistics()
                # ç§»é™¤ awaitï¼Œå› ç‚ºé€™æ˜¯åŒæ­¥å‡½æ•¸
                routing_info['health_status'] = 'available'
            except Exception as e:
                routing_info = {'error': f'è·å–è·¯ç”±ç»Ÿè®¡å¤±è´¥: {str(e)}'}
        
        return {
            'active_sessions': active_count,
            'completed_sessions': completed_count,
            'total_sessions': len(self.session_metrics),
            'average_execution_time': avg_execution_time,
            'available_analysts': list(self.analysts.keys()),
            'workflow_orchestrator': orchestrator_info,
            'intelligent_routing': {
                'enabled': self.enable_intelligent_routing,
                'router_active': self.ai_task_router is not None,
                'transparency_enabled': self.routing_transparency.get('enabled', True),
                'statistics': routing_info
            },
            'system_uptime': (datetime.now() - datetime.now()).total_seconds(),  # å¯¦éš›æ‡‰è©²è¨˜éŒ„å•Ÿå‹•æ™‚é–“
            'timestamp': datetime.now().isoformat()
        }

# ä¾¿åˆ©å‡½æ•¸
async def create_trading_graph(config: Optional[Dict[str, Any]] = None) -> TradingAgentsGraph:
    """å‰µå»ºä¸¦åˆå§‹åŒ–äº¤æ˜“åœ–"""
    graph = TradingAgentsGraph(config)
    await graph.initialize()
    return graph

if __name__ == "__main__":
    # æ¸¬è©¦è…³æœ¬
    async def test_trading_graph():
        print("æ¸¬è©¦ TradingAgentsGraph å·¥ä½œæµå¼•æ“")
        
        # å‰µå»ºå·¥ä½œæµå¼•æ“
        graph = await create_trading_graph()
        
        # æ¨¡æ“¬ç”¨æˆ¶ä¸Šä¸‹æ–‡
        from ..utils.user_context import UserContext, TierType, UserPermissions
        user_context = UserContext(
            user_id='test_user',
            membership_tier=TierType.DIAMOND,
            permissions=UserPermissions()
        )
        
        # æ·»åŠ ç‹€æ…‹å›èª¿
        def status_callback(state):
            print(f"ç‹€æ…‹æ›´æ–°: {state.current_phase.value} - {state.progress_percentage:.1f}%")
        
        graph.add_status_callback(status_callback)
        
        # é–‹å§‹åˆ†æ
        session_id = await graph.analyze_stock("2330", user_context)
        print(f"åˆ†ææœƒè©±é–‹å§‹: {session_id}")
        
        # ç­‰å¾…åˆ†æå®Œæˆ
        while True:
            status = graph.get_session_status(session_id)
            if status and status['overall_status'] in ['completed', 'failed', 'cancelled']:
                break
            await asyncio.sleep(1)
        
        # é¡¯ç¤ºæœ€çµ‚çµæœ
        final_status = graph.get_session_status(session_id)
        print(f"åˆ†æå®Œæˆ: {final_status['overall_status']}")
        
        if final_status.get('final_result'):
            result = final_status['final_result']
            print(f"æœ€çµ‚å»ºè­°: {result['recommendation']}")
            print(f"ä¿¡å¿ƒåº¦: {result['confidence']:.2f}")
            print(f"ç›®æ¨™åƒ¹: {result.get('target_price')}")
        
        print("TradingAgentsGraph æ¸¬è©¦å®Œæˆ")
    
    asyncio.run(test_trading_graph())