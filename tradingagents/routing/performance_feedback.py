#!/usr/bin/env python3
"""
Performance Feedback System - æ€§èƒ½åé¥‹å’Œå‹•æ…‹ç­–ç•¥èª¿æ•´
GPT-OSSæ•´åˆä»»å‹™1.3.1 - æ€§èƒ½åé¥‹æ©Ÿåˆ¶

åŸºæ–¼å¯¦éš›åŸ·è¡Œçµæœå„ªåŒ–è·¯ç”±æ±ºç­–ï¼š
- æ€§èƒ½åé¥‹æ”¶é›†å’Œåˆ†æ
- å‹•æ…‹æ¬Šé‡èª¿æ•´ç®—æ³•
- è‡ªé©æ‡‰ç­–ç•¥å„ªåŒ–
- A/Bæ¸¬è©¦æ”¯æŒ
"""

import logging
import statistics
from datetime import datetime, timezone, timedelta
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass, field
from enum import Enum

from .ai_task_router import (
    RoutingStrategy, RoutingWeights, DecisionFactor,
    ModelScore, RoutingContext, DecisionAudit
)

logger = logging.getLogger(__name__)

# ==================== æ€§èƒ½åé¥‹æ•¸æ“šçµæ§‹ ====================

class FeedbackType(Enum):
    """åé¥‹é¡å‹æšèˆ‰"""
    COST_ACTUAL = "cost_actual"           # å¯¦éš›æˆæœ¬åé¥‹
    LATENCY_ACTUAL = "latency_actual"     # å¯¦éš›å»¶é²åé¥‹  
    QUALITY_ACTUAL = "quality_actual"     # å¯¦éš›å“è³ªåé¥‹
    SUCCESS_FAILURE = "success_failure"   # æˆåŠŸå¤±æ•—åé¥‹
    USER_SATISFACTION = "user_satisfaction"  # ç”¨æˆ¶æ»¿æ„åº¦

@dataclass
class PerformanceFeedback:
    """æ€§èƒ½åé¥‹è¨˜éŒ„"""
    feedback_id: str
    decision_id: str
    request_id: str
    provider: str
    model_id: str
    task_type: str
    feedback_type: FeedbackType
    predicted_value: float
    actual_value: float
    variance: float = field(init=False)
    accuracy_score: float = field(init=False)
    timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def __post_init__(self):
        """è¨ˆç®—æ–¹å·®å’Œæº–ç¢ºåº¦"""
        self.variance = abs(self.actual_value - self.predicted_value)
        
        # æº–ç¢ºåº¦è¨ˆç®—ï¼ˆé¿å…é™¤é›¶ï¼‰
        if self.predicted_value == 0:
            self.accuracy_score = 1.0 if self.actual_value == 0 else 0.0
        else:
            # ç›¸å°èª¤å·®è½‰æ›ç‚ºæº–ç¢ºåº¦åˆ†æ•¸ (0-1)
            relative_error = abs(self.variance) / abs(self.predicted_value)
            self.accuracy_score = max(0.0, 1.0 - relative_error)

@dataclass
class ModelPerformanceProfile:
    """æ¨¡å‹æ€§èƒ½æª”æ¡ˆ"""
    provider: str
    model_id: str
    cost_prediction_accuracy: float = 0.8
    latency_prediction_accuracy: float = 0.8
    quality_prediction_accuracy: float = 0.8
    success_rate: float = 0.95
    total_feedback_count: int = 0
    recent_feedback_count: int = 0
    last_updated: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    
    # é æ¸¬èª¤å·®çµ±è¨ˆ
    cost_variance_stats: Dict[str, float] = field(default_factory=dict)
    latency_variance_stats: Dict[str, float] = field(default_factory=dict)
    quality_variance_stats: Dict[str, float] = field(default_factory=dict)

# ==================== æ€§èƒ½åé¥‹ç³»çµ± ====================

class PerformanceFeedbackSystem:
    """
    æ€§èƒ½åé¥‹å’Œå‹•æ…‹ç­–ç•¥èª¿æ•´ç³»çµ±
    
    åŠŸèƒ½ï¼š
    1. æ”¶é›†å’Œåˆ†æå¯¦éš›åŸ·è¡Œçµæœ
    2. è¨ˆç®—é æ¸¬æº–ç¢ºåº¦å’Œåå·®
    3. å‹•æ…‹èª¿æ•´è·¯ç”±æ¬Šé‡
    4. æä¾›æ€§èƒ½æ´å¯Ÿå’Œå»ºè­°
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        åˆå§‹åŒ–æ€§èƒ½åé¥‹ç³»çµ±
        
        Args:
            config: åé¥‹ç³»çµ±é…ç½®
        """
        self.config = config or {}
        self._load_default_config()
        
        # åé¥‹è¨˜éŒ„å­˜å„²
        self.feedback_history: List[PerformanceFeedback] = []
        self.max_feedback_history = self.config.get('max_feedback_history', 5000)
        
        # æ¨¡å‹æ€§èƒ½æª”æ¡ˆ
        self.model_profiles: Dict[str, ModelPerformanceProfile] = {}
        
        # æ¬Šé‡èª¿æ•´æ­·å²
        self.weight_adjustment_history: List[Dict[str, Any]] = []
        
        # çµ±è¨ˆä¿¡æ¯
        self.stats = {
            'total_feedback_received': 0,
            'weight_adjustments_made': 0,
            'average_prediction_accuracy': {},
            'best_performing_models': {},
            'worst_performing_models': {},
            'last_update': datetime.now(timezone.utc)
        }
        
        self.logger = logger
        self._initialized = False
    
    def _load_default_config(self):
        """è¼‰å…¥é è¨­é…ç½®"""
        defaults = {
            'enable_auto_adjustment': True,
            'adjustment_sensitivity': 0.1,      # èª¿æ•´æ•æ„Ÿåº¦ (0.0-1.0)
            'min_feedback_for_adjustment': 20,  # æœ€å°‘åé¥‹æ•¸é‡æ‰é€²è¡Œèª¿æ•´
            'max_weight_change_per_step': 0.05,  # æ¯æ¬¡æœ€å¤§æ¬Šé‡è®ŠåŒ–
            'prediction_accuracy_threshold': 0.7,  # é æ¸¬æº–ç¢ºåº¦é–¾å€¼
            'performance_window_hours': 72,     # æ€§èƒ½è©•ä¼°æ™‚é–“çª—å£
            'auto_adjustment_interval': 3600,   # è‡ªå‹•èª¿æ•´é–“éš”ï¼ˆç§’ï¼‰
            'variance_tolerance': {
                'cost': 0.15,      # æˆæœ¬æ–¹å·®å®¹å¿åº¦
                'latency': 0.20,   # å»¶é²æ–¹å·®å®¹å¿åº¦  
                'quality': 0.10    # å“è³ªæ–¹å·®å®¹å¿åº¦
            }
        }
        
        for key, value in defaults.items():
            if key not in self.config:
                self.config[key] = value
    
    async def initialize(self) -> bool:
        """åˆå§‹åŒ–åé¥‹ç³»çµ±"""
        try:
            self.logger.info("ğŸš€ Initializing Performance Feedback System...")
            
            # è¼‰å…¥æ­·å²æ€§èƒ½æ•¸æ“šï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
            await self._load_historical_data()
            
            self._initialized = True
            self.logger.info("âœ… Performance Feedback System initialized successfully")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Failed to initialize Performance Feedback System: {e}")
            return False
    
    async def _load_historical_data(self):
        """è¼‰å…¥æ­·å²æ€§èƒ½æ•¸æ“š"""
        # é€™è£¡å¯ä»¥å¾æ•¸æ“šåº«æˆ–æª”æ¡ˆç³»çµ±è¼‰å…¥æ­·å²æ•¸æ“š
        # ç›®å‰ä½œç‚ºç©ºå¯¦ç¾ï¼Œå¯æ ¹æ“šéœ€è¦æ“´å±•
        pass
    
    # ==================== åé¥‹æ”¶é›†æ¥å£ ====================
    
    def record_execution_feedback(
        self,
        decision_id: str,
        request_id: str,
        provider: str,
        model_id: str,
        task_type: str,
        predicted_cost: float,
        actual_cost: float,
        predicted_latency: float,
        actual_latency: float,
        predicted_quality: float,
        actual_quality: float,
        execution_success: bool,
        user_satisfaction: Optional[float] = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> List[str]:
        """
        è¨˜éŒ„åŸ·è¡Œçµæœåé¥‹
        
        Returns:
            åé¥‹è¨˜éŒ„IDåˆ—è¡¨
        """
        if not self._initialized:
            return []
        
        feedback_ids = []
        
        try:
            # è¨˜éŒ„æˆæœ¬åé¥‹
            cost_feedback = PerformanceFeedback(
                feedback_id=f"cost_{decision_id}_{datetime.now().timestamp()}",
                decision_id=decision_id,
                request_id=request_id,
                provider=provider,
                model_id=model_id,
                task_type=task_type,
                feedback_type=FeedbackType.COST_ACTUAL,
                predicted_value=predicted_cost,
                actual_value=actual_cost,
                metadata=metadata or {}
            )
            
            # è¨˜éŒ„å»¶é²åé¥‹
            latency_feedback = PerformanceFeedback(
                feedback_id=f"latency_{decision_id}_{datetime.now().timestamp()}",
                decision_id=decision_id,
                request_id=request_id,
                provider=provider,
                model_id=model_id,
                task_type=task_type,
                feedback_type=FeedbackType.LATENCY_ACTUAL,
                predicted_value=predicted_latency,
                actual_value=actual_latency,
                metadata=metadata or {}
            )
            
            # è¨˜éŒ„å“è³ªåé¥‹
            quality_feedback = PerformanceFeedback(
                feedback_id=f"quality_{decision_id}_{datetime.now().timestamp()}",
                decision_id=decision_id,
                request_id=request_id,
                provider=provider,
                model_id=model_id,
                task_type=task_type,
                feedback_type=FeedbackType.QUALITY_ACTUAL,
                predicted_value=predicted_quality,
                actual_value=actual_quality,
                metadata=metadata or {}
            )
            
            # æ·»åŠ åˆ°æ­·å²è¨˜éŒ„
            feedbacks = [cost_feedback, latency_feedback, quality_feedback]
            
            for feedback in feedbacks:
                self.feedback_history.append(feedback)
                feedback_ids.append(feedback.feedback_id)
            
            # è¨˜éŒ„æˆåŠŸå¤±æ•—åé¥‹
            if not execution_success:
                failure_feedback = PerformanceFeedback(
                    feedback_id=f"failure_{decision_id}_{datetime.now().timestamp()}",
                    decision_id=decision_id,
                    request_id=request_id,
                    provider=provider,
                    model_id=model_id,
                    task_type=task_type,
                    feedback_type=FeedbackType.SUCCESS_FAILURE,
                    predicted_value=1.0,  # é æœŸæˆåŠŸ
                    actual_value=0.0,     # å¯¦éš›å¤±æ•—
                    metadata=metadata or {}
                )
                self.feedback_history.append(failure_feedback)
                feedback_ids.append(failure_feedback.feedback_id)
            
            # è¨˜éŒ„ç”¨æˆ¶æ»¿æ„åº¦ï¼ˆå¦‚æœæä¾›ï¼‰
            if user_satisfaction is not None:
                satisfaction_feedback = PerformanceFeedback(
                    feedback_id=f"satisfaction_{decision_id}_{datetime.now().timestamp()}",
                    decision_id=decision_id,
                    request_id=request_id,
                    provider=provider,
                    model_id=model_id,
                    task_type=task_type,
                    feedback_type=FeedbackType.USER_SATISFACTION,
                    predicted_value=0.8,  # é æœŸæ»¿æ„åº¦
                    actual_value=user_satisfaction,
                    metadata=metadata or {}
                )
                self.feedback_history.append(satisfaction_feedback)
                feedback_ids.append(satisfaction_feedback.feedback_id)
            
            # æ›´æ–°æ¨¡å‹æ€§èƒ½æª”æ¡ˆ
            self._update_model_performance_profile(provider, model_id, feedbacks)
            
            # æ›´æ–°çµ±è¨ˆ
            self.stats['total_feedback_received'] += len(feedbacks)
            
            # ç¶­è­·æ­·å²è¨˜éŒ„å¤§å°
            self._maintain_feedback_history()
            
            # æª¢æŸ¥æ˜¯å¦éœ€è¦è‡ªå‹•èª¿æ•´æ¬Šé‡
            if self.config.get('enable_auto_adjustment', True):
                # ç§»é™¤ awaitï¼Œå› ç‚ºé€™æ˜¯åŒæ­¥å‡½æ•¸
                self._check_auto_adjustment_trigger_sync()
            
            self.logger.debug(f"âœ… Recorded {len(feedbacks)} feedback records for {provider}/{model_id}")
            
            return feedback_ids
            
        except Exception as e:
            self.logger.error(f"âŒ Failed to record execution feedback: {e}")
            return []
    
    def _update_model_performance_profile(
        self,
        provider: str,
        model_id: str,
        feedbacks: List[PerformanceFeedback]
    ):
        """æ›´æ–°æ¨¡å‹æ€§èƒ½æª”æ¡ˆ"""
        model_key = f"{provider}/{model_id}"
        
        if model_key not in self.model_profiles:
            self.model_profiles[model_key] = ModelPerformanceProfile(
                provider=provider,
                model_id=model_id
            )
        
        profile = self.model_profiles[model_key]
        
        # æ›´æ–°å„é …æŒ‡æ¨™
        for feedback in feedbacks:
            profile.total_feedback_count += 1
            profile.recent_feedback_count += 1
            
            if feedback.feedback_type == FeedbackType.COST_ACTUAL:
                # æ›´æ–°æˆæœ¬é æ¸¬æº–ç¢ºåº¦
                self._update_prediction_accuracy(
                    profile, 'cost_prediction_accuracy', feedback.accuracy_score
                )
                self._update_variance_stats(
                    profile.cost_variance_stats, feedback.variance
                )
            
            elif feedback.feedback_type == FeedbackType.LATENCY_ACTUAL:
                # æ›´æ–°å»¶é²é æ¸¬æº–ç¢ºåº¦
                self._update_prediction_accuracy(
                    profile, 'latency_prediction_accuracy', feedback.accuracy_score
                )
                self._update_variance_stats(
                    profile.latency_variance_stats, feedback.variance
                )
            
            elif feedback.feedback_type == FeedbackType.QUALITY_ACTUAL:
                # æ›´æ–°å“è³ªé æ¸¬æº–ç¢ºåº¦
                self._update_prediction_accuracy(
                    profile, 'quality_prediction_accuracy', feedback.accuracy_score
                )
                self._update_variance_stats(
                    profile.quality_variance_stats, feedback.variance
                )
            
            elif feedback.feedback_type == FeedbackType.SUCCESS_FAILURE:
                # æ›´æ–°æˆåŠŸç‡
                if feedback.actual_value == 0.0:  # å¤±æ•—
                    total_attempts = profile.total_feedback_count
                    success_count = profile.success_rate * (total_attempts - 1)
                    profile.success_rate = success_count / total_attempts
        
        profile.last_updated = datetime.now(timezone.utc)
    
    def _update_prediction_accuracy(
        self,
        profile: ModelPerformanceProfile,
        accuracy_field: str,
        new_accuracy: float
    ):
        """æ›´æ–°é æ¸¬æº–ç¢ºåº¦ï¼ˆä½¿ç”¨æŒ‡æ•¸ç§»å‹•å¹³å‡ï¼‰"""
        current_accuracy = getattr(profile, accuracy_field)
        alpha = 0.1  # å­¸ç¿’ç‡
        updated_accuracy = current_accuracy * (1 - alpha) + new_accuracy * alpha
        setattr(profile, accuracy_field, updated_accuracy)
    
    def _update_variance_stats(self, variance_stats: Dict[str, float], new_variance: float):
        """æ›´æ–°æ–¹å·®çµ±è¨ˆ"""
        if 'count' not in variance_stats:
            variance_stats.update({
                'count': 0,
                'sum': 0.0,
                'sum_squares': 0.0,
                'mean': 0.0,
                'std': 0.0
            })
        
        # æ›´æ–°çµ±è¨ˆæ•¸æ“š
        variance_stats['count'] += 1
        variance_stats['sum'] += new_variance
        variance_stats['sum_squares'] += new_variance ** 2
        
        n = variance_stats['count']
        variance_stats['mean'] = variance_stats['sum'] / n
        
        if n > 1:
            variance = (variance_stats['sum_squares'] / n) - (variance_stats['mean'] ** 2)
            variance_stats['std'] = max(0.0, variance) ** 0.5
        else:
            variance_stats['std'] = 0.0
    
    def _maintain_feedback_history(self):
        """ç¶­è­·åé¥‹æ­·å²è¨˜éŒ„å¤§å°"""
        if len(self.feedback_history) > self.max_feedback_history:
            # ä¿ç•™æœ€è¿‘çš„è¨˜éŒ„
            self.feedback_history = self.feedback_history[-self.max_feedback_history:]
    
    def _check_auto_adjustment_trigger_sync(self):
        """æª¢æŸ¥æ˜¯å¦è§¸ç™¼è‡ªå‹•èª¿æ•´ (åŒæ­¥ç‰ˆæœ¬)"""
        try:
            # æª¢æŸ¥æ˜¯å¦æœ‰è¶³å¤ çš„åé¥‹æ•¸æ“š
            recent_feedback_count = len([
                f for f in self.feedback_history
                if (datetime.now(timezone.utc) - f.timestamp).total_seconds() < 
                   self.config.get('performance_window_hours', 72) * 3600
            ])
            
            min_feedback = self.config.get('min_feedback_for_adjustment', 20)
            if recent_feedback_count >= min_feedback:
                self.logger.info(f"ğŸ’¡ Auto-adjustment trigger: {recent_feedback_count} recent feedback items")
                # ç°¡åŒ–ç‰ˆæœ¬ï¼Œåƒ…è¨˜éŒ„è§¸ç™¼ä¿¡æ¯
                
        except Exception as e:
            self.logger.error(f"âŒ Auto-adjustment trigger check failed: {e}")

    async def _check_auto_adjustment_trigger(self):
        """æª¢æŸ¥æ˜¯å¦è§¸ç™¼è‡ªå‹•èª¿æ•´"""
        try:
            # æª¢æŸ¥æ˜¯å¦æœ‰è¶³å¤ çš„åé¥‹æ•¸æ“š
            recent_feedback_count = len([
                f for f in self.feedback_history
                if (datetime.now(timezone.utc) - f.timestamp).total_seconds() < 
                   self.config.get('performance_window_hours', 72) * 3600
            ])
            
            min_feedback = self.config.get('min_feedback_for_adjustment', 20)
            if recent_feedback_count >= min_feedback:
                # åˆ†ææ€§èƒ½ä¸¦å»ºè­°èª¿æ•´
                adjustments = await self._analyze_and_suggest_adjustments()
                
                if adjustments:
                    self.logger.info(f"ğŸ’¡ Auto-adjustment suggestions generated: {len(adjustments)} items")
                    # é€™è£¡å¯ä»¥é¸æ“‡è‡ªå‹•æ‡‰ç”¨èª¿æ•´æˆ–è€…åƒ…è¨˜éŒ„å»ºè­°
                    for adjustment in adjustments:
                        self.logger.info(f"   â€¢ {adjustment['reasoning']}")
            
        except Exception as e:
            self.logger.error(f"âŒ Auto adjustment check failed: {e}")
    
    # ==================== æ€§èƒ½åˆ†æå’Œæ¬Šé‡èª¿æ•´ ====================
    
    async def analyze_performance_trends(
        self,
        hours_back: int = 72,
        min_samples: int = 10
    ) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½è¶¨å‹¢"""
        try:
            cutoff_time = datetime.now(timezone.utc) - timedelta(hours=hours_back)
            recent_feedback = [
                f for f in self.feedback_history
                if f.timestamp >= cutoff_time
            ]
            
            if len(recent_feedback) < min_samples:
                return {
                    'status': 'insufficient_data',
                    'message': f'éœ€è¦è‡³å°‘ {min_samples} å€‹æ¨£æœ¬ï¼Œç›®å‰åªæœ‰ {len(recent_feedback)} å€‹',
                    'analysis': {}
                }
            
            analysis = {
                'overall_accuracy': {},
                'model_performance': {},
                'prediction_errors': {},
                'success_rates': {},
                'variance_analysis': {}
            }
            
            # æŒ‰æ¨¡å‹åˆ†çµ„åˆ†æ
            model_groups = {}
            for feedback in recent_feedback:
                model_key = f"{feedback.provider}/{feedback.model_id}"
                if model_key not in model_groups:
                    model_groups[model_key] = []
                model_groups[model_key].append(feedback)
            
            # åˆ†æå„æ¨¡å‹æ€§èƒ½
            for model_key, model_feedback in model_groups.items():
                if len(model_feedback) < min_samples:
                    continue
                
                # æŒ‰åé¥‹é¡å‹åˆ†çµ„
                feedback_by_type = {}
                for feedback in model_feedback:
                    feedback_type = feedback.feedback_type
                    if feedback_type not in feedback_by_type:
                        feedback_by_type[feedback_type] = []
                    feedback_by_type[feedback_type].append(feedback)
                
                model_analysis = {}
                
                # åˆ†æå„é¡å‹åé¥‹
                for feedback_type, type_feedback in feedback_by_type.items():
                    if len(type_feedback) < 3:  # éœ€è¦æœ€å°‘æ¨£æœ¬
                        continue
                    
                    accuracies = [f.accuracy_score for f in type_feedback]
                    variances = [f.variance for f in type_feedback]
                    
                    model_analysis[feedback_type.value] = {
                        'sample_count': len(type_feedback),
                        'avg_accuracy': statistics.mean(accuracies),
                        'accuracy_std': statistics.stdev(accuracies) if len(accuracies) > 1 else 0.0,
                        'avg_variance': statistics.mean(variances),
                        'variance_std': statistics.stdev(variances) if len(variances) > 1 else 0.0,
                        'accuracy_trend': self._calculate_trend([f.accuracy_score for f in type_feedback[-10:]]),
                        'variance_trend': self._calculate_trend([f.variance for f in type_feedback[-10:]])
                    }
                
                analysis['model_performance'][model_key] = model_analysis
            
            # æ•´é«”æº–ç¢ºåº¦åˆ†æ
            for feedback_type in [FeedbackType.COST_ACTUAL, FeedbackType.LATENCY_ACTUAL, FeedbackType.QUALITY_ACTUAL]:
                type_feedback = [f for f in recent_feedback if f.feedback_type == feedback_type]
                if len(type_feedback) >= min_samples:
                    accuracies = [f.accuracy_score for f in type_feedback]
                    variances = [f.variance for f in type_feedback]
                    
                    analysis['overall_accuracy'][feedback_type.value] = {
                        'avg_accuracy': statistics.mean(accuracies),
                        'accuracy_std': statistics.stdev(accuracies) if len(accuracies) > 1 else 0.0,
                        'avg_variance': statistics.mean(variances),
                        'sample_count': len(type_feedback)
                    }
            
            # æˆåŠŸç‡åˆ†æ
            success_feedback = [f for f in recent_feedback if f.feedback_type == FeedbackType.SUCCESS_FAILURE]
            if success_feedback:
                total_attempts = len(success_feedback) + len([f for f in recent_feedback if f.feedback_type != FeedbackType.SUCCESS_FAILURE])
                failures = len(success_feedback)
                analysis['success_rates']['overall'] = {
                    'success_rate': (total_attempts - failures) / total_attempts,
                    'total_attempts': total_attempts,
                    'failures': failures
                }
            
            return {
                'status': 'success',
                'analysis_period_hours': hours_back,
                'total_samples': len(recent_feedback),
                'analysis': analysis,
                'timestamp': datetime.now(timezone.utc).isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Performance trend analysis failed: {e}")
            return {
                'status': 'error',
                'error': str(e),
                'timestamp': datetime.now(timezone.utc).isoformat()
            }
    
    def _calculate_trend(self, values: List[float]) -> str:
        """è¨ˆç®—è¶¨å‹¢ï¼ˆä¸Šå‡/ä¸‹é™/å¹³ç©©ï¼‰"""
        if len(values) < 3:
            return 'insufficient_data'
        
        # ä½¿ç”¨ç·šæ€§å›æ­¸æ–œç‡åˆ¤æ–·è¶¨å‹¢
        n = len(values)
        x = list(range(n))
        
        # è¨ˆç®—æ–œç‡
        x_mean = statistics.mean(x)
        y_mean = statistics.mean(values)
        
        numerator = sum((x[i] - x_mean) * (values[i] - y_mean) for i in range(n))
        denominator = sum((x[i] - x_mean) ** 2 for i in range(n))
        
        if denominator == 0:
            return 'stable'
        
        slope = numerator / denominator
        
        # è¶¨å‹¢åˆ¤æ–·
        if slope > 0.01:
            return 'improving'
        elif slope < -0.01:
            return 'declining'
        else:
            return 'stable'
    
    async def _analyze_and_suggest_adjustments(self) -> List[Dict[str, Any]]:
        """åˆ†ææ€§èƒ½ä¸¦å»ºè­°æ¬Šé‡èª¿æ•´"""
        try:
            adjustments = []
            
            # ç²å–æ€§èƒ½åˆ†æçµæœ
            performance_analysis = await self.analyze_performance_trends(
                hours_back=self.config.get('performance_window_hours', 72)
            )
            
            if performance_analysis['status'] != 'success':
                return adjustments
            
            analysis = performance_analysis['analysis']
            
            # åˆ†ææ•´é«”æº–ç¢ºåº¦å•é¡Œ
            overall_accuracy = analysis.get('overall_accuracy', {})
            variance_tolerance = self.config.get('variance_tolerance', {})
            
            # æˆæœ¬é æ¸¬èª¿æ•´å»ºè­°
            if 'cost_actual' in overall_accuracy:
                cost_acc = overall_accuracy['cost_actual']['avg_accuracy']
                cost_var = overall_accuracy['cost_actual']['avg_variance']
                
                if cost_acc < self.config.get('prediction_accuracy_threshold', 0.7):
                    adjustments.append({
                        'type': 'weight_adjustment',
                        'factor': DecisionFactor.COST,
                        'current_importance': 'unknown',  # éœ€è¦å¾ç•¶å‰è·¯ç”±å™¨ç²å–
                        'suggested_change': -0.05 if cost_var > variance_tolerance.get('cost', 0.15) else 0.02,
                        'reasoning': f"æˆæœ¬é æ¸¬æº–ç¢ºåº¦ {cost_acc:.3f} ä½æ–¼é–¾å€¼ï¼Œå»ºè­°{'é™ä½' if cost_var > variance_tolerance.get('cost', 0.15) else 'å¾®èª¿'}æˆæœ¬æ¬Šé‡"
                    })
            
            # å»¶é²é æ¸¬èª¿æ•´å»ºè­°
            if 'latency_actual' in overall_accuracy:
                latency_acc = overall_accuracy['latency_actual']['avg_accuracy']
                latency_var = overall_accuracy['latency_actual']['avg_variance']
                
                if latency_acc < self.config.get('prediction_accuracy_threshold', 0.7):
                    adjustments.append({
                        'type': 'weight_adjustment',
                        'factor': DecisionFactor.LATENCY,
                        'suggested_change': -0.03 if latency_var > variance_tolerance.get('latency', 0.20) else 0.02,
                        'reasoning': f"å»¶é²é æ¸¬æº–ç¢ºåº¦ {latency_acc:.3f} ä½æ–¼é–¾å€¼ï¼Œå»ºè­°èª¿æ•´å»¶é²æ¬Šé‡"
                    })
            
            # å“è³ªé æ¸¬èª¿æ•´å»ºè­°
            if 'quality_actual' in overall_accuracy:
                quality_acc = overall_accuracy['quality_actual']['avg_accuracy']
                quality_var = overall_accuracy['quality_actual']['avg_variance']
                
                if quality_acc < self.config.get('prediction_accuracy_threshold', 0.7):
                    adjustments.append({
                        'type': 'weight_adjustment',
                        'factor': DecisionFactor.QUALITY,
                        'suggested_change': 0.03 if quality_var < variance_tolerance.get('quality', 0.10) else -0.02,
                        'reasoning': f"å“è³ªé æ¸¬æº–ç¢ºåº¦ {quality_acc:.3f} ä½æ–¼é–¾å€¼ï¼Œå»ºè­°{'æé«˜' if quality_var < variance_tolerance.get('quality', 0.10) else 'é™ä½'}å“è³ªæ¬Šé‡"
                    })
            
            # æˆåŠŸç‡å•é¡Œèª¿æ•´å»ºè­°
            success_rates = analysis.get('success_rates', {})
            if 'overall' in success_rates:
                success_rate = success_rates['overall']['success_rate']
                if success_rate < 0.9:
                    adjustments.append({
                        'type': 'strategy_adjustment',
                        'suggestion': 'increase_availability_weight',
                        'reasoning': f"æ•´é«”æˆåŠŸç‡ {success_rate:.3f} åä½ï¼Œå»ºè­°æé«˜å¯ç”¨æ€§æ¬Šé‡"
                    })
            
            # æ¨¡å‹ç‰¹å®šèª¿æ•´å»ºè­°
            model_performance = analysis.get('model_performance', {})
            for model_key, model_analysis in model_performance.items():
                # æª¢æŸ¥æ¨¡å‹æ˜¯å¦æœ‰æ˜é¡¯çš„æ€§èƒ½å•é¡Œ
                has_accuracy_issues = any(
                    metrics.get('avg_accuracy', 1.0) < 0.6
                    for metrics in model_analysis.values()
                    if isinstance(metrics, dict)
                )
                
                if has_accuracy_issues:
                    adjustments.append({
                        'type': 'model_recommendation',
                        'model': model_key,
                        'suggestion': 'reduce_selection_probability',
                        'reasoning': f"æ¨¡å‹ {model_key} é æ¸¬æº–ç¢ºåº¦æŒçºŒåä½ï¼Œå»ºè­°é™ä½é¸æ“‡æ¦‚ç‡"
                    })
            
            return adjustments
            
        except Exception as e:
            self.logger.error(f"âŒ Adjustment analysis failed: {e}")
            return []
    
    def suggest_weight_adjustments(
        self,
        current_weights: RoutingWeights,
        strategy: RoutingStrategy,
        analysis_hours: int = 72
    ) -> Tuple[RoutingWeights, List[str]]:
        """
        åŸºæ–¼æ€§èƒ½åˆ†æå»ºè­°æ¬Šé‡èª¿æ•´
        
        Args:
            current_weights: ç•¶å‰æ¬Šé‡é…ç½®
            strategy: ç•¶å‰è·¯ç”±ç­–ç•¥
            analysis_hours: åˆ†ææ™‚é–“çª—å£
            
        Returns:
            (å»ºè­°çš„æ–°æ¬Šé‡, èª¿æ•´åŸå› åˆ—è¡¨)
        """
        try:
            # è¤‡è£½ç•¶å‰æ¬Šé‡ä½œç‚ºåŸºç¤
            new_weights = RoutingWeights(
                cost=current_weights.cost,
                latency=current_weights.latency,
                quality=current_weights.quality,
                availability=current_weights.availability,
                privacy=current_weights.privacy,
                user_preference=current_weights.user_preference
            )
            
            adjustment_reasons = []
            max_change = self.config.get('max_weight_change_per_step', 0.05)
            
            # ç²å–æœ€è¿‘çš„æ€§èƒ½æ•¸æ“š
            cutoff_time = datetime.now(timezone.utc) - timedelta(hours=analysis_hours)
            recent_feedback = [
                f for f in self.feedback_history
                if f.timestamp >= cutoff_time
            ]
            
            if len(recent_feedback) < self.config.get('min_feedback_for_adjustment', 20):
                return current_weights, ['æ•¸æ“šä¸è¶³ï¼Œç„¡æ³•é€²è¡Œæ¬Šé‡èª¿æ•´å»ºè­°']
            
            # æŒ‰åé¥‹é¡å‹åˆ†æ
            feedback_by_type = {}
            for feedback in recent_feedback:
                if feedback.feedback_type not in feedback_by_type:
                    feedback_by_type[feedback.feedback_type] = []
                feedback_by_type[feedback.feedback_type].append(feedback)
            
            # æˆæœ¬æ¬Šé‡èª¿æ•´
            cost_feedback = feedback_by_type.get(FeedbackType.COST_ACTUAL, [])
            if len(cost_feedback) >= 5:
                cost_accuracies = [f.accuracy_score for f in cost_feedback]
                avg_cost_accuracy = statistics.mean(cost_accuracies)
                cost_variance = statistics.mean([f.variance for f in cost_feedback])
                
                if avg_cost_accuracy < 0.7:
                    if cost_variance > self.config.get('variance_tolerance', {}).get('cost', 0.15):
                        # æˆæœ¬é æ¸¬ä¸æº–ä¸”æ–¹å·®å¤§ï¼Œé™ä½æˆæœ¬æ¬Šé‡
                        adjustment = min(max_change, new_weights.cost * 0.1)
                        new_weights.cost = max(0.05, new_weights.cost - adjustment)
                        adjustment_reasons.append(f"æˆæœ¬é æ¸¬æº–ç¢ºåº¦ä½ ({avg_cost_accuracy:.3f})ï¼Œé™ä½æˆæœ¬æ¬Šé‡")
                    else:
                        # æˆæœ¬é æ¸¬ä¸æº–ä½†æ–¹å·®å°ï¼Œå¯èƒ½éœ€è¦å¾®èª¿
                        adjustment = min(max_change * 0.5, 0.02)
                        new_weights.cost = min(0.6, new_weights.cost + adjustment)
                        adjustment_reasons.append(f"æˆæœ¬é æ¸¬éœ€è¦æ”¹é€²ï¼Œå¾®èª¿æˆæœ¬æ¬Šé‡")
            
            # å»¶é²æ¬Šé‡èª¿æ•´
            latency_feedback = feedback_by_type.get(FeedbackType.LATENCY_ACTUAL, [])
            if len(latency_feedback) >= 5:
                latency_accuracies = [f.accuracy_score for f in latency_feedback]
                avg_latency_accuracy = statistics.mean(latency_accuracies)
                
                if avg_latency_accuracy < 0.7:
                    # å»¶é²é æ¸¬ä¸æº–ç¢ºï¼Œæ ¹æ“šç­–ç•¥èª¿æ•´
                    if strategy in [RoutingStrategy.LATENCY_FIRST, RoutingStrategy.PERFORMANCE_OPTIMIZED]:
                        # å°å»¶é²æ•æ„Ÿçš„ç­–ç•¥ï¼Œéœ€è¦æ›´æº–ç¢ºçš„å»¶é²é æ¸¬
                        adjustment = min(max_change, 0.03)
                        new_weights.latency = min(0.6, new_weights.latency + adjustment)
                        adjustment_reasons.append(f"å»¶é²é æ¸¬æº–ç¢ºåº¦ä½ï¼Œæé«˜å»¶é²æ¬Šé‡ä»¥æ”¹å–„é æ¸¬")
                    else:
                        # éå»¶é²æ•æ„Ÿç­–ç•¥ï¼Œå¯ä»¥é™ä½å»¶é²æ¬Šé‡
                        adjustment = min(max_change, new_weights.latency * 0.1)
                        new_weights.latency = max(0.05, new_weights.latency - adjustment)
                        adjustment_reasons.append(f"å»¶é²é æ¸¬ä¸æº–ç¢ºï¼Œé™ä½å»¶é²æ¬Šé‡")
            
            # å“è³ªæ¬Šé‡èª¿æ•´
            quality_feedback = feedback_by_type.get(FeedbackType.QUALITY_ACTUAL, [])
            if len(quality_feedback) >= 5:
                quality_accuracies = [f.accuracy_score for f in quality_feedback]
                avg_quality_accuracy = statistics.mean(quality_accuracies)
                
                if avg_quality_accuracy > 0.85:
                    # å“è³ªé æ¸¬å¾ˆæº–ç¢ºï¼Œå¯ä»¥å¢åŠ å“è³ªæ¬Šé‡
                    adjustment = min(max_change * 0.5, 0.02)
                    new_weights.quality = min(0.6, new_weights.quality + adjustment)
                    adjustment_reasons.append(f"å“è³ªé æ¸¬æº–ç¢ºåº¦é«˜ ({avg_quality_accuracy:.3f})ï¼Œæé«˜å“è³ªæ¬Šé‡")
                elif avg_quality_accuracy < 0.6:
                    # å“è³ªé æ¸¬å¾ˆä¸æº–ç¢ºï¼Œé™ä½å“è³ªæ¬Šé‡
                    adjustment = min(max_change, new_weights.quality * 0.15)
                    new_weights.quality = max(0.1, new_weights.quality - adjustment)
                    adjustment_reasons.append(f"å“è³ªé æ¸¬æº–ç¢ºåº¦ä½ ({avg_quality_accuracy:.3f})ï¼Œé™ä½å“è³ªæ¬Šé‡")
            
            # å¤±æ•—ç‡èª¿æ•´
            failure_feedback = feedback_by_type.get(FeedbackType.SUCCESS_FAILURE, [])
            total_requests = len(recent_feedback)
            failure_rate = len(failure_feedback) / max(total_requests, 1)
            
            if failure_rate > 0.1:  # å¤±æ•—ç‡è¶…é10%
                # æé«˜å¯ç”¨æ€§æ¬Šé‡
                adjustment = min(max_change, 0.03)
                new_weights.availability = min(0.4, new_weights.availability + adjustment)
                adjustment_reasons.append(f"å¤±æ•—ç‡åé«˜ ({failure_rate:.3f})ï¼Œæé«˜å¯ç”¨æ€§æ¬Šé‡")
            
            # æ¨™æº–åŒ–æ¬Šé‡
            new_weights.normalize()
            
            # æª¢æŸ¥èª¿æ•´æ˜¯å¦æœ‰æ„ç¾©
            total_change = sum([
                abs(new_weights.cost - current_weights.cost),
                abs(new_weights.latency - current_weights.latency),
                abs(new_weights.quality - current_weights.quality),
                abs(new_weights.availability - current_weights.availability),
                abs(new_weights.privacy - current_weights.privacy),
                abs(new_weights.user_preference - current_weights.user_preference)
            ])
            
            if total_change < 0.01:  # è®ŠåŒ–å¤ªå°
                return current_weights, ['åŸºæ–¼ç•¶å‰æ€§èƒ½åˆ†æï¼Œæ¬Šé‡é…ç½®å·²ç¶“ç›¸ç•¶åˆé©']
            
            return new_weights, adjustment_reasons
            
        except Exception as e:
            self.logger.error(f"âŒ Weight adjustment suggestion failed: {e}")
            return current_weights, [f'æ¬Šé‡èª¿æ•´å»ºè­°å¤±æ•—: {str(e)}']
    
    def apply_weight_adjustments(
        self,
        router,  # AITaskRouter instance
        suggested_weights: RoutingWeights,
        strategy: RoutingStrategy,
        reasons: List[str]
    ) -> bool:
        """
        æ‡‰ç”¨æ¬Šé‡èª¿æ•´åˆ°è·¯ç”±å™¨
        
        Args:
            router: AITaskRouterå¯¦ä¾‹
            suggested_weights: å»ºè­°çš„æ–°æ¬Šé‡
            strategy: è·¯ç”±ç­–ç•¥
            reasons: èª¿æ•´åŸå› 
            
        Returns:
            æ˜¯å¦æˆåŠŸæ‡‰ç”¨èª¿æ•´
        """
        try:
            # è¨˜éŒ„èª¿æ•´æ­·å²
            adjustment_record = {
                'timestamp': datetime.now(timezone.utc).isoformat(),
                'strategy': strategy.value,
                'old_weights': router.routing_strategies.get(strategy).__dict__.copy(),
                'new_weights': suggested_weights.__dict__.copy(),
                'reasons': reasons,
                'feedback_samples_used': len(self.feedback_history)
            }
            
            # æ‡‰ç”¨èª¿æ•´
            success = router.update_strategy_weights(strategy, suggested_weights)
            
            if success:
                self.weight_adjustment_history.append(adjustment_record)
                self.stats['weight_adjustments_made'] += 1
                self.stats['last_update'] = datetime.now(timezone.utc)
                
                self.logger.info(
                    f"âœ… Applied weight adjustments for strategy {strategy.value}:\n"
                    f"   Reasons: {'; '.join(reasons)}"
                )
            
            return success
            
        except Exception as e:
            self.logger.error(f"âŒ Failed to apply weight adjustments: {e}")
            return False
    
    # ==================== æŸ¥è©¢å’Œå ±å‘Šæ¥å£ ====================
    
    def get_model_performance_summary(
        self,
        provider: Optional[str] = None,
        model_id: Optional[str] = None,
        hours_back: int = 168  # ä¸€å‘¨
    ) -> Dict[str, Any]:
        """ç²å–æ¨¡å‹æ€§èƒ½æ‘˜è¦"""
        try:
            cutoff_time = datetime.now(timezone.utc) - timedelta(hours=hours_back)
            recent_feedback = [
                f for f in self.feedback_history
                if f.timestamp >= cutoff_time
            ]
            
            # éæ¿¾ç‰¹å®šæ¨¡å‹
            if provider or model_id:
                filtered_feedback = []
                for feedback in recent_feedback:
                    if provider and feedback.provider != provider:
                        continue
                    if model_id and feedback.model_id != model_id:
                        continue
                    filtered_feedback.append(feedback)
                recent_feedback = filtered_feedback
            
            if not recent_feedback:
                return {'status': 'no_data', 'message': 'æŒ‡å®šæ¢ä»¶ä¸‹ç„¡æ€§èƒ½æ•¸æ“š'}
            
            # æŒ‰æ¨¡å‹åˆ†çµ„
            model_groups = {}
            for feedback in recent_feedback:
                model_key = f"{feedback.provider}/{feedback.model_id}"
                if model_key not in model_groups:
                    model_groups[model_key] = {
                        'cost': [], 'latency': [], 'quality': [],
                        'failures': 0, 'total': 0
                    }
                
                group = model_groups[model_key]
                group['total'] += 1
                
                if feedback.feedback_type == FeedbackType.COST_ACTUAL:
                    group['cost'].append({
                        'accuracy': feedback.accuracy_score,
                        'variance': feedback.variance,
                        'predicted': feedback.predicted_value,
                        'actual': feedback.actual_value
                    })
                elif feedback.feedback_type == FeedbackType.LATENCY_ACTUAL:
                    group['latency'].append({
                        'accuracy': feedback.accuracy_score,
                        'variance': feedback.variance,
                        'predicted': feedback.predicted_value,
                        'actual': feedback.actual_value
                    })
                elif feedback.feedback_type == FeedbackType.QUALITY_ACTUAL:
                    group['quality'].append({
                        'accuracy': feedback.accuracy_score,
                        'variance': feedback.variance,
                        'predicted': feedback.predicted_value,
                        'actual': feedback.actual_value
                    })
                elif feedback.feedback_type == FeedbackType.SUCCESS_FAILURE:
                    if feedback.actual_value == 0.0:
                        group['failures'] += 1
            
            # è¨ˆç®—æ‘˜è¦çµ±è¨ˆ
            summary = {}
            for model_key, data in model_groups.items():
                model_summary = {
                    'total_samples': data['total'],
                    'success_rate': (data['total'] - data['failures']) / max(data['total'], 1)
                }
                
                # æˆæœ¬æ€§èƒ½
                if data['cost']:
                    model_summary['cost_performance'] = {
                        'avg_accuracy': statistics.mean([d['accuracy'] for d in data['cost']]),
                        'avg_variance': statistics.mean([d['variance'] for d in data['cost']]),
                        'sample_count': len(data['cost'])
                    }
                
                # å»¶é²æ€§èƒ½
                if data['latency']:
                    model_summary['latency_performance'] = {
                        'avg_accuracy': statistics.mean([d['accuracy'] for d in data['latency']]),
                        'avg_variance': statistics.mean([d['variance'] for d in data['latency']]),
                        'sample_count': len(data['latency'])
                    }
                
                # å“è³ªæ€§èƒ½
                if data['quality']:
                    model_summary['quality_performance'] = {
                        'avg_accuracy': statistics.mean([d['accuracy'] for d in data['quality']]),
                        'avg_variance': statistics.mean([d['variance'] for d in data['quality']]),
                        'sample_count': len(data['quality'])
                    }
                
                summary[model_key] = model_summary
            
            return {
                'status': 'success',
                'analysis_period_hours': hours_back,
                'total_feedback_samples': len(recent_feedback),
                'models_analyzed': len(summary),
                'summary': summary,
                'timestamp': datetime.now(timezone.utc).isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Model performance summary failed: {e}")
            return {'status': 'error', 'error': str(e)}
    
    def get_adjustment_history(self, limit: int = 20) -> List[Dict[str, Any]]:
        """ç²å–æ¬Šé‡èª¿æ•´æ­·å²"""
        try:
            recent_adjustments = self.weight_adjustment_history[-limit:] if limit else self.weight_adjustment_history
            return recent_adjustments
        except Exception as e:
            self.logger.error(f"âŒ Failed to get adjustment history: {e}")
            return []
    
    def get_feedback_statistics(self) -> Dict[str, Any]:
        """ç²å–åé¥‹çµ±è¨ˆä¿¡æ¯"""
        try:
            stats = self.stats.copy()
            
            # æ·»åŠ å¯¦æ™‚çµ±è¨ˆ
            total_feedback = len(self.feedback_history)
            if total_feedback > 0:
                # æŒ‰é¡å‹çµ±è¨ˆåé¥‹
                feedback_by_type = {}
                for feedback in self.feedback_history:
                    feedback_type = feedback.feedback_type.value
                    feedback_by_type[feedback_type] = feedback_by_type.get(feedback_type, 0) + 1
                
                stats.update({
                    'total_feedback_records': total_feedback,
                    'feedback_by_type': feedback_by_type,
                    'model_profiles_count': len(self.model_profiles),
                    'adjustment_history_count': len(self.weight_adjustment_history)
                })
                
                # æœ€è¿‘24å°æ™‚çš„åé¥‹çµ±è¨ˆ
                recent_cutoff = datetime.now(timezone.utc) - timedelta(hours=24)
                recent_feedback = [f for f in self.feedback_history if f.timestamp >= recent_cutoff]
                stats['recent_24h_feedback_count'] = len(recent_feedback)
            
            return stats
            
        except Exception as e:
            self.logger.error(f"âŒ Failed to get feedback statistics: {e}")
            return self.stats.copy()
    
    async def health_check(self) -> Dict[str, Any]:
        """æ€§èƒ½åé¥‹ç³»çµ±å¥åº·æª¢æŸ¥"""
        health_status = {
            'system_initialized': self._initialized,
            'timestamp': datetime.now(timezone.utc).isoformat(),
            'statistics': self.get_feedback_statistics(),
            'configuration': {
                'auto_adjustment_enabled': self.config.get('enable_auto_adjustment', True),
                'min_feedback_threshold': self.config.get('min_feedback_for_adjustment', 20),
                'max_history_size': self.max_feedback_history,
                'performance_window_hours': self.config.get('performance_window_hours', 72)
            }
        }
        
        # æ•¸æ“šå¥åº·æª¢æŸ¥
        recent_cutoff = datetime.now(timezone.utc) - timedelta(hours=24)
        recent_feedback_count = len([f for f in self.feedback_history if f.timestamp >= recent_cutoff])
        
        if recent_feedback_count == 0:
            health_status['data_status'] = 'no_recent_feedback'
        elif recent_feedback_count < 10:
            health_status['data_status'] = 'limited_feedback'
        else:
            health_status['data_status'] = 'sufficient_feedback'
        
        # æ•´é«”å¥åº·ç‹€æ…‹
        if self._initialized and recent_feedback_count > 0:
            health_status['overall_status'] = 'healthy'
        elif self._initialized:
            health_status['overall_status'] = 'healthy_no_data'
        else:
            health_status['overall_status'] = 'not_initialized'
        
        return health_status