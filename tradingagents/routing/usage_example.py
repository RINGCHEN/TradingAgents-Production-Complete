#!/usr/bin/env python3
"""
AI Task Router ä½¿ç”¨ç¤ºä¾‹
GPT-OSSæ•´åˆä»»å‹™1.3.1 - å®Œæ•´ç³»çµ±ä½¿ç”¨æŒ‡å—

å±•ç¤ºå¦‚ä½•ä½¿ç”¨ä¼æ¥­ç´šAIä»»å‹™æ™ºèƒ½è·¯ç”±ç³»çµ±çš„æ‰€æœ‰åŠŸèƒ½
"""

import asyncio
import logging
from typing import Dict, Any

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

from .llm_client_integration import EnhancedLLMClient, create_enhanced_llm_client
from .ai_task_router import RoutingStrategy, RoutingWeights
from ..utils.llm_client import AnalysisType

async def basic_usage_example():
    """åŸºç¤ä½¿ç”¨ç¤ºä¾‹"""
    logger.info("ğŸš€ åŸºç¤AIè·¯ç”±ä½¿ç”¨ç¤ºä¾‹")
    
    # 1. å‰µå»ºå¢å¼·ç‰ˆLLMå®¢æˆ¶ç«¯ï¼ˆå•Ÿç”¨AIè·¯ç”±ï¼‰
    client = create_enhanced_llm_client(
        llm_config={
            'openai_api_key': 'your_openai_key',
            'anthropic_api_key': 'your_anthropic_key',
            'gpt_oss_url': 'http://localhost:8080'
        },
        enable_ai_routing=True
    )
    
    try:
        # 2. åˆå§‹åŒ–AIè·¯ç”±åŠŸèƒ½
        await client.initialize_ai_routing()
        
        # 3. åŸ·è¡Œåˆ†æï¼ˆAIè·¯ç”±è‡ªå‹•é¸æ“‡æœ€å„ªæ¨¡å‹ï¼‰
        response = await client.analyze(
            prompt="è«‹åˆ†æå°ç©é›»(2330)çš„æŠ•è³‡å‰æ™¯",
            context={
                "stock_data": {"price": 500, "volume": 1000000},
                "market_condition": "bull_market"
            },
            analysis_type=AnalysisType.INVESTMENT,
            stock_id="2330",
            user_id="user123",
            user_tier="premium",  # AIè·¯ç”±åƒæ•¸
            priority="standard"   # AIè·¯ç”±åƒæ•¸
        )
        
        if response.success:
            logger.info(f"âœ… åˆ†ææˆåŠŸå®Œæˆ")
            logger.info(f"   é¸æ“‡çš„æ¨¡å‹: {response.provider.value}")
            logger.info(f"   éŸ¿æ‡‰æ™‚é–“: {response.response_time:.2f}ç§’")
            
            # æª¢æŸ¥AIè·¯ç”±æ±ºç­–ä¿¡æ¯
            if 'ai_routing_decision' in response.metadata:
                routing_info = response.metadata['ai_routing_decision']
                logger.info(f"   è·¯ç”±æ±ºç­–ä¿¡å¿ƒåº¦: {routing_info['confidence_score']:.3f}")
                logger.info(f"   é æœŸæˆæœ¬: ${routing_info['expected_cost']:.6f}")
                logger.info(f"   æ±ºç­–ç†ç”±: {routing_info['reasoning']}")
        else:
            logger.error(f"âŒ åˆ†æå¤±æ•—: {response.error}")
    
    finally:
        await client.close()

async def advanced_routing_strategies_example():
    """é«˜ç´šè·¯ç”±ç­–ç•¥ç¤ºä¾‹"""
    logger.info("ğŸ¯ é«˜ç´šè·¯ç”±ç­–ç•¥ç¤ºä¾‹")
    
    client = create_enhanced_llm_client(enable_ai_routing=True)
    await client.initialize_ai_routing()
    
    try:
        # ä¸åŒç­–ç•¥çš„ä½¿ç”¨ç¤ºä¾‹
        strategies_to_test = [
            (RoutingStrategy.COST_OPTIMIZED, "æˆæœ¬å„ªåŒ–ç­–ç•¥"),
            (RoutingStrategy.QUALITY_FIRST, "å“è³ªå„ªå…ˆç­–ç•¥"),
            (RoutingStrategy.LATENCY_FIRST, "å»¶é²å„ªå…ˆç­–ç•¥"),
            (RoutingStrategy.PRIVACY_FIRST, "éš±ç§å„ªå…ˆç­–ç•¥")
        ]
        
        for strategy, description in strategies_to_test:
            logger.info(f"\n   æ¸¬è©¦ {description}")
            
            response = await client.analyze(
                prompt="åˆ†æé€™å­£åº¦çš„è²¡å‹™è¡¨ç¾",
                analysis_type=AnalysisType.FUNDAMENTAL,
                routing_strategy=strategy,  # æŒ‡å®šè·¯ç”±ç­–ç•¥
                user_tier="enterprise",
                priority="standard"
            )
            
            if response.success and 'ai_routing_decision' in response.metadata:
                routing_info = response.metadata['ai_routing_decision']
                logger.info(f"     é¸æ“‡: {routing_info['selected_provider']}/{routing_info['selected_model']}")
                logger.info(f"     é æœŸæˆæœ¬: ${routing_info['expected_cost']:.6f}")
                logger.info(f"     é æœŸå»¶é²: {routing_info['expected_latency_ms']}ms")
    
    finally:
        await client.close()

async def custom_weights_example():
    """è‡ªå®šç¾©æ¬Šé‡é…ç½®ç¤ºä¾‹"""
    logger.info("âš–ï¸ è‡ªå®šç¾©æ¬Šé‡é…ç½®ç¤ºä¾‹")
    
    client = create_enhanced_llm_client(enable_ai_routing=True)
    await client.initialize_ai_routing()
    
    try:
        # å‰µå»ºè‡ªå®šç¾©æ¬Šé‡ï¼ˆæ¥µç«¯é‡è¦–æˆæœ¬å’Œéš±ç§ï¼‰
        custom_weights = RoutingWeights(
            cost=0.4,           # 40% æ¬Šé‡çµ¦æˆæœ¬
            latency=0.1,        # 10% æ¬Šé‡çµ¦å»¶é²
            quality=0.2,        # 20% æ¬Šé‡çµ¦å“è³ª
            availability=0.15,  # 15% æ¬Šé‡çµ¦å¯ç”¨æ€§
            privacy=0.35,       # 35% æ¬Šé‡çµ¦éš±ç§
            user_preference=0.0 # 0% æ¬Šé‡çµ¦ç”¨æˆ¶åå¥½
        )
        
        response = await client.analyze(
            prompt="è™•ç†æ©Ÿå¯†çš„è²¡å‹™æ•¸æ“šåˆ†æ",
            analysis_type=AnalysisType.FUNDAMENTAL,
            custom_weights=custom_weights,  # ä½¿ç”¨è‡ªå®šç¾©æ¬Šé‡
            user_tier="enterprise",
            priority="high"
        )
        
        if response.success and 'ai_routing_decision' in response.metadata:
            routing_info = response.metadata['ai_routing_decision']
            logger.info(f"   è‡ªå®šç¾©æ¬Šé‡è·¯ç”±é¸æ“‡: {routing_info['selected_provider']}/{routing_info['selected_model']}")
            logger.info(f"   æ±ºç­–ç†ç”±: {routing_info['reasoning']}")
    
    finally:
        await client.close()

async def performance_feedback_example():
    """æ€§èƒ½åé¥‹å’Œå„ªåŒ–ç¤ºä¾‹"""
    logger.info("ğŸ“ˆ æ€§èƒ½åé¥‹å’Œå„ªåŒ–ç¤ºä¾‹")
    
    client = create_enhanced_llm_client(
        enable_ai_routing=True,
        feedback_config={
            'enable_auto_adjustment': True,
            'min_feedback_for_adjustment': 5
        }
    )
    await client.initialize_ai_routing()
    
    try:
        # åŸ·è¡Œå¤šå€‹è«‹æ±‚ä»¥ç”Ÿæˆæ€§èƒ½æ•¸æ“š
        for i in range(10):
            response = await client.analyze(
                prompt=f"åˆ†ææŠ•è³‡çµ„åˆç¬¬{i+1}éƒ¨åˆ†",
                analysis_type=AnalysisType.INVESTMENT,
                user_tier="standard",
                enable_feedback_collection=True  # å•Ÿç”¨åé¥‹æ”¶é›†
            )
            
            logger.info(f"   è«‹æ±‚ {i+1} å®Œæˆ: {'æˆåŠŸ' if response.success else 'å¤±æ•—'}")
        
        # ç²å–æ€§èƒ½çµ±è¨ˆ
        stats = client.get_ai_routing_statistics()
        logger.info(f"\n   AIè·¯ç”±çµ±è¨ˆ:")
        logger.info(f"     ç¸½è«‹æ±‚æ•¸: {stats.get('total_ai_routed_requests', 0)}")
        logger.info(f"     å›é€€åˆ°å‚³çµ±è·¯ç”±: {stats.get('fallback_to_legacy_routing', 0)}")
        logger.info(f"     æ€§èƒ½åé¥‹è¨˜éŒ„: {stats.get('performance_feedback_records', 0)}")
        
        # å˜—è©¦å„ªåŒ–è·¯ç”±æ¬Šé‡
        optimization_result = await client.optimize_routing_weights(
            strategy=RoutingStrategy.BALANCED,
            analysis_hours=1
        )
        
        logger.info(f"\n   æ¬Šé‡å„ªåŒ–çµæœ: {optimization_result.get('status', 'unknown')}")
        if 'reasons' in optimization_result:
            for reason in optimization_result['reasons']:
                logger.info(f"     â€¢ {reason}")
    
    finally:
        await client.close()

async def monitoring_and_analytics_example():
    """ç›£æ§å’Œåˆ†æç¤ºä¾‹"""
    logger.info("ğŸ“Š ç›£æ§å’Œåˆ†æç¤ºä¾‹")
    
    client = create_enhanced_llm_client(enable_ai_routing=True)
    await client.initialize_ai_routing()
    
    try:
        # åŸ·è¡Œä¸€äº›è«‹æ±‚
        for i in range(5):
            await client.analyze(
                prompt=f"å¸‚å ´åˆ†æå ±å‘Š #{i+1}",
                analysis_type=AnalysisType.TECHNICAL,
                user_tier="premium"
            )
        
        # ç²å–æ±ºç­–æ­·å²
        history = client.get_routing_decision_history(limit=10)
        logger.info(f"\n   æœ€è¿‘æ±ºç­–æ­·å² ({len(history)} æ¢è¨˜éŒ„):")
        for decision in history[-3:]:  # é¡¯ç¤ºæœ€è¿‘3æ¢
            logger.info(f"     æ™‚é–“: {decision.get('timestamp', 'N/A')}")
            logger.info(f"     é¸æ“‡: {decision.get('selected_provider', 'N/A')}/{decision.get('selected_model', 'N/A')}")
            logger.info(f"     ä¿¡å¿ƒåº¦: {decision.get('confidence_score', 'N/A')}")
            logger.info(f"     ----")
        
        # ç²å–æ¨¡å‹æ€§èƒ½å ±å‘Š
        performance_report = client.get_model_performance_report(hours_back=24)
        logger.info(f"\n   æ¨¡å‹æ€§èƒ½å ±å‘Šç‹€æ…‹: {performance_report.get('status', 'unknown')}")
        if performance_report.get('status') == 'success':
            summary = performance_report.get('summary', {})
            for model_key, model_data in summary.items():
                logger.info(f"     {model_key}: æˆåŠŸç‡ {model_data.get('success_rate', 0):.2%}")
        
        # å¥åº·æª¢æŸ¥
        health = await client.health_check()
        ai_routing_health = health.get('ai_routing', {})
        logger.info(f"\n   AIè·¯ç”±å¥åº·ç‹€æ…‹:")
        logger.info(f"     å•Ÿç”¨: {ai_routing_health.get('enabled', False)}")
        logger.info(f"     åˆå§‹åŒ–: {ai_routing_health.get('initialized', False)}")
        logger.info(f"     è·¯ç”±å™¨ç‹€æ…‹: {ai_routing_health.get('router_status', 'unknown')}")
        logger.info(f"     åé¥‹ç‹€æ…‹: {ai_routing_health.get('feedback_status', 'unknown')}")
    
    finally:
        await client.close()

async def backward_compatibility_example():
    """å‘å¾Œå…¼å®¹æ€§ç¤ºä¾‹"""
    logger.info("ğŸ”„ å‘å¾Œå…¼å®¹æ€§ç¤ºä¾‹")
    
    # 1. ç¦ç”¨AIè·¯ç”±ï¼Œä½¿ç”¨å‚³çµ±æ¨¡å¼
    legacy_client = create_enhanced_llm_client(
        enable_ai_routing=False  # ç¦ç”¨AIè·¯ç”±
    )
    
    try:
        # é€™äº›èª¿ç”¨èˆ‡åŸå§‹LLMClientå®Œå…¨ç›¸åŒ
        response = await legacy_client.analyze(
            prompt="å‚³çµ±æ¨¡å¼åˆ†æ",
            analysis_type=AnalysisType.TECHNICAL
        )
        
        logger.info(f"   å‚³çµ±æ¨¡å¼åˆ†æ: {'æˆåŠŸ' if response.success else 'å¤±æ•—'}")
        logger.info(f"   ä½¿ç”¨çš„æ¨¡å‹: {response.model}")
        
        # æª¢æŸ¥AIè·¯ç”±ç‹€æ…‹
        stats = legacy_client.get_ai_routing_statistics()
        logger.info(f"   AIè·¯ç”±ç‹€æ…‹: {stats.get('ai_routing_enabled', 'unknown')}")
    
    finally:
        await legacy_client.close()
    
    # 2. å•Ÿç”¨AIè·¯ç”±ä½†ä¿æŒAPIå…¼å®¹
    enhanced_client = create_enhanced_llm_client(
        enable_ai_routing=True  # å•Ÿç”¨AIè·¯ç”±
    )
    
    try:
        await enhanced_client.initialize_ai_routing()
        
        # ç›¸åŒçš„APIèª¿ç”¨ï¼Œä½†å…§éƒ¨ä½¿ç”¨AIè·¯ç”±
        response = await enhanced_client.analyze(
            prompt="å¢å¼·æ¨¡å¼åˆ†æ",
            analysis_type=AnalysisType.TECHNICAL
        )
        
        logger.info(f"   å¢å¼·æ¨¡å¼åˆ†æ: {'æˆåŠŸ' if response.success else 'å¤±æ•—'}")
        logger.info(f"   ä½¿ç”¨çš„æ¨¡å‹: {response.model}")
        
        # AIè·¯ç”±çš„é¡å¤–ä¿¡æ¯
        if 'ai_routing_decision' in response.metadata:
            routing_info = response.metadata['ai_routing_decision']
            logger.info(f"   è·¯ç”±æ±ºç­–ä¿¡å¿ƒåº¦: {routing_info['confidence_score']:.3f}")
    
    finally:
        await enhanced_client.close()

async def error_handling_and_fallback_example():
    """éŒ¯èª¤è™•ç†å’Œå›é€€æ©Ÿåˆ¶ç¤ºä¾‹"""
    logger.info("ğŸ›¡ï¸ éŒ¯èª¤è™•ç†å’Œå›é€€æ©Ÿåˆ¶ç¤ºä¾‹")
    
    # ä½¿ç”¨å¯èƒ½å°è‡´éŒ¯èª¤çš„é…ç½®
    client = create_enhanced_llm_client(
        llm_config={
            'openai_api_key': 'invalid_key',  # ç„¡æ•ˆçš„APIå¯†é‘°
            'gpt_oss_url': 'http://invalid:8080'  # ç„¡æ•ˆçš„GPT-OSS URL
        },
        enable_ai_routing=True,
        router_config={
            'enable_audit_logging': True,
            'fallback_on_error': True
        }
    )
    
    try:
        # å˜—è©¦åˆå§‹åŒ–ï¼ˆå¯èƒ½å¤±æ•—ï¼‰
        init_success = await client.initialize_ai_routing()
        logger.info(f"   AIè·¯ç”±åˆå§‹åŒ–: {'æˆåŠŸ' if init_success else 'å¤±æ•—'}")
        
        # åŸ·è¡Œè«‹æ±‚ï¼ˆæ‡‰è©²å›é€€åˆ°å¯ç”¨çš„æ–¹æ³•ï¼‰
        response = await client.analyze(
            prompt="æ¸¬è©¦éŒ¯èª¤è™•ç†",
            analysis_type=AnalysisType.TECHNICAL,
            user_tier="standard"
        )
        
        logger.info(f"   è«‹æ±‚çµæœ: {'æˆåŠŸ' if response.success else 'å¤±æ•—'}")
        
        if response.success:
            logger.info(f"   ä½¿ç”¨çš„æ¨¡å‹: {response.model}")
            logger.info(f"   å›é€€çµ±è¨ˆ: {client.get_ai_routing_statistics().get('fallback_to_legacy_routing', 0)}")
        else:
            logger.info(f"   éŒ¯èª¤ä¿¡æ¯: {response.error}")
    
    finally:
        await client.close()

async def main():
    """ä¸»å‡½æ•¸ - é‹è¡Œæ‰€æœ‰ç¤ºä¾‹"""
    logger.info("ğŸ‰ AI Task Router å®Œæ•´ä½¿ç”¨ç¤ºä¾‹é–‹å§‹")
    
    examples = [
        ("åŸºç¤ä½¿ç”¨ç¤ºä¾‹", basic_usage_example),
        ("é«˜ç´šè·¯ç”±ç­–ç•¥ç¤ºä¾‹", advanced_routing_strategies_example),
        ("è‡ªå®šç¾©æ¬Šé‡é…ç½®ç¤ºä¾‹", custom_weights_example),
        ("æ€§èƒ½åé¥‹å’Œå„ªåŒ–ç¤ºä¾‹", performance_feedback_example),
        ("ç›£æ§å’Œåˆ†æç¤ºä¾‹", monitoring_and_analytics_example),
        ("å‘å¾Œå…¼å®¹æ€§ç¤ºä¾‹", backward_compatibility_example),
        ("éŒ¯èª¤è™•ç†å’Œå›é€€æ©Ÿåˆ¶ç¤ºä¾‹", error_handling_and_fallback_example)
    ]
    
    for name, example_func in examples:
        try:
            logger.info(f"\n{'='*60}")
            logger.info(f"é‹è¡Œ {name}")
            logger.info(f"{'='*60}")
            
            await example_func()
            
            logger.info(f"âœ… {name} å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ {name} å¤±æ•—: {e}")
        
        # çŸ­æš«æš«åœ
        await asyncio.sleep(1)
    
    logger.info(f"\nğŸŠ æ‰€æœ‰ç¤ºä¾‹å®Œæˆ!")

if __name__ == "__main__":
    # é‹è¡Œç¤ºä¾‹
    asyncio.run(main())