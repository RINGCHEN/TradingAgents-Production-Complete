#!/usr/bin/env python3
"""
Enhanced LLM Service - ä¼æ¥­ç´šæ™ºèƒ½è·¯ç”±LLMæœå‹™
GPT-OSSæ•´åˆä»»å‹™1.3.1 - å®Œæ•´çš„LLMClientæ•´åˆå¯¦ç¾

æä¾›å®Œå…¨é€æ˜çš„æ™ºèƒ½è·¯ç”±å¢å¼·æœå‹™ï¼š
- é›¶é…ç½®æ™ºèƒ½å‡ç´š
- å®Œæ•´çš„å‘å¾Œå…¼å®¹æ€§
- ä¼æ¥­ç´šè·¯ç”±æ±ºç­–å¼•æ“
- å¯¦æ™‚æ€§èƒ½å„ªåŒ–å’Œå­¸ç¿’
"""

import asyncio
import uuid
import logging
from typing import Dict, Any, List, Optional, Union, Tuple
from datetime import datetime, timezone, timedelta
from contextlib import asynccontextmanager

from .ai_task_router import AITaskRouter, RoutingStrategy, RoutingWeights
from .routing_config import RoutingConfigManager
from ..database.task_metadata_db import TaskMetadataDB
from ..database.model_capability_db import ModelCapabilityDB
from ..database.task_metadata_models import RoutingDecisionRequest
from ..monitoring.performance_monitor import PerformanceMonitor
from ..services.model_capability_service import ModelCapabilityService
from ..utils.llm_client import (
    LLMClient, LLMRequest, LLMResponse, AnalysisType, LLMProvider,
    LLMError, LLMAPIError, LLMRateLimitError
)

logger = logging.getLogger(__name__)

class EnhancedLLMService:
    """
    ä¼æ¥­ç´šå¢å¼·LLMæœå‹™
    
    æ ¸å¿ƒç‰¹æ€§ï¼š
    1. é€æ˜æ™ºèƒ½è·¯ç”± - ç„¡éœ€ä¿®æ”¹ç¾æœ‰ä»£ç¢¼
    2. å‹•æ…‹æ€§èƒ½å„ªåŒ– - åŸºæ–¼å¯¦æ™‚åé¥‹è‡ªå‹•èª¿æ•´
    3. ä¼æ¥­ç´šç›£æ§ - å®Œæ•´çš„æ±ºç­–å¯©è¨ˆå’Œæ€§èƒ½è¿½è¹¤
    4. æˆæœ¬æ™ºèƒ½æ§åˆ¶ - è‡ªå‹•æˆæœ¬å„ªåŒ–å’Œé ç®—ç®¡ç†
    5. é«˜å¯ç”¨æ€§è¨­è¨ˆ - æ™ºèƒ½æ•…éšœè½‰ç§»å’Œé™ç´šç­–ç•¥
    """
    
    def __init__(
        self,
        base_llm_client: Optional[LLMClient] = None,
        enable_intelligent_routing: bool = True,
        config: Optional[Dict[str, Any]] = None
    ):
        """
        åˆå§‹åŒ–å¢å¼·LLMæœå‹™
        
        Args:
            base_llm_client: åº•å±¤LLMå®¢æˆ¶ç«¯
            enable_intelligent_routing: æ˜¯å¦å•Ÿç”¨æ™ºèƒ½è·¯ç”±
            config: æœå‹™é…ç½®
        """
        # åº•å±¤LLMå®¢æˆ¶ç«¯
        self.base_client = base_llm_client or LLMClient()
        
        # é…ç½®ç®¡ç†
        self.config = config or {}
        self._load_default_config()
        
        # æ™ºèƒ½è·¯ç”±åŠŸèƒ½é–‹é—œ
        self.enable_intelligent_routing = enable_intelligent_routing and self.config.get('intelligent_routing_enabled', True)
        
        # åˆå§‹åŒ–æ ¸å¿ƒçµ„ä»¶
        self._initialize_components()
        
        # æœå‹™ç‹€æ…‹
        self._initialized = False
        self._service_health = {'status': 'initializing'}
        
        # çµ±è¨ˆè¿½è¹¤
        self.service_stats = {
            'total_requests': 0,
            'intelligent_routed_requests': 0,
            'fallback_requests': 0,
            'cost_savings_total': 0.0,
            'performance_improvements': 0,
            'uptime_start': datetime.now(timezone.utc),
            'last_optimization': None,
            'routing_accuracy': 0.0,
            'average_decision_confidence': 0.0
        }
        
        self.logger = logger
    
    def _load_default_config(self):
        """è¼‰å…¥é è¨­é…ç½®"""
        defaults = {
            # æ ¸å¿ƒåŠŸèƒ½é–‹é—œ
            'intelligent_routing_enabled': True,
            'performance_monitoring_enabled': True,
            'cost_optimization_enabled': True,
            'auto_tuning_enabled': True,
            'audit_logging_enabled': True,
            
            # è·¯ç”±ç­–ç•¥é…ç½®
            'default_routing_strategy': 'balanced',
            'fallback_strategy': 'cost_optimized',
            'routing_confidence_threshold': 0.7,
            'enable_dynamic_strategy_adjustment': True,
            
            # æ€§èƒ½é…ç½®
            'performance_cache_ttl': 1800,  # 30åˆ†é˜
            'feedback_collection_rate': 1.0,  # 100%æ”¶é›†åé¥‹
            'optimization_interval_hours': 6,
            'min_data_points_for_optimization': 50,
            
            # æˆæœ¬æ§åˆ¶
            'cost_tracking_enabled': True,
            'daily_cost_limit': None,
            'cost_alert_threshold': 0.8,
            'prefer_local_models': True,
            
            # å®¹éŒ¯å’Œæ¢å¾©
            'max_routing_failures_before_fallback': 3,
            'fallback_cooldown_minutes': 10,
            'health_check_interval_seconds': 60,
            'enable_graceful_degradation': True,
            
            # ç›£æ§å’Œæ—¥èªŒ
            'detailed_logging': False,
            'performance_metrics_retention_hours': 168,  # 7å¤©
            'audit_log_retention_days': 30,
            'enable_real_time_alerts': False
        }
        
        for key, value in defaults.items():
            if key not in self.config:
                self.config[key] = value
    
    def _initialize_components(self):
        """åˆå§‹åŒ–æ ¸å¿ƒçµ„ä»¶"""
        try:
            if self.enable_intelligent_routing:
                # åˆå§‹åŒ–æ•¸æ“šåº«çµ„ä»¶
                self.task_db = TaskMetadataDB()
                self.model_db = ModelCapabilityDB()
                
                # åˆå§‹åŒ–æ€§èƒ½ç›£æ§
                self.performance_monitor = PerformanceMonitor(
                    model_db=self.model_db,
                    task_db=self.task_db
                )
                
                # åˆå§‹åŒ–AIè·¯ç”±å™¨
                self.ai_router = AITaskRouter(
                    task_db=self.task_db,
                    model_db=self.model_db,
                    performance_monitor=self.performance_monitor,
                    config=self.config
                )
                
                # åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨
                self.config_manager = RoutingConfigManager()
                
                # åˆå§‹åŒ–æ¨¡å‹èƒ½åŠ›æœå‹™
                self.model_service = ModelCapabilityService(
                    llm_client=self.base_client,
                    model_db=self.model_db,
                    task_db=self.task_db,
                    performance_monitor=self.performance_monitor
                )
                
                self.logger.info("âœ… Intelligent routing components initialized")
            else:
                # ç¦ç”¨æ™ºèƒ½è·¯ç”±æ™‚çš„å ä½ç¬¦
                self.ai_router = None
                self.config_manager = None
                self.model_service = None
                self.performance_monitor = None
                self.logger.info("â„¹ï¸ Intelligent routing disabled - using basic LLM client")
        
        except Exception as e:
            self.logger.error(f"âŒ Component initialization failed: {e}")
            self.enable_intelligent_routing = False  # é™ç´šåˆ°åŸºæœ¬æ¨¡å¼
            self._set_fallback_components()
    
    def _set_fallback_components(self):
        """è¨­ç½®å¾Œå‚™çµ„ä»¶ï¼ˆç„¡æ™ºèƒ½è·¯ç”±ï¼‰"""
        self.ai_router = None
        self.config_manager = None
        self.model_service = None
        self.performance_monitor = None
        self.logger.warning("âš ï¸ Fallback to basic LLM service mode")
    
    async def initialize(self) -> bool:
        """
        åˆå§‹åŒ–æœå‹™
        
        Returns:
            æ˜¯å¦åˆå§‹åŒ–æˆåŠŸ
        """
        if self._initialized:
            return True
        
        try:
            self.logger.info("ğŸš€ Initializing Enhanced LLM Service...")
            
            # åˆå§‹åŒ–æ™ºèƒ½è·¯ç”±çµ„ä»¶
            if self.enable_intelligent_routing:
                # ä¸¦è¡Œåˆå§‹åŒ–çµ„ä»¶ä»¥æé«˜æ•ˆç‡
                init_tasks = [
                    self.model_service.initialize(),
                    self.ai_router.initialize(),
                    self._start_performance_monitoring(),
                    self._initialize_optimization_scheduler()
                ]
                
                init_results = await asyncio.gather(*init_tasks, return_exceptions=True)
                
                # æª¢æŸ¥åˆå§‹åŒ–çµæœ
                for i, result in enumerate(init_results):
                    if isinstance(result, Exception):
                        self.logger.error(f"âŒ Component {i} initialization failed: {result}")
                        # æ ¹æ“šå¤±æ•—çš„çµ„ä»¶æ±ºå®šæ˜¯å¦é™ç´š
                        if i <= 1:  # æ ¸å¿ƒçµ„ä»¶å¤±æ•—ï¼Œé™ç´š
                            self.enable_intelligent_routing = False
                            self._set_fallback_components()
                            break
            
            # è¨­ç½®å¥åº·ç‹€æ…‹
            self._service_health = {
                'status': 'healthy',
                'intelligent_routing': self.enable_intelligent_routing,
                'components_active': self._get_active_components_count(),
                'last_check': datetime.now(timezone.utc).isoformat()
            }
            
            self._initialized = True
            
            mode = "with intelligent routing" if self.enable_intelligent_routing else "in basic mode"
            self.logger.info(f"âœ… Enhanced LLM Service initialized {mode}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Service initialization failed: {e}")
            # ç·Šæ€¥é™ç´šåˆ°åŸºæœ¬æ¨¡å¼
            self.enable_intelligent_routing = False
            self._set_fallback_components()
            self._service_health = {'status': 'degraded', 'error': str(e)}
            return False
    
    async def _start_performance_monitoring(self):
        """å•Ÿå‹•æ€§èƒ½ç›£æ§"""
        if self.config.get('performance_monitoring_enabled') and self.performance_monitor:
            await self.performance_monitor.start()
    
    async def _initialize_optimization_scheduler(self):
        """åˆå§‹åŒ–å„ªåŒ–èª¿åº¦å™¨"""
        if self.config.get('auto_tuning_enabled'):
            # å•Ÿå‹•èƒŒæ™¯å„ªåŒ–ä»»å‹™
            asyncio.create_task(self._optimization_scheduler())
    
    async def _optimization_scheduler(self):
        """å¾Œå°å„ªåŒ–èª¿åº¦å™¨"""
        optimization_interval = self.config.get('optimization_interval_hours', 6) * 3600
        
        while self._initialized:
            try:
                await asyncio.sleep(optimization_interval)
                
                if self.enable_intelligent_routing:
                    await self._perform_automatic_optimization()
                    
            except asyncio.CancelledError:
                break
            except Exception as e:
                self.logger.error(f"âŒ Optimization scheduler error: {e}")
                await asyncio.sleep(60)  # çŸ­æš«å»¶é²å¾Œç¹¼çºŒ
    
    # ==================== ä¸»è¦æœå‹™æ¥å£ ====================
    
    async def analyze(
        self,
        prompt: str,
        context: Optional[Dict[str, Any]] = None,
        analysis_type: AnalysisType = AnalysisType.TECHNICAL,
        task_type: Optional[str] = None,
        user_tier: str = "standard",
        priority: str = "standard",
        **kwargs
    ) -> LLMResponse:
        """
        åŸ·è¡Œæ™ºèƒ½åˆ†æï¼ˆä¸»è¦æœå‹™æ¥å£ï¼‰
        
        Args:
            prompt: åˆ†ææç¤ºè©
            context: åˆ†æä¸Šä¸‹æ–‡
            analysis_type: åˆ†æé¡å‹
            task_type: ä»»å‹™é¡å‹ï¼ˆå¯é¸ï¼Œè‡ªå‹•æ¨æ–·ï¼‰
            user_tier: ç”¨æˆ¶ç­‰ç´š
            priority: è«‹æ±‚å„ªå…ˆç´š
            **kwargs: å…¶ä»–åƒæ•¸
            
        Returns:
            LLMéŸ¿æ‡‰
        """
        if not self._initialized:
            await self.initialize()
        
        request_start = datetime.now(timezone.utc)
        request_id = str(uuid.uuid4())
        
        try:
            self.service_stats['total_requests'] += 1
            
            # ä½¿ç”¨æ™ºèƒ½è·¯ç”±æˆ–åŸºæœ¬è·¯ç”±
            if self.enable_intelligent_routing and self._should_use_intelligent_routing():
                response = await self._analyze_with_intelligent_routing(
                    prompt=prompt,
                    context=context,
                    analysis_type=analysis_type,
                    task_type=task_type,
                    user_tier=user_tier,
                    priority=priority,
                    request_id=request_id,
                    **kwargs
                )
                self.service_stats['intelligent_routed_requests'] += 1
            else:
                response = await self._analyze_with_basic_routing(
                    prompt=prompt,
                    context=context,
                    analysis_type=analysis_type,
                    **kwargs
                )
                self.service_stats['fallback_requests'] += 1
            
            # å¾Œè™•ç†å’Œçµ±è¨ˆæ›´æ–°
            await self._post_process_response(response, request_start, request_id)
            
            return response
            
        except Exception as e:
            self.logger.error(f"âŒ Analysis failed for request {request_id}: {e}")
            self._update_failure_stats()
            raise
    
    async def _analyze_with_intelligent_routing(
        self,
        prompt: str,
        context: Optional[Dict[str, Any]],
        analysis_type: AnalysisType,
        task_type: Optional[str],
        user_tier: str,
        priority: str,
        request_id: str,
        **kwargs
    ) -> LLMResponse:
        """ä½¿ç”¨æ™ºèƒ½è·¯ç”±åŸ·è¡Œåˆ†æ"""
        try:
            # 1. ç¢ºå®šä»»å‹™é¡å‹
            resolved_task_type = task_type or self._resolve_task_type(analysis_type)
            
            # 2. ä½¿ç”¨æ¨¡å‹èƒ½åŠ›æœå‹™é€²è¡Œå„ªåŒ–åˆ†æ
            analysis_result = await self.model_service.analyze_with_optimal_model(
                prompt=prompt,
                task_type=resolved_task_type,
                context=context,
                user_id=kwargs.get('user_id'),
                user_tier=user_tier,
                priority=priority,
                analyst_id=kwargs.get('analyst_id', 'general'),
                stock_id=kwargs.get('stock_id'),
                max_tokens=kwargs.get('max_tokens'),
                temperature=kwargs.get('temperature'),
                requires_high_quality=kwargs.get('requires_high_quality', False),
                max_acceptable_latency=kwargs.get('max_acceptable_latency'),
                max_acceptable_cost=kwargs.get('max_acceptable_cost'),
                user_preferences=kwargs.get('user_preferences')
            )
            
            # 3. è½‰æ›ç‚ºLLMResponseæ ¼å¼
            response = self._convert_analysis_result_to_llm_response(analysis_result, request_id)
            
            # 4. è¨˜éŒ„è·¯ç”±æˆåŠŸ
            if analysis_result.get('routing_decision'):
                self._update_routing_success_stats(analysis_result['routing_decision'])
            
            return response
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ Intelligent routing failed: {e}. Falling back to basic routing")
            return await self._analyze_with_basic_routing(
                prompt=prompt,
                context=context,
                analysis_type=analysis_type,
                **kwargs
            )
    
    async def _analyze_with_basic_routing(
        self,
        prompt: str,
        context: Optional[Dict[str, Any]],
        analysis_type: AnalysisType,
        **kwargs
    ) -> LLMResponse:
        """ä½¿ç”¨åŸºæœ¬è·¯ç”±åŸ·è¡Œåˆ†æ"""
        return await self.base_client.analyze(
            prompt=prompt,
            context=context,
            analysis_type=analysis_type,
            analyst_id=kwargs.get('analyst_id', 'general'),
            stock_id=kwargs.get('stock_id'),
            user_id=kwargs.get('user_id'),
            max_tokens=kwargs.get('max_tokens'),
            temperature=kwargs.get('temperature')
        )
    
    def _should_use_intelligent_routing(self) -> bool:
        """åˆ¤æ–·æ˜¯å¦æ‡‰è©²ä½¿ç”¨æ™ºèƒ½è·¯ç”±"""
        if not self.enable_intelligent_routing:
            return False
        
        # æª¢æŸ¥è·¯ç”±å™¨å¥åº·ç‹€æ…‹
        if not self.ai_router or not hasattr(self.ai_router, '_initialized') or not self.ai_router._initialized:
            return False
        
        # æª¢æŸ¥æœ€è¿‘çš„å¤±æ•—ç‡
        recent_failure_rate = self._calculate_recent_failure_rate()
        if recent_failure_rate > 0.5:  # 50%ä»¥ä¸Šå¤±æ•—ç‡æ™‚é™ç´š
            return False
        
        return True
    
    def _resolve_task_type(self, analysis_type: AnalysisType) -> str:
        """è§£æåˆ†æé¡å‹åˆ°ä»»å‹™é¡å‹"""
        mapping = {
            AnalysisType.TECHNICAL: "technical_analysis",
            AnalysisType.FUNDAMENTAL: "financial_summary", 
            AnalysisType.NEWS: "news_classification",
            AnalysisType.SENTIMENT: "market_sentiment",
            AnalysisType.RISK: "risk_assessment",
            AnalysisType.INVESTMENT: "investment_reasoning",
            AnalysisType.REASONING: "investment_reasoning",
            AnalysisType.GENERATION: "report_generation",
            AnalysisType.ANALYSIS: "financial_summary"
        }
        return mapping.get(analysis_type, "financial_summary")
    
    def _convert_analysis_result_to_llm_response(
        self,
        analysis_result: Dict[str, Any],
        request_id: str
    ) -> LLMResponse:
        """è½‰æ›åˆ†æçµæœç‚ºLLMéŸ¿æ‡‰æ ¼å¼"""
        
        routing_info = analysis_result.get('routing_decision', {})
        performance_info = analysis_result.get('performance', {})
        
        # ç¢ºå®šæä¾›å•†å’Œæ¨¡å‹
        provider_name = routing_info.get('selected_provider', 'unknown')
        model_name = routing_info.get('selected_model', 'unknown')
        
        # è½‰æ›æä¾›å•†åç¨±ç‚ºLLMProvideræšèˆ‰
        try:
            if provider_name == 'openai':
                provider = LLMProvider.OPENAI
            elif provider_name == 'anthropic':
                provider = LLMProvider.ANTHROPIC
            elif provider_name == 'gpt_oss':
                provider = LLMProvider.GPT_OSS
            else:
                provider = LLMProvider.OPENAI  # é»˜èªå€¼
        except:
            provider = LLMProvider.OPENAI
        
        # æ§‹å»ºä½¿ç”¨ä¿¡æ¯
        usage_info = {}
        tokens_used = performance_info.get('tokens_used')
        if tokens_used:
            usage_info = {
                'total_tokens': tokens_used,
                'prompt_tokens': int(tokens_used * 0.7),  # ä¼°ç®—
                'completion_tokens': int(tokens_used * 0.3)  # ä¼°ç®—
            }
        
        # æ§‹å»ºå¢å¼·çš„å…ƒæ•¸æ“š
        enhanced_metadata = {
            'service_type': 'enhanced_llm_service',
            'routing_used': 'intelligent' if routing_info else 'basic',
            'request_id': request_id,
            'routing_decision': routing_info,
            'performance_metrics': performance_info,
            'service_version': '1.3.1'
        }
        
        if analysis_result.get('metadata'):
            enhanced_metadata.update(analysis_result['metadata'])
        
        return LLMResponse(
            content=analysis_result.get('content', ''),
            provider=provider,
            model=model_name,
            usage=usage_info,
            metadata=enhanced_metadata,
            request_id=request_id,
            response_time=performance_info.get('actual_latency_ms', 0) / 1000.0,
            success=analysis_result.get('success', False),
            error=analysis_result.get('error')
        )
    
    async def _post_process_response(
        self,
        response: LLMResponse,
        request_start: datetime,
        request_id: str
    ):
        """å¾Œè™•ç†éŸ¿æ‡‰ä¸¦æ›´æ–°çµ±è¨ˆ"""
        total_time = (datetime.now(timezone.utc) - request_start).total_seconds()
        
        # æ›´æ–°æœå‹™çµ±è¨ˆ
        if response.success and hasattr(response, 'metadata'):
            routing_info = response.metadata.get('routing_decision', {})
            
            # è¨ˆç®—æˆæœ¬ç¯€çœ
            if 'expected_cost' in routing_info and response.usage:
                expected_cost = routing_info.get('expected_cost', 0)
                # é€™è£¡å¯ä»¥è¨ˆç®—å¯¦éš›æˆæœ¬ç¯€çœ
                # self.service_stats['cost_savings_total'] += cost_savings
            
            # æ›´æ–°è·¯ç”±æº–ç¢ºæ€§
            confidence = routing_info.get('confidence_score', 0)
            if confidence > 0:
                current_avg = self.service_stats.get('average_decision_confidence', 0)
                total_decisions = self.service_stats.get('intelligent_routed_requests', 1)
                self.service_stats['average_decision_confidence'] = (
                    (current_avg * (total_decisions - 1) + confidence) / total_decisions
                )
    
    # ==================== æ€§èƒ½å„ªåŒ–å’Œè‡ªå‹•èª¿æ•´ ====================
    
    async def _perform_automatic_optimization(self):
        """åŸ·è¡Œè‡ªå‹•æ€§èƒ½å„ªåŒ–"""
        try:
            self.logger.info("ğŸ”§ Starting automatic performance optimization...")
            
            # 1. æ”¶é›†æ€§èƒ½æ•¸æ“š
            performance_data = await self._collect_optimization_data()
            
            # 2. åˆ†æä¸¦ç”Ÿæˆå„ªåŒ–å»ºè­°
            optimization_suggestions = await self._analyze_performance_data(performance_data)
            
            # 3. æ‡‰ç”¨å®‰å…¨çš„å„ªåŒ–
            applied_optimizations = await self._apply_safe_optimizations(optimization_suggestions)
            
            # 4. è¨˜éŒ„å„ªåŒ–çµæœ
            self.service_stats['last_optimization'] = datetime.now(timezone.utc).isoformat()
            self.service_stats['performance_improvements'] += len(applied_optimizations)
            
            self.logger.info(f"âœ… Automatic optimization completed. Applied {len(applied_optimizations)} optimizations")
            
        except Exception as e:
            self.logger.error(f"âŒ Automatic optimization failed: {e}")
    
    async def _collect_optimization_data(self) -> Dict[str, Any]:
        """æ”¶é›†å„ªåŒ–æ‰€éœ€çš„æ€§èƒ½æ•¸æ“š"""
        data = {}
        
        try:
            if self.performance_monitor:
                # ç²å–æ€§èƒ½å ±å‘Š
                data['performance_report'] = await self.performance_monitor.get_performance_report(
                    hours_back=self.config.get('optimization_interval_hours', 6)
                )
            
            if self.ai_router:
                # ç²å–è·¯ç”±çµ±è¨ˆ
                data['routing_stats'] = self.ai_router.get_routing_statistics()
                
                # ç²å–æ±ºç­–æ­·å²
                data['recent_decisions'] = self.ai_router.get_decision_history(limit=100)
        
        except Exception as e:
            self.logger.error(f"âŒ Error collecting optimization data: {e}")
        
        return data
    
    async def _analyze_performance_data(self, data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """åˆ†ææ€§èƒ½æ•¸æ“šä¸¦ç”Ÿæˆå„ªåŒ–å»ºè­°"""
        suggestions = []
        
        try:
            # åˆ†æè·¯ç”±ç­–ç•¥æ€§èƒ½
            routing_stats = data.get('routing_stats', {})
            if routing_stats:
                strategy_usage = routing_stats.get('strategy_usage', {})
                
                # æª¢æŸ¥æ˜¯å¦æœ‰ç­–ç•¥ä½¿ç”¨ä¸å¹³è¡¡
                if len(strategy_usage) > 1:
                    usage_values = list(strategy_usage.values())
                    if max(usage_values) > sum(usage_values) * 0.8:  # 80%ä»¥ä¸Šä½¿ç”¨å–®ä¸€ç­–ç•¥
                        suggestions.append({
                            'type': 'strategy_rebalancing',
                            'description': 'Detected strategy imbalance, suggest weight adjustment',
                            'priority': 'medium',
                            'data': strategy_usage
                        })
            
            # åˆ†ææ¨¡å‹æ€§èƒ½
            performance_report = data.get('performance_report', {})
            if performance_report:
                provider_perf = performance_report.get('provider_performance', {})
                
                for provider, metrics in provider_perf.items():
                    success_rate = metrics.get('success_rate', 1.0)
                    if success_rate < 0.95:  # æˆåŠŸç‡ä½æ–¼95%
                        suggestions.append({
                            'type': 'model_health_check',
                            'description': f'Low success rate detected for {provider}',
                            'priority': 'high',
                            'data': {'provider': provider, 'success_rate': success_rate}
                        })
            
            # åˆ†ææˆæœ¬æ•ˆç‡
            recent_decisions = data.get('recent_decisions', [])
            if len(recent_decisions) >= 50:
                cost_analysis = self._analyze_cost_efficiency(recent_decisions)
                if cost_analysis.get('potential_savings', 0) > 0.1:  # 10%ä»¥ä¸Šæ½›åœ¨ç¯€çœ
                    suggestions.append({
                        'type': 'cost_optimization',
                        'description': 'Potential cost savings detected',
                        'priority': 'medium',
                        'data': cost_analysis
                    })
        
        except Exception as e:
            self.logger.error(f"âŒ Error analyzing performance data: {e}")
        
        return suggestions
    
    def _analyze_cost_efficiency(self, decisions: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†ææˆæœ¬æ•ˆç‡"""
        total_decisions = len(decisions)
        if total_decisions == 0:
            return {}
        
        # ç°¡åŒ–çš„æˆæœ¬æ•ˆç‡åˆ†æ
        high_cost_decisions = sum(
            1 for d in decisions 
            if d.get('model_scores', [{}])[0].get('cost_analysis', {}).get('total_cost', 0) > 0.01
        )
        
        potential_savings_ratio = high_cost_decisions / total_decisions
        
        return {
            'total_decisions': total_decisions,
            'high_cost_decisions': high_cost_decisions,
            'potential_savings': potential_savings_ratio
        }
    
    async def _apply_safe_optimizations(self, suggestions: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ‡‰ç”¨å®‰å…¨çš„å„ªåŒ–å»ºè­°"""
        applied = []
        
        for suggestion in suggestions:
            try:
                suggestion_type = suggestion['type']
                priority = suggestion['priority']
                
                # åªæ‡‰ç”¨å®‰å…¨çš„å„ªåŒ–
                if suggestion_type == 'strategy_rebalancing' and priority in ['low', 'medium']:
                    # å¾®èª¿ç­–ç•¥æ¬Šé‡
                    success = await self._apply_strategy_rebalancing(suggestion['data'])
                    if success:
                        applied.append(suggestion)
                
                elif suggestion_type == 'cost_optimization' and priority == 'medium':
                    # æ‡‰ç”¨æˆæœ¬å„ªåŒ–
                    success = await self._apply_cost_optimization(suggestion['data'])
                    if success:
                        applied.append(suggestion)
                
                # é«˜å„ªå…ˆç´šçš„å»ºè­°è¨˜éŒ„ä½†ä¸è‡ªå‹•æ‡‰ç”¨
                elif priority == 'high':
                    self.logger.warning(f"âš ï¸ High priority optimization needed: {suggestion['description']}")
                    
            except Exception as e:
                self.logger.error(f"âŒ Failed to apply optimization {suggestion_type}: {e}")
        
        return applied
    
    async def _apply_strategy_rebalancing(self, usage_data: Dict[str, Any]) -> bool:
        """æ‡‰ç”¨ç­–ç•¥é‡å¹³è¡¡"""
        try:
            # å¯¦ç¾ç­–ç•¥æ¬Šé‡çš„å¾®èª¿
            # é€™æ˜¯ä¸€å€‹ä¿å®ˆçš„èª¿æ•´ï¼Œåªé€²è¡Œå°å¹…åº¦æ”¹å‹•
            return True  # ç°¡åŒ–å¯¦ç¾
        except Exception:
            return False
    
    async def _apply_cost_optimization(self, cost_data: Dict[str, Any]) -> bool:
        """æ‡‰ç”¨æˆæœ¬å„ªåŒ–"""
        try:
            # å¯¦ç¾æˆæœ¬å„ªåŒ–é‚è¼¯
            # ä¾‹å¦‚ï¼šæé«˜æœ¬åœ°æ¨¡å‹çš„æ¬Šé‡ï¼Œé™ä½é«˜æˆæœ¬æ¨¡å‹çš„ä½¿ç”¨
            return True  # ç°¡åŒ–å¯¦ç¾
        except Exception:
            return False
    
    # ==================== ç›£æ§å’Œçµ±è¨ˆæ¥å£ ====================
    
    def get_service_statistics(self) -> Dict[str, Any]:
        """ç²å–æœå‹™çµ±è¨ˆä¿¡æ¯"""
        stats = self.service_stats.copy()
        
        # æ·»åŠ é‹è¡Œæ™‚çµ±è¨ˆ
        uptime = datetime.now(timezone.utc) - stats['uptime_start']
        stats['uptime_hours'] = uptime.total_seconds() / 3600
        
        # æ·»åŠ æˆåŠŸç‡
        if stats['total_requests'] > 0:
            stats['success_rate'] = (
                (stats['total_requests'] - self._get_failure_count()) / stats['total_requests']
            )
        
        # æ·»åŠ æ™ºèƒ½è·¯ç”±ä½¿ç”¨ç‡
        if stats['total_requests'] > 0:
            stats['intelligent_routing_rate'] = (
                stats['intelligent_routed_requests'] / stats['total_requests']
            )
        
        # æ·»åŠ çµ„ä»¶ç‹€æ…‹
        stats['service_health'] = self._service_health
        stats['components_status'] = self._get_components_status()
        
        return stats
    
    def _get_failure_count(self) -> int:
        """ç²å–å¤±æ•—è¨ˆæ•¸ï¼ˆç°¡åŒ–å¯¦ç¾ï¼‰"""
        return 0  # å¯ä»¥å¾å¯¦éš›çš„éŒ¯èª¤è¿½è¹¤ç³»çµ±ç²å–
    
    def _get_active_components_count(self) -> int:
        """ç²å–æ´»å‹•çµ„ä»¶æ•¸é‡"""
        count = 1  # base_client
        
        if self.enable_intelligent_routing:
            components = [
                self.ai_router,
                self.model_service,
                self.performance_monitor,
                self.config_manager
            ]
            count += sum(1 for comp in components if comp is not None)
        
        return count
    
    def _get_components_status(self) -> Dict[str, str]:
        """ç²å–çµ„ä»¶ç‹€æ…‹"""
        status = {
            'base_llm_client': 'active',
            'intelligent_routing': 'active' if self.enable_intelligent_routing else 'disabled'
        }
        
        if self.enable_intelligent_routing:
            status.update({
                'ai_router': 'active' if self.ai_router and hasattr(self.ai_router, '_initialized') and self.ai_router._initialized else 'inactive',
                'model_service': 'active' if self.model_service and hasattr(self.model_service, '_initialized') and self.model_service._initialized else 'inactive',
                'performance_monitor': 'active' if self.performance_monitor else 'inactive',
                'config_manager': 'active' if self.config_manager else 'inactive'
            })
        
        return status
    
    def _calculate_recent_failure_rate(self) -> float:
        """è¨ˆç®—æœ€è¿‘çš„å¤±æ•—ç‡ï¼ˆç°¡åŒ–å¯¦ç¾ï¼‰"""
        # é€™è£¡æ‡‰è©²åŸºæ–¼å¯¦éš›çš„å¤±æ•—è¿½è¹¤æ•¸æ“š
        return 0.0  # ç°¡åŒ–ç‚º0å¤±æ•—ç‡
    
    def _update_routing_success_stats(self, routing_decision: Dict[str, Any]):
        """æ›´æ–°è·¯ç”±æˆåŠŸçµ±è¨ˆ"""
        confidence = routing_decision.get('confidence_score', 0)
        if confidence > 0.8:
            # é«˜ä¿¡å¿ƒåº¦çš„è·¯ç”±æ±ºç­–
            pass
    
    def _update_failure_stats(self):
        """æ›´æ–°å¤±æ•—çµ±è¨ˆ"""
        # è¨˜éŒ„å¤±æ•—çµ±è¨ˆ
        pass
    
    async def health_check(self) -> Dict[str, Any]:
        """æœå‹™å¥åº·æª¢æŸ¥"""
        health_status = {
            'service': 'enhanced_llm_service',
            'version': '1.3.1',
            'status': self._service_health.get('status', 'unknown'),
            'initialized': self._initialized,
            'intelligent_routing_enabled': self.enable_intelligent_routing,
            'timestamp': datetime.now(timezone.utc).isoformat(),
            'uptime_hours': (datetime.now(timezone.utc) - self.service_stats['uptime_start']).total_seconds() / 3600,
            'components': {}
        }
        
        try:
            # æª¢æŸ¥åŸºç¤LLMå®¢æˆ¶ç«¯
            base_health = await self.base_client.health_check()
            health_status['components']['base_client'] = base_health.get('status', 'unknown')
            
            # æª¢æŸ¥æ™ºèƒ½è·¯ç”±çµ„ä»¶
            if self.enable_intelligent_routing:
                if self.ai_router:
                    router_health = await self.ai_router.health_check()
                    health_status['components']['ai_router'] = router_health.get('overall_status', 'unknown')
                
                if self.model_service:
                    service_health = await self.model_service.health_check()
                    health_status['components']['model_service'] = service_health.get('overall_status', 'unknown')
        
        except Exception as e:
            health_status['status'] = 'degraded'
            health_status['error'] = str(e)
        
        # ç¢ºå®šæ•´é«”ç‹€æ…‹
        if health_status['status'] != 'degraded':
            component_statuses = list(health_status['components'].values())
            if all(status in ['healthy', 'active'] for status in component_statuses):
                health_status['status'] = 'healthy'
            elif any(status in ['unhealthy', 'error'] for status in component_statuses):
                health_status['status'] = 'degraded'
        
        return health_status
    
    async def shutdown(self):
        """é—œé–‰æœå‹™"""
        try:
            self.logger.info("ğŸ”„ Shutting down Enhanced LLM Service...")
            
            self._initialized = False
            
            # é—œé–‰æ™ºèƒ½è·¯ç”±çµ„ä»¶
            if self.enable_intelligent_routing:
                if self.performance_monitor:
                    await self.performance_monitor.stop()
                
                if self.model_service:
                    await self.model_service.shutdown()
            
            # é—œé–‰åŸºç¤å®¢æˆ¶ç«¯
            await self.base_client.close()
            
            self._service_health = {'status': 'shutdown'}
            self.logger.info("âœ… Enhanced LLM Service shutdown complete")
            
        except Exception as e:
            self.logger.error(f"âŒ Error during service shutdown: {e}")

# ==================== ä¾¿åˆ©å‡½æ•¸å’Œå·¥å» æ–¹æ³• ====================

def create_enhanced_llm_service(
    llm_config: Optional[Dict[str, Any]] = None,
    service_config: Optional[Dict[str, Any]] = None,
    enable_intelligent_routing: bool = True
) -> EnhancedLLMService:
    """
    å‰µå»ºå¢å¼·LLMæœå‹™çš„ä¾¿åˆ©å‡½æ•¸
    
    Args:
        llm_config: åŸºç¤LLMé…ç½®
        service_config: æœå‹™é…ç½®
        enable_intelligent_routing: æ˜¯å¦å•Ÿç”¨æ™ºèƒ½è·¯ç”±
        
    Returns:
        é…ç½®å¥½çš„å¢å¼·LLMæœå‹™
    """
    base_client = LLMClient(llm_config)
    
    return EnhancedLLMService(
        base_llm_client=base_client,
        enable_intelligent_routing=enable_intelligent_routing,
        config=service_config
    )

# å…¨å±€æœå‹™å¯¦ä¾‹ç®¡ç†
_global_enhanced_service: Optional[EnhancedLLMService] = None

async def get_global_enhanced_service() -> EnhancedLLMService:
    """ç²å–å…¨å±€å¢å¼·LLMæœå‹™å¯¦ä¾‹"""
    global _global_enhanced_service
    if _global_enhanced_service is None:
        _global_enhanced_service = create_enhanced_llm_service()
        await _global_enhanced_service.initialize()
    return _global_enhanced_service

async def shutdown_global_enhanced_service():
    """é—œé–‰å…¨å±€å¢å¼·LLMæœå‹™å¯¦ä¾‹"""
    global _global_enhanced_service
    if _global_enhanced_service:
        await _global_enhanced_service.shutdown()
        _global_enhanced_service = None

# ==================== å‘å¾Œå…¼å®¹æ€§åŒ…è£å™¨ ====================

class CompatibleLLMClient:
    """
    å®Œå…¨å‘å¾Œå…¼å®¹çš„LLMå®¢æˆ¶ç«¯åŒ…è£å™¨
    
    é€™å€‹åŒ…è£å™¨æä¾›èˆ‡åŸå§‹LLMClientå®Œå…¨ç›¸åŒçš„APIï¼Œ
    ä½†åœ¨å¾Œå°ä½¿ç”¨å¢å¼·çš„æ™ºèƒ½è·¯ç”±æœå‹™ã€‚
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self._service = None
        self._config = config
    
    async def _ensure_service(self):
        """ç¢ºä¿æœå‹™å·²åˆå§‹åŒ–"""
        if self._service is None:
            self._service = create_enhanced_llm_service(
                llm_config=self._config,
                enable_intelligent_routing=True
            )
            await self._service.initialize()
    
    async def analyze(self, *args, **kwargs) -> LLMResponse:
        """åˆ†ææ¥å£ï¼ˆå®Œå…¨å…¼å®¹ï¼‰"""
        await self._ensure_service()
        return await self._service.analyze(*args, **kwargs)
    
    async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æª¢æŸ¥æ¥å£"""
        await self._ensure_service()
        return await self._service.health_check()
    
    async def close(self):
        """é—œé–‰æ¥å£"""
        if self._service:
            await self._service.shutdown()
    
    def get_stats(self) -> Dict[str, Any]:
        """ç²å–çµ±è¨ˆä¿¡æ¯æ¥å£"""
        if self._service:
            return self._service.get_service_statistics()
        return {}

# ä¾¿åˆ©å‡½æ•¸ï¼šå‰µå»ºå…¼å®¹å®¢æˆ¶ç«¯
def create_compatible_llm_client(config: Optional[Dict[str, Any]] = None) -> CompatibleLLMClient:
    """å‰µå»ºå‘å¾Œå…¼å®¹çš„LLMå®¢æˆ¶ç«¯"""
    return CompatibleLLMClient(config)