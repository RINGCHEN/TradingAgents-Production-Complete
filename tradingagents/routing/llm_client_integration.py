#!/usr/bin/env python3
"""
LLM Client Integration - å°‡AITaskRouteré›†æˆåˆ°ç¾æœ‰LLMClient
GPT-OSSæ•´åˆä»»å‹™1.3.1 - ç„¡ç¸«å‡ç´šé›†æˆ

æä¾›ï¼š
- LLMClientçš„AIè·¯ç”±å¢å¼·ç‰ˆæœ¬
- å‘å¾Œå…¼å®¹çš„API
- æ¼¸é€²å¼å‡ç´šè·¯å¾‘
- å®Œæ•´çš„ä¼æ¥­ç´šè·¯ç”±åŠŸèƒ½
"""

import logging
import asyncio
from datetime import datetime, timezone
from typing import Dict, Any, Optional, List, Union

from .ai_task_router import (
    AITaskRouter, RoutingStrategy, RoutingWeights,
    AITaskRouterError, RoutingDecisionError
)
from .performance_feedback import PerformanceFeedbackSystem
from ..database.task_metadata_db import TaskMetadataDB
from ..database.model_capability_db import ModelCapabilityDB
from ..database.task_metadata_models import RoutingDecisionRequest
from ..monitoring.performance_monitor import PerformanceMonitor
from ..utils.llm_client import LLMClient, LLMRequest, LLMResponse, AnalysisType

logger = logging.getLogger(__name__)

class EnhancedLLMClient(LLMClient):
    """
    å¢å¼·ç‰ˆLLMå®¢æˆ¶ç«¯ï¼Œé›†æˆAIä»»å‹™æ™ºèƒ½è·¯ç”±åŠŸèƒ½
    
    ä¿æŒèˆ‡åŸå§‹LLMClientå®Œå…¨å…¼å®¹çš„åŒæ™‚ï¼Œæ·»åŠ ï¼š
    - æ™ºèƒ½è·¯ç”±æ±ºç­–
    - æ€§èƒ½åé¥‹å­¸ç¿’
    - å‹•æ…‹ç­–ç•¥èª¿æ•´
    - å®Œæ•´çš„æ±ºç­–å¯©è¨ˆ
    """
    
    def __init__(
        self,
        config: Optional[Dict[str, Any]] = None,
        enable_ai_routing: bool = True,
        router_config: Optional[Dict[str, Any]] = None,
        feedback_config: Optional[Dict[str, Any]] = None
    ):
        """
        åˆå§‹åŒ–å¢å¼·ç‰ˆLLMå®¢æˆ¶ç«¯
        
        Args:
            config: åŸºç¤LLMé…ç½®
            enable_ai_routing: æ˜¯å¦å•Ÿç”¨AIæ™ºèƒ½è·¯ç”±
            router_config: è·¯ç”±å™¨é…ç½®
            feedback_config: åé¥‹ç³»çµ±é…ç½®
        """
        # åˆå§‹åŒ–åŸºç¤LLMClient
        super().__init__(config)
        
        # AIè·¯ç”±åŠŸèƒ½é…ç½®
        self.enable_ai_routing = enable_ai_routing
        
        if self.enable_ai_routing:
            # åˆå§‹åŒ–è·¯ç”±çµ„ä»¶
            self.task_db = TaskMetadataDB()
            self.model_db = ModelCapabilityDB()
            self.performance_monitor = PerformanceMonitor(
                model_db=self.model_db,
                task_db=self.task_db
            )
            
            # å‰µå»ºAIè·¯ç”±å™¨
            self.ai_router = AITaskRouter(
                task_db=self.task_db,
                model_db=self.model_db,
                performance_monitor=self.performance_monitor,
                config=router_config or {}
            )
            
            # å‰µå»ºæ€§èƒ½åé¥‹ç³»çµ±
            self.feedback_system = PerformanceFeedbackSystem(
                config=feedback_config or {}
            )
            
            # è·¯ç”±çµ±è¨ˆ
            self.routing_stats = {
                'ai_routing_enabled': True,
                'total_ai_routed_requests': 0,
                'fallback_to_legacy_routing': 0,
                'performance_feedback_records': 0,
                'last_routing_decision': None
            }
        else:
            # ç¦ç”¨AIè·¯ç”±æ™‚çš„ä½”ä½ç¬¦
            self.ai_router = None
            self.feedback_system = None
            self.routing_stats = {
                'ai_routing_enabled': False,
                'note': 'AI routing disabled, using legacy routing only'
            }
        
        self._ai_routing_initialized = False
        self.logger = logger
    
    async def initialize_ai_routing(self) -> bool:
        """
        åˆå§‹åŒ–AIè·¯ç”±åŠŸèƒ½
        
        Returns:
            æ˜¯å¦åˆå§‹åŒ–æˆåŠŸ
        """
        if not self.enable_ai_routing:
            self.logger.info("â„¹ï¸ AI routing disabled, using legacy routing")
            return True
        
        if self._ai_routing_initialized:
            return True
        
        try:
            self.logger.info("ğŸš€ Initializing AI Task Router integration...")
            
            # åˆå§‹åŒ–å„çµ„ä»¶
            await self.ai_router.initialize()
            await self.feedback_system.initialize()
            await self.performance_monitor.start()
            
            self._ai_routing_initialized = True
            self.logger.info("âœ… AI Task Router integration initialized successfully")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Failed to initialize AI routing: {e}")
            self.logger.info("ğŸ“± Falling back to legacy routing")
            return False
    
    async def analyze(
        self,
        prompt: str,
        context: Optional[Dict[str, Any]] = None,
        analysis_type: AnalysisType = AnalysisType.TECHNICAL,
        analyst_id: str = "general",
        stock_id: Optional[str] = None,
        user_id: Optional[str] = None,
        # æ–°å¢çš„AIè·¯ç”±åƒæ•¸
        routing_strategy: Optional[RoutingStrategy] = None,
        custom_weights: Optional[RoutingWeights] = None,
        enable_feedback_collection: bool = True,
        user_tier: str = "standard",
        priority: str = "standard",
        **kwargs
    ) -> LLMResponse:
        """
        åŸ·è¡Œåˆ†æï¼ˆå¢å¼·ç‰ˆï¼Œæ”¯æŒAIæ™ºèƒ½è·¯ç”±ï¼‰
        
        Args:
            prompt: åˆ†ææç¤ºè©
            context: åˆ†æä¸Šä¸‹æ–‡æ•¸æ“š
            analysis_type: åˆ†æé¡å‹
            analyst_id: åˆ†æå¸«ID
            stock_id: è‚¡ç¥¨ä»£ç¢¼
            user_id: ç”¨æˆ¶ID
            routing_strategy: è·¯ç”±ç­–ç•¥ï¼ˆAIè·¯ç”±ï¼‰
            custom_weights: è‡ªå®šç¾©æ¬Šé‡ï¼ˆAIè·¯ç”±ï¼‰
            enable_feedback_collection: æ˜¯å¦æ”¶é›†æ€§èƒ½åé¥‹
            user_tier: ç”¨æˆ¶ç­‰ç´š
            priority: è«‹æ±‚å„ªå…ˆç´š
            **kwargs: å…¶ä»–åƒæ•¸
            
        Returns:
            LLMå›æ‡‰
        """
        # ç¢ºä¿AIè·¯ç”±å·²åˆå§‹åŒ–
        if self.enable_ai_routing and not self._ai_routing_initialized:
            await self.initialize_ai_routing()
        
        # ä½¿ç”¨AIè·¯ç”±æˆ–å›é€€åˆ°å‚³çµ±è·¯ç”±
        if self.enable_ai_routing and self._ai_routing_initialized:
            return await self._analyze_with_ai_routing(
                prompt=prompt,
                context=context,
                analysis_type=analysis_type,
                analyst_id=analyst_id,
                stock_id=stock_id,
                user_id=user_id,
                routing_strategy=routing_strategy,
                custom_weights=custom_weights,
                enable_feedback_collection=enable_feedback_collection,
                user_tier=user_tier,
                priority=priority,
                **kwargs
            )
        else:
            # å›é€€åˆ°å‚³çµ±è·¯ç”±
            self.routing_stats['fallback_to_legacy_routing'] += 1
            return await super().analyze(
                prompt=prompt,
                context=context,
                analysis_type=analysis_type,
                analyst_id=analyst_id,
                stock_id=stock_id,
                user_id=user_id,
                **kwargs
            )
    
    async def _analyze_with_ai_routing(
        self,
        prompt: str,
        context: Optional[Dict[str, Any]],
        analysis_type: AnalysisType,
        analyst_id: str,
        stock_id: Optional[str],
        user_id: Optional[str],
        routing_strategy: Optional[RoutingStrategy],
        custom_weights: Optional[RoutingWeights],
        enable_feedback_collection: bool,
        user_tier: str,
        priority: str,
        **kwargs
    ) -> LLMResponse:
        """ä½¿ç”¨AIè·¯ç”±åŸ·è¡Œåˆ†æ"""
        try:
            start_time = datetime.now(timezone.utc)
            
            # 1. å‰µå»ºè·¯ç”±æ±ºç­–è«‹æ±‚
            task_type = self._map_analysis_type_to_task_type(analysis_type)
            
            routing_request = RoutingDecisionRequest(
                task_type=task_type,
                user_tier=user_tier,
                estimated_tokens=self._estimate_tokens(prompt, context),
                priority=priority,
                requires_high_quality=kwargs.get('requires_high_quality', False),
                max_acceptable_latency=kwargs.get('max_acceptable_latency'),
                max_acceptable_cost=kwargs.get('max_acceptable_cost'),
                user_preferences=kwargs.get('user_preferences')
            )
            
            # 2. åŸ·è¡Œæ™ºèƒ½è·¯ç”±æ±ºç­–
            routing_decision = await self.ai_router.make_routing_decision(
                request=routing_request,
                strategy=routing_strategy,
                custom_weights=custom_weights
            )
            
            self.routing_stats['total_ai_routed_requests'] += 1
            self.routing_stats['last_routing_decision'] = {
                'provider': routing_decision.selected_provider,
                'model': routing_decision.selected_model,
                'confidence': routing_decision.confidence_score,
                'timestamp': datetime.now(timezone.utc).isoformat()
            }
            
            self.logger.info(
                f"ğŸ¯ AI routing decision: {routing_decision.selected_provider}/"
                f"{routing_decision.selected_model} (confidence: {routing_decision.confidence_score:.3f})"
            )
            
            # 3. å¼·åˆ¶ä½¿ç”¨è·¯ç”±æ±ºç­–é¸æ“‡çš„æä¾›å•†å’Œæ¨¡å‹
            original_provider = self.provider
            original_model = self.model
            
            try:
                # è‡¨æ™‚åˆ‡æ›åˆ°è·¯ç”±é¸æ“‡çš„æä¾›å•†å’Œæ¨¡å‹
                self._temporarily_switch_provider(
                    routing_decision.selected_provider,
                    routing_decision.selected_model
                )
                
                # 4. åŸ·è¡Œå¯¦éš›çš„LLMè«‹æ±‚
                request = LLMRequest(
                    prompt=prompt,
                    context=context or {},
                    analysis_type=analysis_type,
                    analyst_id=analyst_id,
                    stock_id=stock_id,
                    user_id=user_id,
                    max_tokens=kwargs.get('max_tokens'),
                    temperature=kwargs.get('temperature')
                )
                
                response = await self._execute_request(request)
                
                # 5. å¢å¼·éŸ¿æ‡‰ä¿¡æ¯
                response.metadata = response.metadata or {}
                response.metadata.update({
                    'ai_routing_decision': {
                        'selected_provider': routing_decision.selected_provider,
                        'selected_model': routing_decision.selected_model,
                        'reasoning': routing_decision.reasoning,
                        'confidence_score': routing_decision.confidence_score,
                        'expected_cost': routing_decision.expected_cost,
                        'expected_latency_ms': routing_decision.expected_latency_ms,
                        'fallback_options': routing_decision.fallback_options
                    },
                    'routing_execution_time_ms': (datetime.now(timezone.utc) - start_time).total_seconds() * 1000
                })
                
                # 6. æ”¶é›†æ€§èƒ½åé¥‹ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
                if enable_feedback_collection and self.feedback_system:
                    await self._collect_performance_feedback(
                        routing_decision=routing_decision,
                        response=response,
                        request_context={
                            'task_type': task_type,
                            'user_id': user_id,
                            'request_start_time': start_time
                        }
                    )
                
                return response
                
            finally:
                # æ¢å¾©åŸå§‹æä¾›å•†å’Œæ¨¡å‹è¨­ç½®
                self.provider = original_provider
                self.model = original_model
            
        except (AITaskRouterError, RoutingDecisionError) as e:
            self.logger.warning(f"âš ï¸ AI routing failed: {e}. Falling back to legacy routing")
            self.routing_stats['fallback_to_legacy_routing'] += 1
            
            # å›é€€åˆ°å‚³çµ±è·¯ç”±
            return await super().analyze(
                prompt=prompt,
                context=context,
                analysis_type=analysis_type,
                analyst_id=analyst_id,
                stock_id=stock_id,
                user_id=user_id,
                **kwargs
            )
        except Exception as e:
            self.logger.error(f"âŒ Unexpected error in AI routing: {e}")
            self.routing_stats['fallback_to_legacy_routing'] += 1
            raise
    
    def _map_analysis_type_to_task_type(self, analysis_type: AnalysisType) -> str:
        """å°‡åˆ†æé¡å‹æ˜ å°„ç‚ºä»»å‹™é¡å‹"""
        mapping = {
            AnalysisType.TECHNICAL: "technical_analysis",
            AnalysisType.FUNDAMENTAL: "financial_summary",
            AnalysisType.NEWS: "news_classification",
            AnalysisType.SENTIMENT: "market_sentiment",
            AnalysisType.RISK: "risk_assessment",
            AnalysisType.INVESTMENT: "investment_reasoning",
            AnalysisType.REASONING: "investment_reasoning",
            AnalysisType.GENERATION: "report_generation",
            AnalysisType.ANALYSIS: "financial_summary"
        }
        return mapping.get(analysis_type, "financial_summary")
    
    def _estimate_tokens(self, prompt: str, context: Optional[Dict[str, Any]]) -> float:
        """ä¼°ç®—tokenæ•¸é‡"""
        # ç°¡åŒ–çš„tokenä¼°ç®—
        token_count = len(prompt.split()) * 1.3  # è‹±æ–‡å–®è©åˆ°tokençš„ç²—ç•¥æ¯”ä¾‹
        
        if context:
            context_text = str(context)
            token_count += len(context_text.split()) * 1.3
        
        return token_count
    
    def _temporarily_switch_provider(self, provider: str, model: str):
        """è‡¨æ™‚åˆ‡æ›æä¾›å•†å’Œæ¨¡å‹"""
        # æ ¹æ“šè·¯ç”±æ±ºç­–è‡¨æ™‚åˆ‡æ›æä¾›å•†
        from ..utils.llm_client import LLMProvider
        
        if provider == "openai":
            self.provider = LLMProvider.OPENAI
        elif provider == "anthropic":
            self.provider = LLMProvider.ANTHROPIC
        elif provider == "gpt_oss":
            self.provider = LLMProvider.GPT_OSS
        
        self.model = model
    
    async def _collect_performance_feedback(
        self,
        routing_decision,
        response: LLMResponse,
        request_context: Dict[str, Any]
    ):
        """æ”¶é›†æ€§èƒ½åé¥‹"""
        try:
            if not response.success:
                # è¨˜éŒ„å¤±æ•—åé¥‹
                self.feedback_system.record_execution_feedback(
                    decision_id=str(hash(routing_decision.decision_metadata.get('request_id', ''))),
                    request_id=request_context.get('user_id', 'unknown'),
                    provider=routing_decision.selected_provider,
                    model_id=routing_decision.selected_model,
                    task_type=request_context['task_type'],
                    predicted_cost=routing_decision.expected_cost,
                    actual_cost=0.0,  # å¤±æ•—æ™‚æˆæœ¬ç‚º0
                    predicted_latency=routing_decision.expected_latency_ms,
                    actual_latency=response.response_time * 1000 if response.response_time else 0,
                    predicted_quality=routing_decision.expected_quality_score,
                    actual_quality=0.0,  # å¤±æ•—æ™‚å“è³ªç‚º0
                    execution_success=False
                )
            else:
                # è¨ˆç®—å¯¦éš›æŒ‡æ¨™
                actual_cost = self._calculate_actual_cost(response)
                actual_latency = response.response_time * 1000 if response.response_time else 0
                actual_quality = self._evaluate_response_quality(response, request_context['task_type'])
                
                self.feedback_system.record_execution_feedback(
                    decision_id=str(hash(routing_decision.decision_metadata.get('request_id', ''))),
                    request_id=request_context.get('user_id', 'unknown'),
                    provider=routing_decision.selected_provider,
                    model_id=routing_decision.selected_model,
                    task_type=request_context['task_type'],
                    predicted_cost=routing_decision.expected_cost,
                    actual_cost=actual_cost,
                    predicted_latency=routing_decision.expected_latency_ms,
                    actual_latency=actual_latency,
                    predicted_quality=routing_decision.expected_quality_score,
                    actual_quality=actual_quality,
                    execution_success=True
                )
            
            self.routing_stats['performance_feedback_records'] += 1
            
        except Exception as e:
            self.logger.error(f"âŒ Failed to collect performance feedback: {e}")
    
    def _calculate_actual_cost(self, response: LLMResponse) -> float:
        """è¨ˆç®—å¯¦éš›æˆæœ¬"""
        if not response.usage:
            return 0.0
        
        # ç°¡åŒ–çš„æˆæœ¬è¨ˆç®—
        input_tokens = response.usage.get('prompt_tokens', 0)
        output_tokens = response.usage.get('completion_tokens', 0)
        
        # ä½¿ç”¨åŸºæœ¬çš„æˆæœ¬ä¼°ç®—
        cost_per_1k = self._get_cost_per_1k_tokens(response.provider, response.model)
        return ((input_tokens + output_tokens) / 1000) * cost_per_1k
    
    def _evaluate_response_quality(self, response: LLMResponse, task_type: str) -> float:
        """è©•ä¼°éŸ¿æ‡‰å“è³ª"""
        if not response.content:
            return 0.0
        
        # åŸºæœ¬å“è³ªè©•ä¼°
        base_score = 0.7
        
        # å…§å®¹é•·åº¦æª¢æŸ¥
        if len(response.content) > 50:
            base_score += 0.1
        
        # JSONæ ¼å¼æª¢æŸ¥
        content = response.content.strip()
        if content.startswith('{') and content.endswith('}'):
            base_score += 0.1
        
        return min(1.0, base_score)
    
    # ==================== AIè·¯ç”±ç®¡ç†æ¥å£ ====================
    
    async def optimize_routing_weights(
        self,
        strategy: RoutingStrategy = RoutingStrategy.BALANCED,
        analysis_hours: int = 72
    ) -> Dict[str, Any]:
        """
        åŸºæ–¼æ€§èƒ½åé¥‹å„ªåŒ–è·¯ç”±æ¬Šé‡
        
        Args:
            strategy: è¦å„ªåŒ–çš„è·¯ç”±ç­–ç•¥
            analysis_hours: åˆ†ææ™‚é–“çª—å£
            
        Returns:
            å„ªåŒ–çµæœ
        """
        if not self.enable_ai_routing or not self._ai_routing_initialized:
            return {'status': 'ai_routing_disabled'}
        
        try:
            current_weights = self.ai_router.routing_strategies.get(strategy)
            if not current_weights:
                return {'status': 'strategy_not_found'}
            
            # ç²å–æ¬Šé‡èª¿æ•´å»ºè­°
            suggested_weights, reasons = self.feedback_system.suggest_weight_adjustments(
                current_weights=current_weights,
                strategy=strategy,
                analysis_hours=analysis_hours
            )
            
            # æ‡‰ç”¨èª¿æ•´
            if reasons and reasons[0] != 'æ•¸æ“šä¸è¶³ï¼Œç„¡æ³•é€²è¡Œæ¬Šé‡èª¿æ•´å»ºè­°':
                success = self.feedback_system.apply_weight_adjustments(
                    router=self.ai_router,
                    suggested_weights=suggested_weights,
                    strategy=strategy,
                    reasons=reasons
                )
                
                return {
                    'status': 'optimized' if success else 'optimization_failed',
                    'strategy': strategy.value,
                    'old_weights': current_weights.__dict__,
                    'new_weights': suggested_weights.__dict__,
                    'reasons': reasons,
                    'timestamp': datetime.now(timezone.utc).isoformat()
                }
            else:
                return {
                    'status': 'no_optimization_needed',
                    'strategy': strategy.value,
                    'current_weights': current_weights.__dict__,
                    'reason': reasons[0] if reasons else 'No feedback data available'
                }
            
        except Exception as e:
            self.logger.error(f"âŒ Weight optimization failed: {e}")
            return {'status': 'error', 'error': str(e)}
    
    def get_ai_routing_statistics(self) -> Dict[str, Any]:
        """ç²å–AIè·¯ç”±çµ±è¨ˆä¿¡æ¯"""
        if not self.enable_ai_routing:
            return self.routing_stats
        
        try:
            stats = self.routing_stats.copy()
            
            if self._ai_routing_initialized:
                # æ·»åŠ è·¯ç”±å™¨çµ±è¨ˆ
                router_stats = self.ai_router.get_routing_statistics()
                stats.update({
                    'router_statistics': router_stats,
                    'feedback_statistics': self.feedback_system.get_feedback_statistics(),
                    'decision_history_count': len(self.ai_router.get_decision_history())
                })
            
            return stats
            
        except Exception as e:
            self.logger.error(f"âŒ Failed to get AI routing statistics: {e}")
            return self.routing_stats
    
    def get_routing_decision_history(self, limit: int = 20) -> List[Dict[str, Any]]:
        """ç²å–è·¯ç”±æ±ºç­–æ­·å²"""
        if not self.enable_ai_routing or not self._ai_routing_initialized:
            return []
        
        try:
            return self.ai_router.get_decision_history(limit=limit)
        except Exception as e:
            self.logger.error(f"âŒ Failed to get routing decision history: {e}")
            return []
    
    def get_model_performance_report(
        self,
        provider: Optional[str] = None,
        model_id: Optional[str] = None,
        hours_back: int = 168
    ) -> Dict[str, Any]:
        """ç²å–æ¨¡å‹æ€§èƒ½å ±å‘Š"""
        if not self.enable_ai_routing or not self._ai_routing_initialized:
            return {'status': 'ai_routing_disabled'}
        
        try:
            return self.feedback_system.get_model_performance_summary(
                provider=provider,
                model_id=model_id,
                hours_back=hours_back
            )
        except Exception as e:
            self.logger.error(f"âŒ Failed to get model performance report: {e}")
            return {'status': 'error', 'error': str(e)}
    
    # ==================== å¢å¼·çš„å¥åº·æª¢æŸ¥ ====================
    
    async def health_check(self) -> Dict[str, Any]:
        """å¢å¼·çš„å¥åº·æª¢æŸ¥ï¼ŒåŒ…å«AIè·¯ç”±ç‹€æ…‹"""
        # ç²å–åŸºç¤å¥åº·æª¢æŸ¥
        health_status = await super().health_check()
        
        # æ·»åŠ AIè·¯ç”±å¥åº·ç‹€æ…‹
        if self.enable_ai_routing:
            try:
                if self._ai_routing_initialized:
                    router_health = await self.ai_router.health_check()
                    feedback_health = await self.feedback_system.health_check()
                    
                    health_status['ai_routing'] = {
                        'enabled': True,
                        'initialized': True,
                        'router_status': router_health['overall_status'],
                        'feedback_status': feedback_health['overall_status'],
                        'statistics': self.routing_stats
                    }
                else:
                    health_status['ai_routing'] = {
                        'enabled': True,
                        'initialized': False,
                        'note': 'AI routing not yet initialized'
                    }
            except Exception as e:
                health_status['ai_routing'] = {
                    'enabled': True,
                    'status': 'error',
                    'error': str(e)
                }
        else:
            health_status['ai_routing'] = {
                'enabled': False,
                'note': 'AI routing disabled'
            }
        
        return health_status
    
    async def close(self):
        """é—œé–‰å®¢æˆ¶ç«¯ï¼ŒåŒ…æ‹¬AIè·¯ç”±çµ„ä»¶"""
        try:
            # é—œé–‰AIè·¯ç”±çµ„ä»¶
            if self.enable_ai_routing and self._ai_routing_initialized:
                if self.performance_monitor:
                    await self.performance_monitor.stop()
                
            # é—œé–‰åŸºç¤LLMå®¢æˆ¶ç«¯
            await super().close()
            
            self.logger.info("âœ… Enhanced LLM Client closed successfully")
            
        except Exception as e:
            self.logger.error(f"âŒ Error closing Enhanced LLM Client: {e}")

# ==================== ä¾¿åˆ©å‡½æ•¸ ====================

def create_enhanced_llm_client(
    llm_config: Optional[Dict[str, Any]] = None,
    enable_ai_routing: bool = True,
    router_config: Optional[Dict[str, Any]] = None,
    feedback_config: Optional[Dict[str, Any]] = None
) -> EnhancedLLMClient:
    """
    å‰µå»ºå¢å¼·ç‰ˆLLMå®¢æˆ¶ç«¯çš„ä¾¿åˆ©å‡½æ•¸
    
    Args:
        llm_config: åŸºç¤LLMé…ç½®
        enable_ai_routing: æ˜¯å¦å•Ÿç”¨AIè·¯ç”±
        router_config: è·¯ç”±å™¨é…ç½®
        feedback_config: åé¥‹ç³»çµ±é…ç½®
        
    Returns:
        é…ç½®å¥½çš„å¢å¼·ç‰ˆLLMå®¢æˆ¶ç«¯
    """
    return EnhancedLLMClient(
        config=llm_config,
        enable_ai_routing=enable_ai_routing,
        router_config=router_config,
        feedback_config=feedback_config
    )

# å…¨å±€å¢å¼·å®¢æˆ¶ç«¯ç®¡ç†
_global_enhanced_llm_client: Optional[EnhancedLLMClient] = None

def get_global_enhanced_llm_client() -> EnhancedLLMClient:
    """ç²å–å…¨å±€å¢å¼·ç‰ˆLLMå®¢æˆ¶ç«¯"""
    global _global_enhanced_llm_client
    if _global_enhanced_llm_client is None:
        _global_enhanced_llm_client = create_enhanced_llm_client()
    return _global_enhanced_llm_client

async def close_global_enhanced_llm_client():
    """é—œé–‰å…¨å±€å¢å¼·ç‰ˆLLMå®¢æˆ¶ç«¯"""
    global _global_enhanced_llm_client
    if _global_enhanced_llm_client:
        await _global_enhanced_llm_client.close()
        _global_enhanced_llm_client = None