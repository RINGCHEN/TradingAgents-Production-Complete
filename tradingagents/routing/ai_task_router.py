#!/usr/bin/env python3
"""
AI Task Router - ä¼æ¥­ç´šæ™ºèƒ½è·¯ç”±æ±ºç­–å¼•æ“
GPT-OSSæ•´åˆä»»å‹™1.3.1 - æ ¸å¿ƒè·¯ç”±æ±ºç­–å¼•æ“å¯¦ç¾

åŸºæ–¼ç¾æœ‰åŸºç¤è¨­æ–½ï¼Œå¯¦ç¾é«˜ç´šAIä»»å‹™è·¯ç”±æ±ºç­–ç³»çµ±ï¼š
- æ™ºèƒ½æˆæœ¬æ•ˆç›Šåˆ†æ
- å¤šç¶­åº¦è·¯ç”±æ±ºç­–
- å®Œæ•´æ±ºç­–å¯©è¨ˆ
- å‹•æ…‹ç­–ç•¥èª¿æ•´
"""

import uuid
import json
import logging
import asyncio
import statistics
from datetime import datetime, timezone, timedelta
from typing import Dict, Any, List, Optional, Tuple, Union, NamedTuple
from dataclasses import dataclass, field, asdict
from enum import Enum
from contextlib import asynccontextmanager

from ..database.task_metadata_db import TaskMetadataDB, TaskMetadataDBError
from ..database.model_capability_db import ModelCapabilityDB, ModelCapabilityDBError
from ..database.task_metadata_models import (
    RoutingDecisionRequest, RoutingDecisionResponse,
    TaskMetadataResponse, ModelCapabilityResponse
)
from ..utils.llm_client import LLMProvider
from ..monitoring.performance_monitor import PerformanceMonitor

logger = logging.getLogger(__name__)

# ==================== æ ¸å¿ƒæ•¸æ“šé¡å‹ ====================

class RoutingStrategy(Enum):
    """è·¯ç”±ç­–ç•¥æšèˆ‰"""
    COST_OPTIMIZED = "cost_optimized"           # æˆæœ¬å„ªåŒ–
    PERFORMANCE_OPTIMIZED = "performance_optimized"  # æ€§èƒ½å„ªåŒ–  
    BALANCED = "balanced"                       # å¹³è¡¡æ¨¡å¼
    QUALITY_FIRST = "quality_first"             # å“è³ªå„ªå…ˆ
    LATENCY_FIRST = "latency_first"             # å»¶é²å„ªå…ˆ
    PRIVACY_FIRST = "privacy_first"             # éš±ç§å„ªå…ˆ
    CUSTOM = "custom"                          # è‡ªå®šç¾©ç­–ç•¥

class DecisionFactor(Enum):
    """æ±ºç­–å› å­æšèˆ‰"""
    COST = "cost"
    LATENCY = "latency"
    QUALITY = "quality"
    AVAILABILITY = "availability"
    PRIVACY = "privacy"
    USER_PREFERENCE = "user_preference"
    HISTORICAL_PERFORMANCE = "historical_performance"
    LOAD_BALANCING = "load_balancing"

@dataclass
class RoutingWeights:
    """è·¯ç”±æ±ºç­–æ¬Šé‡é…ç½®"""
    cost: float = 0.3
    latency: float = 0.2
    quality: float = 0.25
    availability: float = 0.15
    privacy: float = 0.05
    user_preference: float = 0.05
    
    def normalize(self):
        """æ¨™æº–åŒ–æ¬Šé‡ä½¿ç¸½å’Œç‚º1.0"""
        total = self.cost + self.latency + self.quality + self.availability + self.privacy + self.user_preference
        if total > 0:
            self.cost /= total
            self.latency /= total
            self.quality /= total
            self.availability /= total
            self.privacy /= total
            self.user_preference /= total

@dataclass
class ModelScore:
    """æ¨¡å‹è©•åˆ†è©³æƒ…"""
    model: ModelCapabilityResponse
    total_score: float
    factor_scores: Dict[DecisionFactor, float] = field(default_factory=dict)
    cost_analysis: Dict[str, Any] = field(default_factory=dict)
    performance_prediction: Dict[str, Any] = field(default_factory=dict)
    confidence: float = 0.0
    reasoning: List[str] = field(default_factory=list)

@dataclass
class RoutingContext:
    """è·¯ç”±ä¸Šä¸‹æ–‡ä¿¡æ¯"""
    request_id: str
    task_metadata: TaskMetadataResponse
    available_models: List[ModelCapabilityResponse]
    routing_request: RoutingDecisionRequest
    strategy: RoutingStrategy
    weights: RoutingWeights
    timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    user_context: Dict[str, Any] = field(default_factory=dict)

@dataclass
class DecisionAudit:
    """æ±ºç­–å¯©è¨ˆè¨˜éŒ„"""
    decision_id: str
    request_id: str
    task_type: str
    selected_model: ModelCapabilityResponse
    all_model_scores: List[ModelScore]
    routing_context: RoutingContext
    decision_reasoning: List[str]
    confidence_score: float
    execution_time_ms: float
    fallback_chain: List[str]
    timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    
    def to_dict(self) -> Dict[str, Any]:
        """è½‰æ›ç‚ºå­—å…¸æ ¼å¼ä¾›å­˜å„²"""
        return {
            'decision_id': self.decision_id,
            'request_id': self.request_id,
            'task_type': self.task_type,
            'selected_provider': self.selected_model.provider,
            'selected_model': self.selected_model.model_id,
            'selected_model_name': self.selected_model.model_name,
            'total_candidates': len(self.all_model_scores),
            'decision_reasoning': self.decision_reasoning,
            'confidence_score': self.confidence_score,
            'execution_time_ms': self.execution_time_ms,
            'routing_strategy': self.routing_context.strategy.value,
            'weights_used': asdict(self.routing_context.weights),
            'fallback_chain': self.fallback_chain,
            'timestamp': self.timestamp.isoformat(),
            'model_scores': [
                {
                    'provider': score.model.provider,
                    'model_id': score.model.model_id,
                    'total_score': score.total_score,
                    'confidence': score.confidence,
                    'cost_analysis': score.cost_analysis,
                    'reasoning': score.reasoning
                }
                for score in self.all_model_scores
            ]
        }

# ==================== ç•°å¸¸é¡å‹ ====================

class AITaskRouterError(Exception):
    """AIä»»å‹™è·¯ç”±å™¨åŸºç¤éŒ¯èª¤"""
    pass

class RoutingDecisionError(AITaskRouterError):
    """è·¯ç”±æ±ºç­–éŒ¯èª¤"""
    pass

class CostCalculationError(AITaskRouterError):
    """æˆæœ¬è¨ˆç®—éŒ¯èª¤"""
    pass

class ModelEvaluationError(AITaskRouterError):
    """æ¨¡å‹è©•ä¼°éŒ¯èª¤"""
    pass

# ==================== æ ¸å¿ƒè·¯ç”±å™¨é¡ ====================

class AITaskRouter:
    """
    ä¼æ¥­ç´šAIä»»å‹™æ™ºèƒ½è·¯ç”±å™¨
    
    åŠŸèƒ½ç‰¹æ€§ï¼š
    1. å¤šç¶­åº¦æ™ºèƒ½è·¯ç”±æ±ºç­–
    2. çœŸå¯¦æˆæœ¬æ•ˆç›Šè¨ˆç®—  
    3. å®Œæ•´æ±ºç­–å¯©è¨ˆè¿½è¹¤
    4. å‹•æ…‹ç­–ç•¥èª¿æ•´
    5. æ€§èƒ½åé¥‹å­¸ç¿’
    """
    
    def __init__(
        self,
        task_db: Optional[TaskMetadataDB] = None,
        model_db: Optional[ModelCapabilityDB] = None,
        performance_monitor: Optional[PerformanceMonitor] = None,
        config: Optional[Dict[str, Any]] = None
    ):
        """
        åˆå§‹åŒ–AIä»»å‹™è·¯ç”±å™¨
        
        Args:
            task_db: ä»»å‹™å…ƒæ•¸æ“šæ•¸æ“šåº«
            model_db: æ¨¡å‹èƒ½åŠ›æ•¸æ“šåº«  
            performance_monitor: æ€§èƒ½ç›£æ§å™¨
            config: è·¯ç”±å™¨é…ç½®
        """
        self.task_db = task_db or TaskMetadataDB()
        self.model_db = model_db or ModelCapabilityDB()
        self.performance_monitor = performance_monitor
        
        # é…ç½®åˆå§‹åŒ–
        self.config = config or {}
        self._load_default_config()
        
        # è·¯ç”±ç­–ç•¥é…ç½®
        self.routing_strategies = self._initialize_routing_strategies()
        self.default_strategy = RoutingStrategy(
            self.config.get('default_strategy', 'balanced')
        )
        
        # æ±ºç­–å¯©è¨ˆå­˜å„²
        self.decision_history: List[DecisionAudit] = []
        self.max_history_size = self.config.get('max_decision_history', 1000)
        
        # æ€§èƒ½çµ±è¨ˆ
        self.stats = {
            'total_decisions': 0,
            'successful_decisions': 0,
            'failed_decisions': 0,
            'average_decision_time_ms': 0.0,
            'strategy_usage': {strategy.value: 0 for strategy in RoutingStrategy},
            'model_selection_frequency': {},
            'cost_savings': 0.0,
            'last_reset': datetime.now(timezone.utc)
        }
        
        # æ¨¡å‹æ€§èƒ½ç·©å­˜
        self.model_performance_cache = {}
        self.cache_ttl_seconds = self.config.get('performance_cache_ttl', 3600)
        
        self.logger = logger
        self._initialized = False
    
    def _load_default_config(self):
        """è¼‰å…¥é è¨­é…ç½®"""
        defaults = {
            'enable_cost_optimization': True,
            'enable_performance_learning': True,
            'enable_load_balancing': True,
            'max_fallback_attempts': 3,
            'decision_confidence_threshold': 0.7,
            'cost_variance_threshold': 0.15,
            'latency_variance_threshold': 0.20,
            'quality_variance_threshold': 0.10,
            'performance_cache_ttl': 3600,
            'max_decision_history': 1000,
            'default_strategy': 'balanced',
            'enable_audit_logging': True,
            'audit_detail_level': 'full'
        }
        
        for key, value in defaults.items():
            if key not in self.config:
                self.config[key] = value
    
    def _initialize_routing_strategies(self) -> Dict[RoutingStrategy, RoutingWeights]:
        """åˆå§‹åŒ–è·¯ç”±ç­–ç•¥æ¬Šé‡é…ç½®"""
        strategies = {
            RoutingStrategy.COST_OPTIMIZED: RoutingWeights(
                cost=0.5, latency=0.15, quality=0.2, availability=0.1, privacy=0.03, user_preference=0.02
            ),
            RoutingStrategy.PERFORMANCE_OPTIMIZED: RoutingWeights(
                cost=0.1, latency=0.4, quality=0.35, availability=0.1, privacy=0.03, user_preference=0.02
            ),
            RoutingStrategy.BALANCED: RoutingWeights(
                cost=0.25, latency=0.25, quality=0.25, availability=0.15, privacy=0.05, user_preference=0.05
            ),
            RoutingStrategy.QUALITY_FIRST: RoutingWeights(
                cost=0.1, latency=0.15, quality=0.5, availability=0.15, privacy=0.05, user_preference=0.05
            ),
            RoutingStrategy.LATENCY_FIRST: RoutingWeights(
                cost=0.15, latency=0.5, quality=0.2, availability=0.1, privacy=0.03, user_preference=0.02
            ),
            RoutingStrategy.PRIVACY_FIRST: RoutingWeights(
                cost=0.15, latency=0.15, quality=0.2, availability=0.1, privacy=0.35, user_preference=0.05
            ),
        }
        
        # æ¨™æº–åŒ–æ‰€æœ‰æ¬Šé‡
        for weights in strategies.values():
            weights.normalize()
        
        return strategies
    
    async def initialize(self) -> bool:
        """
        åˆå§‹åŒ–è·¯ç”±å™¨
        
        Returns:
            æ˜¯å¦åˆå§‹åŒ–æˆåŠŸ
        """
        try:
            self.logger.info("ğŸš€ Initializing AI Task Router...")
            
            # é ç†±æ¨¡å‹æ€§èƒ½ç·©å­˜
            if self.config.get('enable_performance_learning'):
                await self._warmup_performance_cache()
            
            self._initialized = True
            self.logger.info("âœ… AI Task Router initialized successfully")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Failed to initialize AI Task Router: {e}")
            return False
    
    async def _warmup_performance_cache(self):
        """é ç†±æ€§èƒ½ç·©å­˜"""
        try:
            if self.performance_monitor:
                # ç²å–æœ€è¿‘çš„æ€§èƒ½æ•¸æ“š
                report = await self.performance_monitor.get_performance_report(hours_back=24)
                
                # æ›´æ–°ç·©å­˜
                for provider_model, metrics in report.get('provider_performance', {}).items():
                    self.model_performance_cache[provider_model] = {
                        'avg_latency': metrics.get('avg_latency_ms', 0),
                        'success_rate': metrics.get('success_rate', 0.95),
                        'avg_quality': metrics.get('avg_quality_score', 0.8),
                        'total_requests': metrics.get('total_requests', 0),
                        'last_updated': datetime.now(timezone.utc)
                    }
            
            self.logger.info(f"âœ… Performance cache warmed up with {len(self.model_performance_cache)} models")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ Performance cache warmup failed: {e}")
    
    # ==================== ä¸»è¦è·¯ç”±æ±ºç­–æ¥å£ ====================
    
    async def make_routing_decision(
        self,
        request: RoutingDecisionRequest,
        strategy: Optional[RoutingStrategy] = None,
        custom_weights: Optional[RoutingWeights] = None
    ) -> RoutingDecisionResponse:
        """
        åŸ·è¡Œæ™ºèƒ½è·¯ç”±æ±ºç­–
        
        Args:
            request: è·¯ç”±æ±ºç­–è«‹æ±‚
            strategy: æŒ‡å®šçš„è·¯ç”±ç­–ç•¥
            custom_weights: è‡ªå®šç¾©æ¬Šé‡é…ç½®
            
        Returns:
            è·¯ç”±æ±ºç­–éŸ¿æ‡‰
            
        Raises:
            RoutingDecisionError: æ±ºç­–åŸ·è¡Œå¤±æ•—
        """
        if not self._initialized:
            raise RoutingDecisionError("Router not initialized")
        
        decision_start_time = datetime.now()
        request_id = str(uuid.uuid4())
        
        try:
            # æ›´æ–°çµ±è¨ˆ
            self.stats['total_decisions'] += 1
            
            # ç²å–ä»»å‹™å…ƒæ•¸æ“š
            task_metadata = await self.task_db.get_task_metadata(request.task_type)
            if not task_metadata:
                raise RoutingDecisionError(f"Task type '{request.task_type}' not found")
            
            # ç²å–å¯ç”¨æ¨¡å‹
            available_models = await self._get_available_models_for_task(task_metadata, request)
            if not available_models:
                raise RoutingDecisionError("No suitable models available")
            
            # ç¢ºå®šè·¯ç”±ç­–ç•¥å’Œæ¬Šé‡
            selected_strategy = strategy or self._determine_optimal_strategy(request, task_metadata)
            weights = custom_weights or self.routing_strategies.get(selected_strategy, self.routing_strategies[RoutingStrategy.BALANCED])
            
            # å‰µå»ºè·¯ç”±ä¸Šä¸‹æ–‡
            routing_context = RoutingContext(
                request_id=request_id,
                task_metadata=task_metadata,
                available_models=available_models,
                routing_request=request,
                strategy=selected_strategy,
                weights=weights,
                user_context=self._extract_user_context(request)
            )
            
            # è©•ä¼°æ‰€æœ‰å€™é¸æ¨¡å‹
            model_scores = await self._evaluate_models(routing_context)
            
            if not model_scores:
                raise RoutingDecisionError("Failed to evaluate any models")
            
            # é¸æ“‡æœ€å„ªæ¨¡å‹
            best_model_score = self._select_optimal_model(model_scores, weights)
            
            # ç”Ÿæˆæ±ºç­–éŸ¿æ‡‰
            decision_response = await self._generate_decision_response(
                best_model_score, model_scores, routing_context
            )
            
            # è¨˜éŒ„æ±ºç­–å¯©è¨ˆ
            execution_time = (datetime.now() - decision_start_time).total_seconds() * 1000
            await self._record_decision_audit(
                decision_response, model_scores, routing_context, execution_time
            )
            
            # æ›´æ–°çµ±è¨ˆ
            self.stats['successful_decisions'] += 1
            self.stats['strategy_usage'][selected_strategy.value] += 1
            self._update_performance_stats(execution_time)
            
            self.logger.info(
                f"âœ… Routing decision completed: {decision_response.selected_provider}/"
                f"{decision_response.selected_model} (confidence: {decision_response.confidence_score:.3f})"
            )
            
            return decision_response
            
        except Exception as e:
            self.stats['failed_decisions'] += 1
            self.logger.error(f"âŒ Routing decision failed: {e}")
            raise RoutingDecisionError(f"Failed to make routing decision: {e}")
    
    async def _get_available_models_for_task(
        self,
        task_metadata: TaskMetadataResponse,
        request: RoutingDecisionRequest
    ) -> List[ModelCapabilityResponse]:
        """ç²å–ä»»å‹™çš„å¯ç”¨æ¨¡å‹"""
        try:
            # æ§‹å»ºä»»å‹™éœ€æ±‚
            task_requirements = {
                'min_capability_score': task_metadata.min_model_capability_score,
                'max_cost_per_1k': request.max_acceptable_cost or task_metadata.max_acceptable_cost_per_1k,
                'max_latency_ms': request.max_acceptable_latency or task_metadata.max_acceptable_latency_ms,
                'required_features': task_metadata.required_features,
                'max_tokens': max(task_metadata.max_tokens_input, task_metadata.max_tokens_output),
            }
            
            # ç¢ºå®šéš±ç§è¦æ±‚
            privacy_level = None
            if task_metadata.requires_local_processing:
                privacy_level = "local"
            elif task_metadata.data_sensitivity_level in ["high", "confidential"]:
                privacy_level = "local" if not task_metadata.allow_cloud_fallback else None
            
            # ç²å–å¯ç”¨æ¨¡å‹
            available_models = await self.model_db.list_model_capabilities(
                privacy_level=privacy_level,
                min_capability_score=task_requirements['min_capability_score'],
                max_cost_per_1k=task_requirements['max_cost_per_1k'],
                is_available=True
            )
            
            return available_models
            
        except Exception as e:
            self.logger.error(f"âŒ Failed to get available models: {e}")
            raise ModelEvaluationError(f"Failed to get available models: {e}")
    
    def _determine_optimal_strategy(
        self,
        request: RoutingDecisionRequest,
        task_metadata: TaskMetadataResponse
    ) -> RoutingStrategy:
        """æ™ºèƒ½ç¢ºå®šæœ€å„ªè·¯ç”±ç­–ç•¥"""
        
        # ç”¨æˆ¶æ˜ç¢ºæŒ‡å®šç­–ç•¥
        user_preferences = request.user_preferences or {}
        if 'routing_strategy' in user_preferences:
            try:
                return RoutingStrategy(user_preferences['routing_strategy'])
            except ValueError:
                pass
        
        # åŸºæ–¼ä»»å‹™ç‰¹æ€§æ¨è–¦ç­–ç•¥
        if task_metadata.requires_local_processing or task_metadata.data_sensitivity_level == "high":
            return RoutingStrategy.PRIVACY_FIRST
        
        if request.requires_high_quality:
            return RoutingStrategy.QUALITY_FIRST
        
        if request.priority == "urgent":
            return RoutingStrategy.LATENCY_FIRST
        
        if request.user_tier in ["free", "basic"]:
            return RoutingStrategy.COST_OPTIMIZED
        
        # é»˜èªå¹³è¡¡ç­–ç•¥
        return self.default_strategy
    
    def _extract_user_context(self, request: RoutingDecisionRequest) -> Dict[str, Any]:
        """æå–ç”¨æˆ¶ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        return {
            'user_tier': request.user_tier,
            'priority': request.priority,
            'preferences': request.user_preferences or {},
            'historical_usage': {},  # å¯ä»¥å¾æ•¸æ“šåº«ç²å–
            'estimated_tokens': request.estimated_tokens
        }
    
    # ==================== æ¨¡å‹è©•ä¼°å’Œè©•åˆ† ====================
    
    async def _evaluate_models(self, context: RoutingContext) -> List[ModelScore]:
        """è©•ä¼°æ‰€æœ‰å€™é¸æ¨¡å‹"""
        model_scores = []
        
        for model in context.available_models:
            try:
                score = await self._evaluate_single_model(model, context)
                model_scores.append(score)
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ Failed to evaluate model {model.provider}/{model.model_id}: {e}")
                continue
        
        # æŒ‰ç¸½åˆ†æ’åº
        model_scores.sort(key=lambda x: x.total_score, reverse=True)
        
        return model_scores
    
    async def _evaluate_single_model(
        self,
        model: ModelCapabilityResponse,
        context: RoutingContext
    ) -> ModelScore:
        """è©•ä¼°å–®å€‹æ¨¡å‹"""
        factor_scores = {}
        reasoning = []
        
        # 1. æˆæœ¬è©•ä¼°
        cost_score, cost_analysis = await self._calculate_cost_score(model, context)
        factor_scores[DecisionFactor.COST] = cost_score
        reasoning.append(f"æˆæœ¬è©•åˆ†: {cost_score:.3f} ({cost_analysis.get('reasoning', '')})")
        
        # 2. å»¶é²è©•ä¼°
        latency_score, latency_analysis = await self._calculate_latency_score(model, context)
        factor_scores[DecisionFactor.LATENCY] = latency_score
        reasoning.append(f"å»¶é²è©•åˆ†: {latency_score:.3f}")
        
        # 3. å“è³ªè©•ä¼°
        quality_score = self._calculate_quality_score(model, context)
        factor_scores[DecisionFactor.QUALITY] = quality_score
        reasoning.append(f"å“è³ªè©•åˆ†: {quality_score:.3f}")
        
        # 4. å¯ç”¨æ€§è©•ä¼°
        availability_score = await self._calculate_availability_score(model, context)
        factor_scores[DecisionFactor.AVAILABILITY] = availability_score
        reasoning.append(f"å¯ç”¨æ€§è©•åˆ†: {availability_score:.3f}")
        
        # 5. éš±ç§è©•ä¼°
        privacy_score = self._calculate_privacy_score(model, context)
        factor_scores[DecisionFactor.PRIVACY] = privacy_score
        
        # 6. ç”¨æˆ¶åå¥½è©•ä¼°
        preference_score = self._calculate_user_preference_score(model, context)
        factor_scores[DecisionFactor.USER_PREFERENCE] = preference_score
        
        # è¨ˆç®—åŠ æ¬Šç¸½åˆ†
        weights = context.weights
        total_score = (
            factor_scores[DecisionFactor.COST] * weights.cost +
            factor_scores[DecisionFactor.LATENCY] * weights.latency +
            factor_scores[DecisionFactor.QUALITY] * weights.quality +
            factor_scores[DecisionFactor.AVAILABILITY] * weights.availability +
            factor_scores[DecisionFactor.PRIVACY] * weights.privacy +
            factor_scores[DecisionFactor.USER_PREFERENCE] * weights.user_preference
        )
        
        # è¨ˆç®—ä¿¡å¿ƒåº¦
        confidence = self._calculate_model_confidence(model, factor_scores, context)
        
        return ModelScore(
            model=model,
            total_score=total_score,
            factor_scores=factor_scores,
            cost_analysis=cost_analysis,
            performance_prediction=latency_analysis,
            confidence=confidence,
            reasoning=reasoning
        )
    
    async def _calculate_cost_score(
        self,
        model: ModelCapabilityResponse,
        context: RoutingContext
    ) -> Tuple[float, Dict[str, Any]]:
        """è¨ˆç®—æˆæœ¬è©•åˆ†"""
        try:
            # åŸºç¤æˆæœ¬è¨ˆç®—
            estimated_tokens = context.routing_request.estimated_tokens
            input_tokens = estimated_tokens * 0.7  # ä¼°ç®—70%ç‚ºè¼¸å…¥
            output_tokens = estimated_tokens * 0.3  # ä¼°ç®—30%ç‚ºè¼¸å‡º
            
            input_cost = (input_tokens / 1000) * model.cost_per_1k_input_tokens
            output_cost = (output_tokens / 1000) * model.cost_per_1k_output_tokens
            total_cost = input_cost + output_cost
            
            # ç²å–ä»»å‹™æœ€å¤§å¯æ¥å—æˆæœ¬
            max_cost = context.task_metadata.max_acceptable_cost_per_1k
            max_total_cost = (estimated_tokens / 1000) * max_cost
            
            # è¨ˆç®—æˆæœ¬è©•åˆ†ï¼ˆæˆæœ¬è¶Šä½åˆ†æ•¸è¶Šé«˜ï¼‰
            if total_cost == 0:  # å…è²»æ¨¡å‹
                cost_score = 1.0
            elif total_cost <= max_total_cost * 0.5:  # æˆæœ¬åœ¨é ç®—50%å…§
                cost_score = 1.0
            elif total_cost <= max_total_cost:  # æˆæœ¬åœ¨é ç®—å…§
                cost_score = 1.0 - (total_cost / max_total_cost) * 0.5
            else:  # è¶…å‡ºé ç®—
                cost_score = max(0.0, 0.5 - (total_cost - max_total_cost) / max_total_cost)
            
            # æˆæœ¬åˆ†æè©³æƒ…
            cost_analysis = {
                'total_cost': round(total_cost, 6),
                'input_cost': round(input_cost, 6),
                'output_cost': round(output_cost, 6),
                'cost_per_1k_input': model.cost_per_1k_input_tokens,
                'cost_per_1k_output': model.cost_per_1k_output_tokens,
                'estimated_input_tokens': int(input_tokens),
                'estimated_output_tokens': int(output_tokens),
                'max_acceptable_cost': round(max_total_cost, 6),
                'cost_efficiency': round(cost_score, 3),
                'is_within_budget': total_cost <= max_total_cost,
                'reasoning': f"ç¸½æˆæœ¬ ${total_cost:.4f}, é ç®— ${max_total_cost:.4f}"
            }
            
            return cost_score, cost_analysis
            
        except Exception as e:
            self.logger.error(f"âŒ Cost calculation error for {model.provider}/{model.model_id}: {e}")
            return 0.0, {'error': str(e), 'total_cost': 0.0}
    
    async def _calculate_latency_score(
        self,
        model: ModelCapabilityResponse,
        context: RoutingContext
    ) -> Tuple[float, Dict[str, Any]]:
        """è¨ˆç®—å»¶é²è©•åˆ†"""
        try:
            # ç²å–æ­·å²æ€§èƒ½æ•¸æ“š
            performance_data = await self._get_model_performance_data(model)
            
            # ä½¿ç”¨æ­·å²æ•¸æ“šæˆ–æ¨¡å‹é»˜èªå»¶é²
            if performance_data and 'avg_latency' in performance_data:
                predicted_latency = performance_data['avg_latency']
                confidence = 0.9
            else:
                predicted_latency = model.avg_latency_ms
                confidence = 0.6
            
            # åŸºæ–¼ä»»å‹™è¤‡é›œåº¦èª¿æ•´å»¶é²é æ¸¬
            token_complexity_factor = min(2.0, context.routing_request.estimated_tokens / 1000)
            adjusted_latency = predicted_latency * (1 + token_complexity_factor * 0.1)
            
            # ç²å–ä»»å‹™æœ€å¤§å¯æ¥å—å»¶é²
            max_latency = context.task_metadata.max_acceptable_latency_ms
            
            # è¨ˆç®—å»¶é²è©•åˆ†
            if adjusted_latency <= max_latency * 0.5:  # å»¶é²åœ¨é™åˆ¶50%å…§
                latency_score = 1.0
            elif adjusted_latency <= max_latency:  # å»¶é²åœ¨é™åˆ¶å…§
                latency_score = 1.0 - (adjusted_latency / max_latency) * 0.5
            else:  # è¶…å‡ºå»¶é²é™åˆ¶
                latency_score = max(0.0, 0.5 - (adjusted_latency - max_latency) / max_latency)
            
            latency_analysis = {
                'predicted_latency_ms': round(adjusted_latency, 2),
                'base_latency_ms': model.avg_latency_ms,
                'max_acceptable_latency_ms': max_latency,
                'complexity_factor': round(token_complexity_factor, 2),
                'confidence': confidence,
                'is_within_limit': adjusted_latency <= max_latency,
                'performance_data_available': performance_data is not None
            }
            
            return latency_score, latency_analysis
            
        except Exception as e:
            self.logger.error(f"âŒ Latency calculation error for {model.provider}/{model.model_id}: {e}")
            return 0.0, {'error': str(e)}
    
    def _calculate_quality_score(
        self,
        model: ModelCapabilityResponse,
        context: RoutingContext
    ) -> float:
        """è¨ˆç®—å“è³ªè©•åˆ†"""
        try:
            # åŸºç¤å“è³ªè©•åˆ†
            base_score = model.capability_score
            
            # æ ¹æ“šä»»å‹™è¦æ±‚èª¿æ•´
            min_required = context.task_metadata.min_model_capability_score
            if base_score >= min_required:
                # è¶…å‡ºæœ€ä½è¦æ±‚çš„é¡å¤–åŠ åˆ†
                quality_score = min(1.0, base_score + (base_score - min_required) * 0.1)
            else:
                # ä¸æ»¿è¶³æœ€ä½è¦æ±‚
                quality_score = base_score * 0.5
            
            # åŸºæ–¼æ­·å²å“è³ªè¡¨ç¾èª¿æ•´
            performance_data = self.model_performance_cache.get(f"{model.provider}/{model.model_id}")
            if performance_data and 'avg_quality' in performance_data:
                historical_quality = performance_data['avg_quality']
                # çµåˆæ­·å²è¡¨ç¾ï¼ˆ70%ç•¶å‰æ¨¡å‹èƒ½åŠ› + 30%æ­·å²è¡¨ç¾ï¼‰
                quality_score = quality_score * 0.7 + historical_quality * 0.3
            
            return min(1.0, max(0.0, quality_score))
            
        except Exception as e:
            self.logger.error(f"âŒ Quality calculation error: {e}")
            return 0.5
    
    async def _calculate_availability_score(
        self,
        model: ModelCapabilityResponse,
        context: RoutingContext
    ) -> float:
        """è¨ˆç®—å¯ç”¨æ€§è©•åˆ†"""
        try:
            # åŸºç¤å¯ç”¨æ€§ï¼ˆæ¨¡å‹æœ¬èº«æ˜¯å¦å¯ç”¨ï¼‰
            if not model.is_available:
                return 0.0
            
            base_availability = 0.8  # åŸºç¤å¯ç”¨æ€§è©•åˆ†
            
            # ç²å–æ­·å²æˆåŠŸç‡
            performance_data = await self._get_model_performance_data(model)
            if performance_data and 'success_rate' in performance_data:
                historical_success_rate = performance_data['success_rate']
                base_availability = historical_success_rate
            
            # è€ƒæ…®è² è¼‰å¹³è¡¡
            if self.config.get('enable_load_balancing'):
                load_factor = await self._calculate_load_factor(model)
                availability_score = base_availability * load_factor
            else:
                availability_score = base_availability
            
            return min(1.0, max(0.0, availability_score))
            
        except Exception as e:
            self.logger.error(f"âŒ Availability calculation error: {e}")
            return 0.5
    
    def _calculate_privacy_score(
        self,
        model: ModelCapabilityResponse,
        context: RoutingContext
    ) -> float:
        """è¨ˆç®—éš±ç§è©•åˆ†"""
        try:
            task_sensitivity = context.task_metadata.data_sensitivity_level
            model_privacy = model.privacy_level
            requires_local = context.task_metadata.requires_local_processing
            
            # éš±ç§åŒ¹é…åº¦è©•åˆ†
            if requires_local:
                return 1.0 if model_privacy == 'local' else 0.0
            
            privacy_scores = {
                ('low', 'cloud'): 1.0,
                ('low', 'hybrid'): 0.9,
                ('low', 'local'): 1.0,
                ('medium', 'cloud'): 0.7,
                ('medium', 'hybrid'): 0.9,
                ('medium', 'local'): 1.0,
                ('high', 'cloud'): 0.0 if not context.task_metadata.allow_cloud_fallback else 0.3,
                ('high', 'hybrid'): 0.6,
                ('high', 'local'): 1.0,
                ('confidential', 'cloud'): 0.0,
                ('confidential', 'hybrid'): 0.0,
                ('confidential', 'local'): 1.0
            }
            
            return privacy_scores.get((task_sensitivity, model_privacy), 0.5)
            
        except Exception as e:
            self.logger.error(f"âŒ Privacy calculation error: {e}")
            return 0.5
    
    def _calculate_user_preference_score(
        self,
        model: ModelCapabilityResponse,
        context: RoutingContext
    ) -> float:
        """è¨ˆç®—ç”¨æˆ¶åå¥½è©•åˆ†"""
        try:
            preferences = context.user_context.get('preferences', {})
            score = 0.5  # åŸºç¤ä¸­æ€§è©•åˆ†
            
            # æä¾›å•†åå¥½
            preferred_provider = preferences.get('preferred_provider')
            if preferred_provider:
                if model.provider == preferred_provider:
                    score += 0.3
                elif model.provider in preferences.get('avoided_providers', []):
                    score -= 0.3
            
            # æ¨¡å‹é¡å‹åå¥½
            preferred_model_type = preferences.get('preferred_model_type')
            if preferred_model_type and preferred_model_type.lower() in model.model_name.lower():
                score += 0.2
            
            # æ­·å²ä½¿ç”¨åå¥½ï¼ˆå¯ä»¥åŸºæ–¼ç”¨æˆ¶æ­·å²ä½¿ç”¨æ•¸æ“šï¼‰
            historical_preference = preferences.get('historical_preference', {})
            model_key = f"{model.provider}/{model.model_id}"
            if model_key in historical_preference:
                historical_score = historical_preference[model_key]
                score = score * 0.7 + historical_score * 0.3
            
            return min(1.0, max(0.0, score))
            
        except Exception as e:
            self.logger.error(f"âŒ User preference calculation error: {e}")
            return 0.5
    
    def _calculate_model_confidence(
        self,
        model: ModelCapabilityResponse,
        factor_scores: Dict[DecisionFactor, float],
        context: RoutingContext
    ) -> float:
        """è¨ˆç®—æ¨¡å‹é¸æ“‡ä¿¡å¿ƒåº¦"""
        try:
            # åŸºæ–¼å„å› å­è©•åˆ†çš„ç©©å®šæ€§è¨ˆç®—ä¿¡å¿ƒåº¦
            scores = list(factor_scores.values())
            
            if not scores:
                return 0.5
            
            # è©•åˆ†çš„å¹³å‡å€¼å’Œæ¨™æº–å·®
            avg_score = statistics.mean(scores)
            if len(scores) > 1:
                score_variance = statistics.variance(scores)
                stability = 1.0 / (1.0 + score_variance)
            else:
                stability = 0.8
            
            # åŸºç¤ä¿¡å¿ƒåº¦
            base_confidence = avg_score * stability
            
            # æ ¹æ“šæ­·å²è¡¨ç¾èª¿æ•´ä¿¡å¿ƒåº¦
            performance_data = self.model_performance_cache.get(f"{model.provider}/{model.model_id}")
            if performance_data:
                requests_count = performance_data.get('total_requests', 0)
                # æ›´å¤šæ­·å²æ•¸æ“š = æ›´é«˜ä¿¡å¿ƒ
                experience_factor = min(1.0, requests_count / 100)
                base_confidence = base_confidence * 0.8 + experience_factor * 0.2
            
            return min(1.0, max(0.0, base_confidence))
            
        except Exception as e:
            self.logger.error(f"âŒ Confidence calculation error: {e}")
            return 0.5
    
    def _select_optimal_model(
        self,
        model_scores: List[ModelScore],
        weights: RoutingWeights
    ) -> ModelScore:
        """é¸æ“‡æœ€å„ªæ¨¡å‹"""
        if not model_scores:
            raise RoutingDecisionError("No model scores available for selection")
        
        # éæ¿¾æ‰ä¿¡å¿ƒåº¦éä½çš„æ¨¡å‹
        confidence_threshold = self.config.get('decision_confidence_threshold', 0.7)
        qualified_models = [
            score for score in model_scores 
            if score.confidence >= confidence_threshold
        ]
        
        # å¦‚æœæ²’æœ‰æ»¿è¶³ä¿¡å¿ƒåº¦çš„æ¨¡å‹ï¼Œä½¿ç”¨æœ€é«˜åˆ†çš„æ¨¡å‹
        if not qualified_models:
            self.logger.warning("âš ï¸ No models meet confidence threshold, using best available")
            qualified_models = model_scores[:1]
        
        # é¸æ“‡æœ€é«˜ç¸½åˆ†çš„æ¨¡å‹
        best_model = max(qualified_models, key=lambda x: x.total_score)
        
        return best_model
    
    # ==================== æ±ºç­–éŸ¿æ‡‰å’Œå¯©è¨ˆ ====================
    
    async def _generate_decision_response(
        self,
        best_model_score: ModelScore,
        all_model_scores: List[ModelScore],
        context: RoutingContext
    ) -> RoutingDecisionResponse:
        """ç”Ÿæˆè·¯ç”±æ±ºç­–éŸ¿æ‡‰"""
        try:
            selected_model = best_model_score.model
            
            # ç”Ÿæˆæ±ºç­–æ¨ç†
            decision_reasoning = self._generate_detailed_reasoning(
                best_model_score, all_model_scores, context
            )
            
            # è¨ˆç®—æœŸæœ›æˆæœ¬
            expected_cost = best_model_score.cost_analysis.get('total_cost', 0.0)
            
            # è¨ˆç®—æœŸæœ›å»¶é²
            expected_latency = best_model_score.performance_prediction.get('predicted_latency_ms', 0.0)
            
            # ç²å–å¾Œå‚™é¸é …
            fallback_options = self._generate_fallback_options(all_model_scores, best_model_score)
            
            # æ±ºç­–å…ƒæ•¸æ“š
            decision_metadata = {
                'routing_strategy': context.strategy.value,
                'weights_used': asdict(context.weights),
                'total_candidates_evaluated': len(all_model_scores),
                'decision_factors': {
                    factor.value: score 
                    for factor, score in best_model_score.factor_scores.items()
                },
                'cost_analysis': best_model_score.cost_analysis,
                'performance_prediction': best_model_score.performance_prediction,
                'model_reasoning': best_model_score.reasoning,
                'decision_timestamp': context.timestamp.isoformat(),
                'request_id': context.request_id
            }
            
            return RoutingDecisionResponse(
                selected_provider=selected_model.provider,
                selected_model=selected_model.model_id,
                reasoning=decision_reasoning,
                expected_cost=expected_cost,
                expected_latency_ms=int(expected_latency),
                expected_quality_score=selected_model.capability_score,
                confidence_score=best_model_score.confidence,
                fallback_options=fallback_options,
                decision_metadata=decision_metadata
            )
            
        except Exception as e:
            self.logger.error(f"âŒ Failed to generate decision response: {e}")
            raise RoutingDecisionError(f"Failed to generate decision response: {e}")
    
    def _generate_detailed_reasoning(
        self,
        best_model_score: ModelScore,
        all_model_scores: List[ModelScore],
        context: RoutingContext
    ) -> str:
        """ç”Ÿæˆè©³ç´°æ±ºç­–æ¨ç†"""
        reasoning_parts = []
        
        # ç­–ç•¥èªªæ˜
        strategy_desc = {
            RoutingStrategy.COST_OPTIMIZED: "æˆæœ¬å„ªåŒ–ç­–ç•¥ï¼šå„ªå…ˆé¸æ“‡æˆæœ¬æœ€ä½çš„æ¨¡å‹",
            RoutingStrategy.PERFORMANCE_OPTIMIZED: "æ€§èƒ½å„ªåŒ–ç­–ç•¥ï¼šå„ªå…ˆé¸æ“‡å»¶é²æœ€ä½ã€å“è³ªæœ€é«˜çš„æ¨¡å‹",
            RoutingStrategy.BALANCED: "å¹³è¡¡ç­–ç•¥ï¼šåœ¨æˆæœ¬ã€æ€§èƒ½ã€å“è³ªé–“å°‹æ±‚æœ€ä½³å¹³è¡¡",
            RoutingStrategy.QUALITY_FIRST: "å“è³ªå„ªå…ˆç­–ç•¥ï¼šå„ªå…ˆé¸æ“‡èƒ½åŠ›è©•åˆ†æœ€é«˜çš„æ¨¡å‹",
            RoutingStrategy.LATENCY_FIRST: "å»¶é²å„ªå…ˆç­–ç•¥ï¼šå„ªå…ˆé¸æ“‡éŸ¿æ‡‰æœ€å¿«çš„æ¨¡å‹",
            RoutingStrategy.PRIVACY_FIRST: "éšç§å„ªå…ˆç­–ç•¥ï¼šå„ªå…ˆé¸æ“‡æœ¬åœ°æˆ–é«˜éš±ç§æ¨¡å‹"
        }
        
        reasoning_parts.append(
            strategy_desc.get(context.strategy, f"ä½¿ç”¨ç­–ç•¥: {context.strategy.value}")
        )
        
        # æ¨¡å‹é¸æ“‡åŸå› 
        selected_model = best_model_score.model
        reasoning_parts.append(
            f"é¸æ“‡æ¨¡å‹ {selected_model.provider}/{selected_model.model_id} "
            f"(ç¸½åˆ†: {best_model_score.total_score:.3f}, ä¿¡å¿ƒåº¦: {best_model_score.confidence:.3f})"
        )
        
        # é—œéµæ±ºç­–å› å­
        top_factors = sorted(
            best_model_score.factor_scores.items(),
            key=lambda x: x[1] * getattr(context.weights, x[0].value.lower()),
            reverse=True
        )[:3]
        
        reasoning_parts.append("ä¸»è¦æ±ºç­–å› å­:")
        for factor, score in top_factors:
            weight = getattr(context.weights, factor.value.lower())
            weighted_score = score * weight
            reasoning_parts.append(f"â€¢ {factor.value}: è©•åˆ† {score:.3f} (æ¬Šé‡ {weight:.2f}, åŠ æ¬Šåˆ† {weighted_score:.3f})")
        
        # èˆ‡å…¶ä»–å€™é¸æ¨¡å‹æ¯”è¼ƒ
        if len(all_model_scores) > 1:
            second_best = all_model_scores[1] if len(all_model_scores) > 1 else None
            if second_best:
                score_diff = best_model_score.total_score - second_best.total_score
                reasoning_parts.append(
                    f"ç›¸æ¯”æ¬¡å„ªé¸é … {second_best.model.provider}/{second_best.model.model_id}, "
                    f"é ˜å…ˆ {score_diff:.3f} åˆ†"
                )
        
        return "; ".join(reasoning_parts)
    
    def _generate_fallback_options(
        self,
        all_model_scores: List[ModelScore],
        selected_model_score: ModelScore
    ) -> List[Dict[str, str]]:
        """ç”Ÿæˆå¾Œå‚™é¸é …"""
        fallback_options = []
        
        # æ’é™¤å·²é¸æ“‡çš„æ¨¡å‹ï¼Œé¸æ“‡å‰3å€‹ä½œç‚ºå¾Œå‚™
        other_models = [
            score for score in all_model_scores 
            if score.model.id != selected_model_score.model.id
        ]
        
        for model_score in other_models[:3]:
            model = model_score.model
            reason_parts = []
            
            # ç”Ÿæˆå¾Œå‚™ç†ç”±
            if model_score.factor_scores.get(DecisionFactor.COST, 0) > 0.8:
                reason_parts.append("ä½æˆæœ¬")
            if model_score.factor_scores.get(DecisionFactor.LATENCY, 0) > 0.8:
                reason_parts.append("ä½å»¶é²")
            if model_score.factor_scores.get(DecisionFactor.QUALITY, 0) > 0.8:
                reason_parts.append("é«˜å“è³ª")
            if model.privacy_level == 'local':
                reason_parts.append("æœ¬åœ°éƒ¨ç½²")
            
            reason = "ã€".join(reason_parts) if reason_parts else f"ç¶œåˆè©•åˆ† {model_score.total_score:.3f}"
            
            fallback_options.append({
                'provider': model.provider,
                'model': model.model_id,
                'reason': f"å¾Œå‚™é¸é … - {reason}"
            })
        
        return fallback_options
    
    async def _record_decision_audit(
        self,
        decision_response: RoutingDecisionResponse,
        all_model_scores: List[ModelScore],
        context: RoutingContext,
        execution_time_ms: float
    ):
        """è¨˜éŒ„æ±ºç­–å¯©è¨ˆ"""
        try:
            if not self.config.get('enable_audit_logging', True):
                return
            
            # æ‰¾åˆ°é¸ä¸­çš„æ¨¡å‹è©•åˆ†
            selected_model_score = None
            for score in all_model_scores:
                if (score.model.provider == decision_response.selected_provider and 
                    score.model.model_id == decision_response.selected_model):
                    selected_model_score = score
                    break
            
            if not selected_model_score:
                self.logger.error("âŒ Could not find selected model score for audit")
                return
            
            # ç”Ÿæˆå¾Œå‚™éˆ
            fallback_chain = [
                f"{option['provider']}/{option['model']}"
                for option in decision_response.fallback_options
            ]
            
            # å‰µå»ºå¯©è¨ˆè¨˜éŒ„
            audit_record = DecisionAudit(
                decision_id=str(uuid.uuid4()),
                request_id=context.request_id,
                task_type=context.task_metadata.task_type,
                selected_model=selected_model_score.model,
                all_model_scores=all_model_scores,
                routing_context=context,
                decision_reasoning=decision_response.reasoning.split("; "),
                confidence_score=decision_response.confidence_score,
                execution_time_ms=execution_time_ms,
                fallback_chain=fallback_chain
            )
            
            # å­˜å„²å¯©è¨ˆè¨˜éŒ„
            self.decision_history.append(audit_record)
            
            # ç¶­è­·æ­·å²è¨˜éŒ„å¤§å°
            if len(self.decision_history) > self.max_history_size:
                self.decision_history = self.decision_history[-self.max_history_size:]
            
            # è©³ç´°å¯©è¨ˆæ—¥èªŒ
            if self.config.get('audit_detail_level') == 'full':
                self.logger.info(f"ğŸ“ Decision audit recorded: {audit_record.decision_id}")
                if self.logger.isEnabledFor(logging.DEBUG):
                    audit_dict = audit_record.to_dict()
                    self.logger.debug(f"ğŸ“‹ Full audit details: {json.dumps(audit_dict, indent=2, ensure_ascii=False)}")
            
        except Exception as e:
            self.logger.error(f"âŒ Failed to record decision audit: {e}")
    
    # ==================== è¼”åŠ©å·¥å…·æ–¹æ³• ====================
    
    async def _get_model_performance_data(
        self,
        model: ModelCapabilityResponse
    ) -> Optional[Dict[str, Any]]:
        """ç²å–æ¨¡å‹æ­·å²æ€§èƒ½æ•¸æ“š"""
        model_key = f"{model.provider}/{model.model_id}"
        
        # æª¢æŸ¥ç·©å­˜
        cached_data = self.model_performance_cache.get(model_key)
        if cached_data:
            last_updated = cached_data.get('last_updated')
            if last_updated and (datetime.now(timezone.utc) - last_updated).seconds < self.cache_ttl_seconds:
                return cached_data
        
        # å¾æ€§èƒ½ç›£æ§å™¨ç²å–æ•¸æ“š
        if self.performance_monitor:
            try:
                report = await self.performance_monitor.get_performance_report(
                    provider=model.provider,
                    model_id=model.model_id,
                    hours_back=24
                )
                
                if report and 'provider_performance' in report:
                    performance_data = report['provider_performance'].get(model_key, {})
                    if performance_data:
                        # æ›´æ–°ç·©å­˜
                        performance_data['last_updated'] = datetime.now(timezone.utc)
                        self.model_performance_cache[model_key] = performance_data
                        return performance_data
                        
            except Exception as e:
                self.logger.debug(f"Could not get performance data for {model_key}: {e}")
        
        return None
    
    async def _calculate_load_factor(self, model: ModelCapabilityResponse) -> float:
        """è¨ˆç®—æ¨¡å‹è² è¼‰å› å­"""
        try:
            # ç°¡åŒ–çš„è² è¼‰è¨ˆç®— - å¯ä»¥åŸºæ–¼å¯¦éš›è«‹æ±‚è¨ˆæ•¸ã€éšŠåˆ—é•·åº¦ç­‰
            model_key = f"{model.provider}/{model.model_id}"
            
            # æª¢æŸ¥æœ€è¿‘çš„ä½¿ç”¨é »ç‡
            recent_selections = sum(
                1 for audit in self.decision_history[-50:]  # æœ€è¿‘50å€‹æ±ºç­–
                if f"{audit.selected_model.provider}/{audit.selected_model.model_id}" == model_key
            )
            
            # è² è¼‰å› å­ï¼šä½¿ç”¨é »ç‡è¶Šé«˜ï¼Œå› å­è¶Šä½
            if recent_selections == 0:
                return 1.0  # æœªä½¿ç”¨ï¼Œæœ€é«˜å„ªå…ˆ
            elif recent_selections <= 5:
                return 0.9
            elif recent_selections <= 15:
                return 0.8
            else:
                return 0.6  # é«˜è² è¼‰ï¼Œé™ä½å„ªå…ˆç´š
                
        except Exception as e:
            self.logger.debug(f"Load factor calculation error: {e}")
            return 0.8  # é è¨­ä¸­ç­‰è² è¼‰å› å­
    
    def _update_performance_stats(self, execution_time_ms: float):
        """æ›´æ–°æ€§èƒ½çµ±è¨ˆ"""
        # æ›´æ–°å¹³å‡æ±ºç­–æ™‚é–“
        current_avg = self.stats['average_decision_time_ms']
        total_decisions = self.stats['total_decisions']
        
        if total_decisions > 1:
            self.stats['average_decision_time_ms'] = (
                (current_avg * (total_decisions - 1) + execution_time_ms) / total_decisions
            )
        else:
            self.stats['average_decision_time_ms'] = execution_time_ms
    
    # ==================== é…ç½®å’Œç®¡ç†æ¥å£ ====================
    
    def update_strategy_weights(
        self,
        strategy: RoutingStrategy,
        weights: RoutingWeights
    ) -> bool:
        """å‹•æ…‹æ›´æ–°è·¯ç”±ç­–ç•¥æ¬Šé‡"""
        try:
            weights.normalize()
            self.routing_strategies[strategy] = weights
            self.logger.info(f"âœ… Updated weights for strategy {strategy.value}")
            return True
        except Exception as e:
            self.logger.error(f"âŒ Failed to update strategy weights: {e}")
            return False
    
    def add_custom_strategy(
        self,
        name: str,
        weights: RoutingWeights
    ) -> bool:
        """æ·»åŠ è‡ªå®šç¾©è·¯ç”±ç­–ç•¥"""
        try:
            weights.normalize()
            custom_strategy = RoutingStrategy(name)  # é€™æœƒå‰µå»ºå‹•æ…‹æšèˆ‰å€¼
            self.routing_strategies[custom_strategy] = weights
            self.stats['strategy_usage'][name] = 0
            self.logger.info(f"âœ… Added custom strategy: {name}")
            return True
        except Exception as e:
            self.logger.error(f"âŒ Failed to add custom strategy: {e}")
            return False
    
    def get_decision_history(
        self,
        limit: int = 50,
        task_type: Optional[str] = None,
        provider: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """ç²å–æ±ºç­–æ­·å²"""
        try:
            # éæ¿¾æ¢ä»¶
            filtered_history = self.decision_history
            
            if task_type:
                filtered_history = [
                    audit for audit in filtered_history 
                    if audit.task_type == task_type
                ]
            
            if provider:
                filtered_history = [
                    audit for audit in filtered_history 
                    if audit.selected_model.provider == provider
                ]
            
            # é™åˆ¶æ•¸é‡ä¸¦è½‰æ›ç‚ºå­—å…¸
            recent_history = filtered_history[-limit:] if filtered_history else []
            return [audit.to_dict() for audit in recent_history]
            
        except Exception as e:
            self.logger.error(f"âŒ Failed to get decision history: {e}")
            return []
    
    def get_routing_statistics(self) -> Dict[str, Any]:
        """ç²å–è·¯ç”±çµ±è¨ˆä¿¡æ¯"""
        try:
            # è¨ˆç®—æ¨¡å‹é¸æ“‡é »ç‡
            model_frequency = {}
            for audit in self.decision_history:
                model_key = f"{audit.selected_model.provider}/{audit.selected_model.model_id}"
                model_frequency[model_key] = model_frequency.get(model_key, 0) + 1
            
            # è¨ˆç®—å¹³å‡ä¿¡å¿ƒåº¦
            confidence_scores = [audit.confidence_score for audit in self.decision_history]
            avg_confidence = statistics.mean(confidence_scores) if confidence_scores else 0.0
            
            stats = self.stats.copy()
            stats.update({
                'model_selection_frequency': model_frequency,
                'average_confidence_score': round(avg_confidence, 3),
                'total_audit_records': len(self.decision_history),
                'strategy_distribution': {
                    strategy: count for strategy, count in self.stats['strategy_usage'].items()
                    if count > 0
                },
                'performance_cache_size': len(self.model_performance_cache),
                'uptime_seconds': (datetime.now(timezone.utc) - self.stats['last_reset']).total_seconds()
            })
            
            return stats
            
        except Exception as e:
            self.logger.error(f"âŒ Failed to get routing statistics: {e}")
            return self.stats.copy()
    
    def reset_statistics(self):
        """é‡ç½®çµ±è¨ˆä¿¡æ¯"""
        self.stats = {
            'total_decisions': 0,
            'successful_decisions': 0,
            'failed_decisions': 0,
            'average_decision_time_ms': 0.0,
            'strategy_usage': {strategy.value: 0 for strategy in RoutingStrategy},
            'model_selection_frequency': {},
            'cost_savings': 0.0,
            'last_reset': datetime.now(timezone.utc)
        }
        self.decision_history.clear()
        self.logger.info("âœ… Routing statistics reset")
    
    async def health_check(self) -> Dict[str, Any]:
        """è·¯ç”±å™¨å¥åº·æª¢æŸ¥"""
        health_status = {
            'router_initialized': self._initialized,
            'timestamp': datetime.now(timezone.utc).isoformat(),
            'statistics': self.get_routing_statistics(),
            'configuration': {
                'strategies_available': len(self.routing_strategies),
                'default_strategy': self.default_strategy.value,
                'audit_logging_enabled': self.config.get('enable_audit_logging', True),
                'performance_learning_enabled': self.config.get('enable_performance_learning', True),
                'load_balancing_enabled': self.config.get('enable_load_balancing', True)
            },
            'components_status': {}
        }
        
        try:
            # æª¢æŸ¥ä»»å‹™æ•¸æ“šåº«
            task_metadata = await self.task_db.list_task_metadata(limit=1)
            health_status['components_status']['task_db'] = {
                'status': 'healthy',
                'task_types_available': len(task_metadata)
            }
        except Exception as e:
            health_status['components_status']['task_db'] = {
                'status': 'unhealthy',
                'error': str(e)
            }
        
        try:
            # æª¢æŸ¥æ¨¡å‹æ•¸æ“šåº«
            models = await self.model_db.list_model_capabilities(limit=1)
            health_status['components_status']['model_db'] = {
                'status': 'healthy',
                'models_available': len(models)
            }
        except Exception as e:
            health_status['components_status']['model_db'] = {
                'status': 'unhealthy',
                'error': str(e)
            }
        
        # æª¢æŸ¥æ€§èƒ½ç›£æ§å™¨
        if self.performance_monitor:
            health_status['components_status']['performance_monitor'] = {
                'status': 'available',
                'cache_size': len(self.model_performance_cache)
            }
        else:
            health_status['components_status']['performance_monitor'] = {
                'status': 'not_configured'
            }
        
        # æ•´é«”å¥åº·ç‹€æ…‹
        component_statuses = [
            comp.get('status', 'unknown') 
            for comp in health_status['components_status'].values()
        ]
        
        if all(status in ['healthy', 'available', 'not_configured'] for status in component_statuses):
            health_status['overall_status'] = 'healthy'
        else:
            health_status['overall_status'] = 'degraded'
        
        return health_status