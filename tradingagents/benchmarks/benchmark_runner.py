#!/usr/bin/env python3
"""
Benchmark Runner
Âü∫Ê∫ñÊ∏¨Ë©¶Âü∑Ë°åÂô® - GPT-OSSÊï¥Âêà‰ªªÂãô1.2.2
"""

import asyncio
import logging
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime, timezone

from .benchmark_framework import BenchmarkFramework, BenchmarkError
from .benchmark_tasks import StandardBenchmarkSuite
from ..database.model_capability_db import ModelCapabilityDB, ModelCapabilityDBError
from ..utils.llm_client import LLMClient

logger = logging.getLogger(__name__)

class BenchmarkRunner:
    """
    Âü∫Ê∫ñÊ∏¨Ë©¶Âü∑Ë°åÂô®
    
    ÂäüËÉΩÔºö
    1. Áµ±‰∏ÄÁÆ°ÁêÜÂü∫Ê∫ñÊ∏¨Ë©¶Âü∑Ë°å
    2. ËàáÊ®°ÂûãËÉΩÂäõÊï∏ÊìöÂ∫´ÈõÜÊàê
    3. ÊîØÊåÅÊâπÈáèÊ∏¨Ë©¶ÂíåË™øÂ∫¶
    4. Êèê‰æõÊ∏¨Ë©¶ÁµêÊûúÂàÜÊûê
    """
    
    def __init__(
        self,
        llm_client: Optional[LLMClient] = None,
        model_db: Optional[ModelCapabilityDB] = None
    ):
        """
        ÂàùÂßãÂåñÂü∫Ê∫ñÊ∏¨Ë©¶Âü∑Ë°åÂô®
        
        Args:
            llm_client: LLMÂÆ¢Êà∂Á´ØÂØ¶‰æã
            model_db: Ê®°ÂûãËÉΩÂäõÊï∏ÊìöÂ∫´ÂØ¶‰æã
        """
        self.llm_client = llm_client
        self.model_db = model_db or ModelCapabilityDB()
        self.framework = BenchmarkFramework()
        self.logger = logger
        
        # Ë®ªÂÜäÊ®ôÊ∫ñÊ∏¨Ë©¶Â•ó‰ª∂
        self._register_standard_suites()
    
    def _register_standard_suites(self):
        """Ë®ªÂÜäÊ®ôÊ∫ñÊ∏¨Ë©¶Â•ó‰ª∂"""
        try:
            standard_suite = StandardBenchmarkSuite()
            self.framework.register_suite(standard_suite)
            self.logger.info("‚úÖ Standard benchmark suite registered")
        except Exception as e:
            self.logger.error(f"‚ùå Failed to register standard suite: {e}")
    
    async def run_model_benchmark(
        self,
        provider: str,
        model_id: str,
        suite_name: str = "standard",
        llm_client: Optional[LLMClient] = None,
        update_database: bool = True,
        parallel: bool = False,
        max_concurrent: int = 3
    ) -> Dict[str, Any]:
        """
        Âü∑Ë°åÂñÆÂÄãÊ®°ÂûãÁöÑÂü∫Ê∫ñÊ∏¨Ë©¶
        
        Args:
            provider: Êèê‰æõÂïÜÂêçÁ®±
            model_id: Ê®°ÂûãÊ®ôË≠òÁ¨¶
            suite_name: Ê∏¨Ë©¶Â•ó‰ª∂ÂêçÁ®±
            llm_client: LLMÂÆ¢Êà∂Á´ØÔºàÂèØÈÅ∏Ôºå‰ΩøÁî®ÂØ¶‰æãËÆäÈáèÔºâ
            update_database: ÊòØÂê¶Êõ¥Êñ∞Êï∏ÊìöÂ∫´
            parallel: ÊòØÂê¶‰∏¶Ë°åÂü∑Ë°å
            max_concurrent: ÊúÄÂ§ß‰∏¶ÁôºÊï∏
            
        Returns:
            Âü∫Ê∫ñÊ∏¨Ë©¶ÁµêÊûú
            
        Raises:
            BenchmarkError: Ê∏¨Ë©¶Âü∑Ë°åÂ§±Êïó
        """
        client = llm_client or self.llm_client
        if not client:
            raise BenchmarkError("No LLM client available for benchmarking")
        
        self.logger.info(f"üöÄ Starting benchmark for {provider}/{model_id} with suite '{suite_name}'")
        
        try:
            # Âü∑Ë°åÂü∫Ê∫ñÊ∏¨Ë©¶
            result = await self.framework.run_suite(
                suite_name=suite_name,
                llm_client=client,
                provider=provider,
                model_id=model_id,
                parallel=parallel,
                max_concurrent=max_concurrent
            )
            
            # ËôïÁêÜÊ∏¨Ë©¶ÁµêÊûú
            processed_result = self._process_benchmark_result(result)
            
            # Êõ¥Êñ∞Êï∏ÊìöÂ∫´
            if update_database:
                await self._update_model_database(provider, model_id, processed_result)
            
            self.logger.info(
                f"‚úÖ Benchmark completed for {provider}/{model_id}: "
                f"score={processed_result['overall_score']:.3f}"
            )
            
            return processed_result
            
        except Exception as e:
            self.logger.error(f"‚ùå Benchmark failed for {provider}/{model_id}: {e}")
            raise BenchmarkError(f"Benchmark execution failed: {e}")
    
    async def run_batch_benchmarks(
        self,
        models: List[Tuple[str, str]],
        suite_name: str = "standard",
        llm_client: Optional[LLMClient] = None,
        update_database: bool = True,
        parallel_models: bool = False,
        parallel_tasks: bool = False,
        max_concurrent_models: int = 2,
        max_concurrent_tasks: int = 3
    ) -> Dict[str, Dict[str, Any]]:
        """
        ÊâπÈáèÂü∑Ë°åÂ§öÂÄãÊ®°ÂûãÁöÑÂü∫Ê∫ñÊ∏¨Ë©¶
        
        Args:
            models: Ê®°ÂûãÂàóË°® [(provider, model_id), ...]
            suite_name: Ê∏¨Ë©¶Â•ó‰ª∂ÂêçÁ®±
            llm_client: LLMÂÆ¢Êà∂Á´Ø
            update_database: ÊòØÂê¶Êõ¥Êñ∞Êï∏ÊìöÂ∫´
            parallel_models: ÊòØÂê¶‰∏¶Ë°åÊ∏¨Ë©¶Â§öÂÄãÊ®°Âûã
            parallel_tasks: ÊòØÂê¶‰∏¶Ë°åÂü∑Ë°åÊ∏¨Ë©¶‰ªªÂãô
            max_concurrent_models: ÊúÄÂ§ß‰∏¶ÁôºÊ®°ÂûãÊï∏
            max_concurrent_tasks: ÊúÄÂ§ß‰∏¶Áôº‰ªªÂãôÊï∏
            
        Returns:
            ÊâÄÊúâÊ®°ÂûãÁöÑÊ∏¨Ë©¶ÁµêÊûúÂ≠óÂÖ∏
        """
        if not models:
            self.logger.warning("No models specified for batch benchmarking")
            return {}
        
        self.logger.info(f"üöÄ Starting batch benchmark for {len(models)} models")
        
        client = llm_client or self.llm_client
        if not client:
            raise BenchmarkError("No LLM client available for benchmarking")
        
        results = {}
        
        if parallel_models:
            # ‰∏¶Ë°åÊ∏¨Ë©¶Â§öÂÄãÊ®°Âûã
            semaphore = asyncio.Semaphore(max_concurrent_models)
            
            async def run_single_model(provider: str, model_id: str) -> Tuple[str, Dict[str, Any]]:
                async with semaphore:
                    try:
                        result = await self.run_model_benchmark(
                            provider=provider,
                            model_id=model_id,
                            suite_name=suite_name,
                            llm_client=client,
                            update_database=update_database,
                            parallel=parallel_tasks,
                            max_concurrent=max_concurrent_tasks
                        )
                        return f"{provider}/{model_id}", result
                    except Exception as e:
                        self.logger.error(f"‚ùå Failed to benchmark {provider}/{model_id}: {e}")
                        return f"{provider}/{model_id}", {"error": str(e), "success": False}
            
            # Âü∑Ë°åÊâÄÊúâÊ®°ÂûãÊ∏¨Ë©¶
            tasks = [run_single_model(provider, model_id) for provider, model_id in models]
            completed_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # ËôïÁêÜÁµêÊûú
            for result in completed_results:
                if isinstance(result, tuple):
                    model_key, model_result = result
                    results[model_key] = model_result
                else:
                    self.logger.error(f"‚ùå Unexpected result type: {result}")
        
        else:
            # È†ÜÂ∫èÊ∏¨Ë©¶Ê®°Âûã
            for provider, model_id in models:
                model_key = f"{provider}/{model_id}"
                
                try:
                    result = await self.run_model_benchmark(
                        provider=provider,
                        model_id=model_id,
                        suite_name=suite_name,
                        llm_client=client,
                        update_database=update_database,
                        parallel=parallel_tasks,
                        max_concurrent=max_concurrent_tasks
                    )
                    results[model_key] = result
                    
                except Exception as e:
                    self.logger.error(f"‚ùå Failed to benchmark {provider}/{model_id}: {e}")
                    results[model_key] = {"error": str(e), "success": False}
        
        # ÁîüÊàêÊâπÈáèÊ∏¨Ë©¶Á∏ΩÁµê
        summary = self._generate_batch_summary(results)
        
        self.logger.info(
            f"‚úÖ Batch benchmark completed: "
            f"{summary['successful_models']}/{summary['total_models']} models successful"
        )
        
        return {
            "summary": summary,
            "results": results,
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
    
    async def run_all_registered_models(
        self,
        suite_name: str = "standard",
        llm_client: Optional[LLMClient] = None,
        update_database: bool = True,
        parallel_models: bool = False,
        parallel_tasks: bool = False,
        provider_filter: Optional[str] = None,
        privacy_level_filter: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Â∞çÊâÄÊúâÂ∑≤Ë®ªÂÜäÁöÑÊ®°ÂûãÂü∑Ë°åÂü∫Ê∫ñÊ∏¨Ë©¶
        
        Args:
            suite_name: Ê∏¨Ë©¶Â•ó‰ª∂ÂêçÁ®±
            llm_client: LLMÂÆ¢Êà∂Á´Ø
            update_database: ÊòØÂê¶Êõ¥Êñ∞Êï∏ÊìöÂ∫´
            parallel_models: ÊòØÂê¶‰∏¶Ë°åÊ∏¨Ë©¶Ê®°Âûã
            parallel_tasks: ÊòØÂê¶‰∏¶Ë°åÂü∑Ë°å‰ªªÂãô
            provider_filter: Êèê‰æõÂïÜÈÅéÊøæ
            privacy_level_filter: Èö±ÁßÅÁ¥öÂà•ÈÅéÊøæ
            
        Returns:
            ÊâÄÊúâÊ®°ÂûãÁöÑÊ∏¨Ë©¶ÁµêÊûú
        """
        try:
            # Áç≤ÂèñÊâÄÊúâÂèØÁî®Ê®°Âûã
            models = await self.model_db.list_model_capabilities(
                provider=provider_filter,
                privacy_level=privacy_level_filter,
                is_available=True
            )
            
            if not models:
                self.logger.warning("No registered models found for benchmarking")
                return {"error": "No models available", "results": {}}
            
            self.logger.info(f"Found {len(models)} registered models for benchmarking")
            
            # ËΩâÊèõÁÇ∫ÊâπÈáèÊ∏¨Ë©¶Ê†ºÂºè
            model_list = [(model.provider, model.model_id) for model in models]
            
            # Âü∑Ë°åÊâπÈáèÊ∏¨Ë©¶
            return await self.run_batch_benchmarks(
                models=model_list,
                suite_name=suite_name,
                llm_client=llm_client,
                update_database=update_database,
                parallel_models=parallel_models,
                parallel_tasks=parallel_tasks
            )
            
        except ModelCapabilityDBError as e:
            self.logger.error(f"‚ùå Database error during model benchmarking: {e}")
            raise BenchmarkError(f"Database error: {e}")
        except Exception as e:
            self.logger.error(f"‚ùå Error during registered model benchmarking: {e}")
            raise BenchmarkError(f"Benchmarking error: {e}")
    
    def _process_benchmark_result(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """ËôïÁêÜÂü∫Ê∫ñÊ∏¨Ë©¶ÁµêÊûúÔºåÊèêÂèñÈóúÈçµÊåáÊ®ô"""
        processed = {
            "overall_score": result.get("overall_score", 0.0),
            "success_rate": result.get("success_rate", 0.0),
            "avg_latency_ms": result.get("avg_latency_ms", 0.0),
            "detailed_scores": result.get("detailed_scores", {}),
            "task_results": result.get("task_results", []),
            "timestamp": result.get("start_time"),
            "duration_ms": result.get("duration_ms", 0.0),
            "suite_name": result.get("suite_name"),
            "provider": result.get("provider"),
            "model_id": result.get("model_id")
        }
        
        # Á¢∫‰øùË©≥Á¥∞Ë©ïÂàÜÂ≠òÂú®
        if not processed["detailed_scores"]:
            processed["detailed_scores"] = {
                "reasoning_score": None,
                "creativity_score": None,
                "accuracy_score": None,
                "speed_score": None
            }
        
        return processed
    
    async def _update_model_database(
        self,
        provider: str,
        model_id: str,
        benchmark_result: Dict[str, Any]
    ) -> bool:
        """Êõ¥Êñ∞Ê®°ÂûãÊï∏ÊìöÂ∫´‰∏≠ÁöÑÂü∫Ê∫ñÊ∏¨Ë©¶ÁµêÊûú"""
        try:
            # ÊßãÂª∫Âü∫Ê∫ñÊ∏¨Ë©¶ÁµêÊûúÊï∏Êìö
            benchmark_scores = {
                "overall_score": benchmark_result["overall_score"],
                "reasoning_score": benchmark_result["detailed_scores"].get("reasoning_score"),
                "creativity_score": benchmark_result["detailed_scores"].get("creativity_score"),
                "accuracy_score": benchmark_result["detailed_scores"].get("accuracy_score"),
                "speed_score": benchmark_result["detailed_scores"].get("speed_score"),
                "success_rate": benchmark_result["success_rate"],
                "avg_latency_ms": benchmark_result["avg_latency_ms"],
                "suite_name": benchmark_result["suite_name"],
                "timestamp": benchmark_result["timestamp"],
                "duration_ms": benchmark_result["duration_ms"],
                "task_count": len(benchmark_result["task_results"]),
                "detailed_results": benchmark_result["task_results"]
            }
            
            # Êõ¥Êñ∞Êï∏ÊìöÂ∫´
            success = await self.model_db.update_benchmark_scores(
                provider=provider,
                model_id=model_id,
                benchmark_results=benchmark_scores
            )
            
            if success:
                self.logger.info(f"‚úÖ Updated benchmark scores in database for {provider}/{model_id}")
            else:
                self.logger.warning(f"‚ö†Ô∏è Failed to update benchmark scores for {provider}/{model_id}")
            
            return success
            
        except Exception as e:
            self.logger.error(f"‚ùå Error updating model database: {e}")
            return False
    
    def _generate_batch_summary(self, results: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
        """ÁîüÊàêÊâπÈáèÊ∏¨Ë©¶Á∏ΩÁµê"""
        total_models = len(results)
        successful_models = sum(1 for r in results.values() if r.get("success", True) and "error" not in r)
        failed_models = total_models - successful_models
        
        # Ë®àÁÆóÊàêÂäüÊ®°ÂûãÁöÑÂπ≥ÂùáÊåáÊ®ô
        successful_results = [r for r in results.values() if r.get("success", True) and "error" not in r]
        
        if successful_results:
            avg_overall_score = sum(r.get("overall_score", 0) for r in successful_results) / len(successful_results)
            avg_success_rate = sum(r.get("success_rate", 0) for r in successful_results) / len(successful_results)
            avg_latency = sum(r.get("avg_latency_ms", 0) for r in successful_results) / len(successful_results)
        else:
            avg_overall_score = 0.0
            avg_success_rate = 0.0
            avg_latency = 0.0
        
        # ÊâæÂá∫ÊúÄ‰Ω≥ÂíåÊúÄÂ∑ÆÊ®°Âûã
        best_model = None
        worst_model = None
        best_score = -1.0
        worst_score = 2.0
        
        for model_key, result in results.items():
            if result.get("success", True) and "error" not in result:
                score = result.get("overall_score", 0)
                if score > best_score:
                    best_score = score
                    best_model = model_key
                if score < worst_score:
                    worst_score = score
                    worst_model = model_key
        
        return {
            "total_models": total_models,
            "successful_models": successful_models,
            "failed_models": failed_models,
            "success_rate": successful_models / max(total_models, 1),
            "avg_overall_score": avg_overall_score,
            "avg_success_rate": avg_success_rate,
            "avg_latency_ms": avg_latency,
            "best_model": {
                "name": best_model,
                "score": best_score
            } if best_model else None,
            "worst_model": {
                "name": worst_model,
                "score": worst_score
            } if worst_model else None,
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
    
    async def get_benchmark_report(
        self,
        provider: Optional[str] = None,
        model_id: Optional[str] = None,
        suite_name: Optional[str] = None,
        days_back: int = 30
    ) -> Dict[str, Any]:
        """
        Áç≤ÂèñÂü∫Ê∫ñÊ∏¨Ë©¶Â†±Âëä
        
        Args:
            provider: Êèê‰æõÂïÜÈÅéÊøæ
            model_id: Ê®°ÂûãÈÅéÊøæ
            suite_name: Ê∏¨Ë©¶Â•ó‰ª∂ÈÅéÊøæ
            days_back: ÂõûÊ∫ØÂ§©Êï∏
            
        Returns:
            Âü∫Ê∫ñÊ∏¨Ë©¶Â†±Âëä
        """
        try:
            # ÂæûÊ°ÜÊû∂Áç≤ÂèñÊ≠∑Âè≤Êï∏Êìö
            framework_history = self.framework.get_history(
                provider=provider,
                model_id=model_id,
                suite_name=suite_name,
                limit=100
            )
            
            # ÂæûÊï∏ÊìöÂ∫´Áç≤ÂèñÊ≠∑Âè≤Êï∏Êìö
            db_history = await self.model_db.get_benchmark_history(
                provider=provider,
                model_id=model_id,
                days_back=days_back
            )
            
            # ÁîüÊàêÁµ±Ë®àÂ†±Âëä
            report = {
                "summary": {
                    "total_framework_records": len(framework_history),
                    "total_db_records": len(db_history),
                    "date_range_days": days_back,
                    "generated_at": datetime.now(timezone.utc).isoformat()
                },
                "framework_history": framework_history[:10],  # ÊúÄËøë10Ê¢ù
                "database_history": db_history[:10],  # ÊúÄËøë10Ê¢ù
                "statistics": self._calculate_report_statistics(framework_history, db_history)
            }
            
            return report
            
        except Exception as e:
            self.logger.error(f"‚ùå Error generating benchmark report: {e}")
            return {
                "error": str(e),
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
    
    def _calculate_report_statistics(
        self,
        framework_history: List[Dict[str, Any]],
        db_history: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Ë®àÁÆóÂ†±ÂëäÁµ±Ë®à‰ø°ÊÅØ"""
        stats = {
            "framework_stats": {},
            "database_stats": {}
        }
        
        # Ê°ÜÊû∂Áµ±Ë®à
        if framework_history:
            scores = [h.get("overall_score", 0) for h in framework_history if h.get("overall_score")]
            if scores:
                stats["framework_stats"] = {
                    "avg_score": sum(scores) / len(scores),
                    "max_score": max(scores),
                    "min_score": min(scores),
                    "score_trend": "improving" if len(scores) > 1 and scores[0] > scores[-1] else "stable"
                }
        
        # Êï∏ÊìöÂ∫´Áµ±Ë®à
        if db_history:
            capability_scores = [h.get("capability_score", 0) for h in db_history if h.get("capability_score")]
            if capability_scores:
                stats["database_stats"] = {
                    "avg_capability_score": sum(capability_scores) / len(capability_scores),
                    "max_capability_score": max(capability_scores),
                    "min_capability_score": min(capability_scores),
                    "models_benchmarked": len(set(f"{h.get('provider', '')}/{h.get('model_id', '')}" for h in db_history))
                }
        
        return stats
    
    async def cleanup_old_results(self, days_to_keep: int = 90):
        """Ê∏ÖÁêÜËàäÁöÑÊ∏¨Ë©¶ÁµêÊûú"""
        try:
            # Ê∏ÖÁêÜÊ°ÜÊû∂Ê≠∑Âè≤Ôºà‰øùÁïôÊúÄËøëÁöÑË®òÈåÑÔºâ
            all_history = self.framework.get_history(limit=1000)
            if len(all_history) > days_to_keep * 5:  # ÂÅáË®≠ÊØèÂ§©5Ê¨°Ê∏¨Ë©¶
                self.framework.results_history = all_history[:days_to_keep * 5]
                self.logger.info(f"‚úÖ Cleaned framework history, kept {len(self.framework.results_history)} records")
            
            self.logger.info(f"‚úÖ Cleanup completed, keeping results from last {days_to_keep} days")
            
        except Exception as e:
            self.logger.error(f"‚ùå Error during cleanup: {e}")
    
    def get_registered_suites(self) -> List[str]:
        """Áç≤ÂèñÂ∑≤Ë®ªÂÜäÁöÑÊ∏¨Ë©¶Â•ó‰ª∂ÂàóË°®"""
        return list(self.framework.suites.keys())
    
    def get_suite_info(self, suite_name: str) -> Optional[Dict[str, Any]]:
        """Áç≤ÂèñÊ∏¨Ë©¶Â•ó‰ª∂‰ø°ÊÅØ"""
        if suite_name not in self.framework.suites:
            return None
        
        suite = self.framework.suites[suite_name]
        return {
            "name": suite.name,
            "description": suite.description,
            "task_count": len(suite.tasks),
            "tasks": [
                {
                    "name": task.name,
                    "description": task.description,
                    "weight": task.weight,
                    "timeout_seconds": task.timeout_seconds
                }
                for task in suite.tasks
            ]
        }