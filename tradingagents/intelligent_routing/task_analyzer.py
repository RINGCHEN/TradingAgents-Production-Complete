"""
AI Task Analyzer
AIä»»å‹™åˆ†æå™¨

ä»»å‹™6.1: AIä»»å‹™åˆ†æå’Œåˆ†é¡
è² è²¬äºº: å°k (AIè¨“ç·´å°ˆå®¶åœ˜éšŠ)

åŠŸèƒ½ï¼š
- æ™ºèƒ½åˆ†æAIä»»å‹™ç‰¹æ€§
- ä»»å‹™è¤‡é›œåº¦è‡ªå‹•è©•ä¼°
- è³‡æºéœ€æ±‚é æ¸¬
- æ™‚é–“æ•æ„Ÿåº¦åˆ†æ
- ä»»å‹™é¡å‹è‡ªå‹•åˆ†é¡
"""

import re
import json
import logging
from typing import Dict, List, Optional, Any, Tuple, Union
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
import numpy as np

logger = logging.getLogger(__name__)


class TaskType(Enum):
    """ä»»å‹™é¡å‹æšèˆ‰"""
    TRAINING = "training"           # æ¨¡å‹è¨“ç·´
    INFERENCE = "inference"         # æ¨ç†é æ¸¬
    ANALYSIS = "analysis"           # æ•¸æ“šåˆ†æ
    OPTIMIZATION = "optimization"   # å„ªåŒ–èª¿æ•´
    EVALUATION = "evaluation"       # æ¨¡å‹è©•ä¼°
    PREPROCESSING = "preprocessing" # æ•¸æ“šé è™•ç†
    DEPLOYMENT = "deployment"       # æ¨¡å‹éƒ¨ç½²
    MONITORING = "monitoring"       # ç³»çµ±ç›£æ§


class TaskComplexity(Enum):
    """ä»»å‹™è¤‡é›œåº¦æšèˆ‰"""
    SIMPLE = "simple"       # ç°¡å–®ä»»å‹™ (< 1å°æ™‚)
    MODERATE = "moderate"   # ä¸­ç­‰ä»»å‹™ (1-4å°æ™‚)
    COMPLEX = "complex"     # è¤‡é›œä»»å‹™ (4-12å°æ™‚)
    ADVANCED = "advanced"   # é«˜ç´šä»»å‹™ (12å°æ™‚+)


class ResourceType(Enum):
    """è³‡æºé¡å‹æšèˆ‰"""
    CPU = "cpu"
    GPU = "gpu"
    MEMORY = "memory"
    STORAGE = "storage"
    NETWORK = "network"


class TimeSensitivity(Enum):
    """æ™‚é–“æ•æ„Ÿåº¦æšèˆ‰"""
    REAL_TIME = "real_time"     # å¯¦æ™‚ (< 1ç§’)
    INTERACTIVE = "interactive" # äº¤äº’å¼ (< 10ç§’)
    BATCH = "batch"            # æ‰¹è™•ç† (åˆ†é˜ç´š)
    OFFLINE = "offline"        # é›¢ç·š (å°æ™‚ç´š)


@dataclass
class ResourceRequirement:
    """è³‡æºéœ€æ±‚"""
    cpu_cores: int = 1
    gpu_memory_gb: float = 0.0
    system_memory_gb: float = 4.0
    storage_gb: float = 10.0
    network_bandwidth_mbps: float = 100.0
    estimated_duration_minutes: float = 60.0


@dataclass
class TaskClassification:
    """ä»»å‹™åˆ†é¡çµæœ"""
    task_type: TaskType
    complexity: TaskComplexity
    time_sensitivity: TimeSensitivity
    resource_requirements: ResourceRequirement
    confidence_score: float = 0.0
    tags: List[str] = field(default_factory=list)
    recommendations: List[str] = field(default_factory=list)
    
    def to_dict(self) -> Dict[str, Any]:
        """è½‰æ›ç‚ºå­—å…¸æ ¼å¼"""
        return {
            'task_type': self.task_type.value,
            'complexity': self.complexity.value,
            'time_sensitivity': self.time_sensitivity.value,
            'resource_requirements': {
                'cpu_cores': self.resource_requirements.cpu_cores,
                'gpu_memory_gb': self.resource_requirements.gpu_memory_gb,
                'system_memory_gb': self.resource_requirements.system_memory_gb,
                'storage_gb': self.resource_requirements.storage_gb,
                'network_bandwidth_mbps': self.resource_requirements.network_bandwidth_mbps,
                'estimated_duration_minutes': self.resource_requirements.estimated_duration_minutes
            },
            'confidence_score': self.confidence_score,
            'tags': self.tags,
            'recommendations': self.recommendations
        }


class TaskAnalyzer:
    """
    AIä»»å‹™åˆ†æå™¨
    
    æ™ºèƒ½åˆ†æAIä»»å‹™çš„ç‰¹æ€§ï¼ŒåŒ…æ‹¬ï¼š
    - ä»»å‹™é¡å‹è­˜åˆ¥
    - è¤‡é›œåº¦è©•ä¼°
    - è³‡æºéœ€æ±‚é æ¸¬
    - æ™‚é–“æ•æ„Ÿåº¦åˆ†æ
    """
    
    def __init__(self):
        # ä»»å‹™é¡å‹é—œéµè©æ˜ å°„
        self.task_type_keywords = {
            TaskType.TRAINING: [
                'train', 'training', 'è¨“ç·´', 'fine-tune', 'finetune', 'å¾®èª¿',
                'learn', 'learning', 'å­¸ç¿’', 'fit', 'optimize', 'grpo', 'ppo',
                'rlhf', 'sft', 'lora', 'adapter'
            ],
            TaskType.INFERENCE: [
                'predict', 'prediction', 'é æ¸¬', 'inference', 'æ¨ç†', 'generate',
                'completion', 'ç”Ÿæˆ', 'chat', 'å°è©±', 'answer', 'å›ç­”'
            ],
            TaskType.ANALYSIS: [
                'analyze', 'analysis', 'åˆ†æ', 'evaluate', 'è©•ä¼°', 'assess',
                'review', 'æª¢è¦–', 'examine', 'æª¢æŸ¥', 'study', 'ç ”ç©¶'
            ],
            TaskType.OPTIMIZATION: [
                'optimize', 'optimization', 'å„ªåŒ–', 'tune', 'tuning', 'èª¿æ•´',
                'improve', 'æ”¹å–„', 'enhance', 'å¢å¼·', 'refine', 'ç²¾ç…‰'
            ],
            TaskType.EVALUATION: [
                'evaluate', 'evaluation', 'è©•ä¼°', 'test', 'testing', 'æ¸¬è©¦',
                'validate', 'validation', 'é©—è­‰', 'benchmark', 'åŸºæº–æ¸¬è©¦'
            ],
            TaskType.PREPROCESSING: [
                'preprocess', 'preprocessing', 'é è™•ç†', 'clean', 'cleaning', 'æ¸…ç†',
                'transform', 'è½‰æ›', 'normalize', 'æ¨™æº–åŒ–', 'tokenize', 'åˆ†è©'
            ],
            TaskType.DEPLOYMENT: [
                'deploy', 'deployment', 'éƒ¨ç½²', 'serve', 'serving', 'æœå‹™',
                'publish', 'ç™¼å¸ƒ', 'release', 'é‡‹å‡º', 'production', 'ç”Ÿç”¢'
            ],
            TaskType.MONITORING: [
                'monitor', 'monitoring', 'ç›£æ§', 'track', 'tracking', 'è¿½è¹¤',
                'observe', 'è§€å¯Ÿ', 'watch', 'ç›£è¦–', 'alert', 'å‘Šè­¦'
            ]
        }
        
        # è¤‡é›œåº¦æŒ‡æ¨™
        self.complexity_indicators = {
            'simple': [
                'simple', 'ç°¡å–®', 'basic', 'åŸºç¤', 'quick', 'å¿«é€Ÿ',
                'small', 'å°å‹', 'lightweight', 'è¼•é‡'
            ],
            'moderate': [
                'moderate', 'ä¸­ç­‰', 'medium', 'ä¸­å‹', 'standard', 'æ¨™æº–',
                'regular', 'å¸¸è¦', 'typical', 'å…¸å‹'
            ],
            'complex': [
                'complex', 'è¤‡é›œ', 'advanced', 'é«˜ç´š', 'sophisticated', 'ç²¾å¯†',
                'large', 'å¤§å‹', 'comprehensive', 'å…¨é¢'
            ],
            'advanced': [
                'enterprise', 'ä¼æ¥­ç´š', 'production', 'ç”Ÿç”¢ç´š', 'distributed', 'åˆ†æ•£å¼',
                'scalable', 'å¯æ“´å±•', 'high-performance', 'é«˜æ€§èƒ½'
            ]
        }
        
        # æ™‚é–“æ•æ„Ÿåº¦æŒ‡æ¨™
        self.time_sensitivity_indicators = {
            'real_time': [
                'real-time', 'å¯¦æ™‚', 'realtime', 'instant', 'å³æ™‚',
                'immediate', 'ç«‹å³', 'live', 'å³æ™‚', 'streaming', 'ä¸²æµ'
            ],
            'interactive': [
                'interactive', 'äº¤äº’', 'responsive', 'éŸ¿æ‡‰å¼', 'fast', 'å¿«é€Ÿ',
                'quick', 'è¿…é€Ÿ', 'user-facing', 'é¢å‘ç”¨æˆ¶'
            ],
            'batch': [
                'batch', 'æ‰¹è™•ç†', 'scheduled', 'å®šæ™‚', 'periodic', 'é€±æœŸæ€§',
                'background', 'å¾Œå°', 'queue', 'éšŠåˆ—'
            ],
            'offline': [
                'offline', 'é›¢ç·š', 'asynchronous', 'ç•°æ­¥', 'delayed', 'å»¶é²',
                'non-urgent', 'éç·Šæ€¥', 'bulk', 'æ‰¹é‡'
            ]
        }
        
        # è³‡æºéœ€æ±‚æ¨¡å¼
        self.resource_patterns = {
            'gpu_intensive': [
                'gpu', 'cuda', 'training', 'è¨“ç·´', 'deep learning', 'æ·±åº¦å­¸ç¿’',
                'neural network', 'ç¥ç¶“ç¶²çµ¡', 'transformer', 'llm'
            ],
            'cpu_intensive': [
                'cpu', 'compute', 'è¨ˆç®—', 'processing', 'è™•ç†', 'analysis', 'åˆ†æ',
                'optimization', 'å„ªåŒ–', 'simulation', 'æ¨¡æ“¬'
            ],
            'memory_intensive': [
                'large model', 'å¤§æ¨¡å‹', 'big data', 'å¤§æ•¸æ“š', 'cache', 'ç·©å­˜',
                'in-memory', 'å…§å­˜', 'embedding', 'åµŒå…¥'
            ],
            'storage_intensive': [
                'dataset', 'æ•¸æ“šé›†', 'backup', 'å‚™ä»½', 'archive', 'æ­¸æª”',
                'checkpoint', 'æª¢æŸ¥é»', 'model storage', 'æ¨¡å‹å­˜å„²'
            ]
        }
    
    def analyze_task(
        self,
        task_description: str,
        context: Optional[Dict[str, Any]] = None
    ) -> TaskClassification:
        """
        åˆ†æAIä»»å‹™ä¸¦è¿”å›åˆ†é¡çµæœ
        
        Args:
            task_description: ä»»å‹™æè¿°
            context: é¡å¤–ä¸Šä¸‹æ–‡ä¿¡æ¯
            
        Returns:
            TaskClassification: ä»»å‹™åˆ†é¡çµæœ
        """
        logger.info(f"ğŸ” åˆ†æä»»å‹™: {task_description[:50]}...")
        
        # é è™•ç†ä»»å‹™æè¿°
        processed_desc = self._preprocess_description(task_description)
        
        # è­˜åˆ¥ä»»å‹™é¡å‹
        task_type = self._identify_task_type(processed_desc)
        
        # è©•ä¼°è¤‡é›œåº¦
        complexity = self._assess_complexity(processed_desc, context)
        
        # åˆ†ææ™‚é–“æ•æ„Ÿåº¦
        time_sensitivity = self._analyze_time_sensitivity(processed_desc, context)
        
        # é æ¸¬è³‡æºéœ€æ±‚
        resource_requirements = self._predict_resource_requirements(
            processed_desc, task_type, complexity, context
        )
        
        # ç”Ÿæˆæ¨™ç±¤
        tags = self._generate_tags(processed_desc, task_type, complexity)
        
        # ç”Ÿæˆå»ºè­°
        recommendations = self._generate_recommendations(
            task_type, complexity, time_sensitivity, resource_requirements
        )
        
        # è¨ˆç®—ä¿¡å¿ƒåˆ†æ•¸
        confidence_score = self._calculate_confidence_score(
            processed_desc, task_type, complexity
        )
        
        classification = TaskClassification(
            task_type=task_type,
            complexity=complexity,
            time_sensitivity=time_sensitivity,
            resource_requirements=resource_requirements,
            confidence_score=confidence_score,
            tags=tags,
            recommendations=recommendations
        )
        
        logger.info(f"âœ… ä»»å‹™åˆ†æå®Œæˆ: {task_type.value}, {complexity.value}, ä¿¡å¿ƒåº¦: {confidence_score:.2f}")
        
        return classification
    
    def _preprocess_description(self, description: str) -> str:
        """é è™•ç†ä»»å‹™æè¿°"""
        # è½‰æ›ç‚ºå°å¯«
        processed = description.lower()
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ä½†ä¿ç•™ä¸­æ–‡
        processed = re.sub(r'[^\w\s\u4e00-\u9fff]', ' ', processed)
        
        # æ¨™æº–åŒ–ç©ºç™½å­—ç¬¦
        processed = re.sub(r'\s+', ' ', processed).strip()
        
        return processed
    
    def _identify_task_type(self, description: str) -> TaskType:
        """è­˜åˆ¥ä»»å‹™é¡å‹"""
        type_scores = {}
        
        for task_type, keywords in self.task_type_keywords.items():
            score = 0
            for keyword in keywords:
                if keyword in description:
                    # å®Œæ•´åŒ¹é…çµ¦æ›´é«˜åˆ†æ•¸
                    if f" {keyword} " in f" {description} ":
                        score += 2
                    else:
                        score += 1
            type_scores[task_type] = score
        
        # è¿”å›å¾—åˆ†æœ€é«˜çš„ä»»å‹™é¡å‹
        if type_scores:
            best_type = max(type_scores, key=type_scores.get)
            if type_scores[best_type] > 0:
                return best_type
        
        # é»˜èªè¿”å›åˆ†æé¡å‹
        return TaskType.ANALYSIS
    
    def _assess_complexity(
        self,
        description: str,
        context: Optional[Dict[str, Any]] = None
    ) -> TaskComplexity:
        """è©•ä¼°ä»»å‹™è¤‡é›œåº¦"""
        complexity_score = 0
        
        # åŸºæ–¼é—œéµè©è©•ä¼°
        for complexity_level, keywords in self.complexity_indicators.items():
            for keyword in keywords:
                if keyword in description:
                    if complexity_level == 'simple':
                        complexity_score -= 1
                    elif complexity_level == 'moderate':
                        complexity_score += 0
                    elif complexity_level == 'complex':
                        complexity_score += 2
                    elif complexity_level == 'advanced':
                        complexity_score += 3
        
        # åŸºæ–¼æè¿°é•·åº¦è©•ä¼°
        if len(description) > 200:
            complexity_score += 1
        elif len(description) < 50:
            complexity_score -= 1
        
        # åŸºæ–¼ä¸Šä¸‹æ–‡ä¿¡æ¯è©•ä¼°
        if context:
            if context.get('model_size', 0) > 1000000000:  # 10å„„åƒæ•¸ä»¥ä¸Š
                complexity_score += 2
            if context.get('dataset_size', 0) > 1000000:   # 100è¬æ¨£æœ¬ä»¥ä¸Š
                complexity_score += 1
            if context.get('distributed', False):
                complexity_score += 2
        
        # åŸºæ–¼æŠ€è¡“é—œéµè©è©•ä¼°
        advanced_keywords = [
            'distributed', 'multi-gpu', 'cluster', 'kubernetes',
            'microservices', 'pipeline', 'orchestration'
        ]
        for keyword in advanced_keywords:
            if keyword in description:
                complexity_score += 1
        
        # æ˜ å°„åˆ°è¤‡é›œåº¦æšèˆ‰
        if complexity_score <= -1:
            return TaskComplexity.SIMPLE
        elif complexity_score <= 1:
            return TaskComplexity.MODERATE
        elif complexity_score <= 3:
            return TaskComplexity.COMPLEX
        else:
            return TaskComplexity.ADVANCED
    
    def _analyze_time_sensitivity(
        self,
        description: str,
        context: Optional[Dict[str, Any]] = None
    ) -> TimeSensitivity:
        """åˆ†ææ™‚é–“æ•æ„Ÿåº¦"""
        sensitivity_scores = {}
        
        for sensitivity, keywords in self.time_sensitivity_indicators.items():
            score = 0
            for keyword in keywords:
                if keyword in description:
                    score += 1
            sensitivity_scores[sensitivity] = score
        
        # åŸºæ–¼ä¸Šä¸‹æ–‡è©•ä¼°
        if context:
            if context.get('user_facing', False):
                sensitivity_scores['interactive'] = sensitivity_scores.get('interactive', 0) + 2
            if context.get('deadline_hours', 24) < 1:
                sensitivity_scores['real_time'] = sensitivity_scores.get('real_time', 0) + 3
            elif context.get('deadline_hours', 24) < 24:
                sensitivity_scores['interactive'] = sensitivity_scores.get('interactive', 0) + 1
        
        # è¿”å›å¾—åˆ†æœ€é«˜çš„æ•æ„Ÿåº¦
        if sensitivity_scores:
            best_sensitivity = max(sensitivity_scores, key=sensitivity_scores.get)
            if sensitivity_scores[best_sensitivity] > 0:
                return TimeSensitivity(best_sensitivity)
        
        # é»˜èªè¿”å›æ‰¹è™•ç†
        return TimeSensitivity.BATCH
    
    def _predict_resource_requirements(
        self,
        description: str,
        task_type: TaskType,
        complexity: TaskComplexity,
        context: Optional[Dict[str, Any]] = None
    ) -> ResourceRequirement:
        """é æ¸¬è³‡æºéœ€æ±‚"""
        
        # åŸºç¤è³‡æºéœ€æ±‚
        base_requirements = ResourceRequirement()
        
        # æ ¹æ“šä»»å‹™é¡å‹èª¿æ•´
        if task_type == TaskType.TRAINING:
            base_requirements.gpu_memory_gb = 8.0
            base_requirements.system_memory_gb = 16.0
            base_requirements.estimated_duration_minutes = 120.0
        elif task_type == TaskType.INFERENCE:
            base_requirements.gpu_memory_gb = 4.0
            base_requirements.system_memory_gb = 8.0
            base_requirements.estimated_duration_minutes = 5.0
        elif task_type == TaskType.ANALYSIS:
            base_requirements.cpu_cores = 4
            base_requirements.system_memory_gb = 8.0
            base_requirements.estimated_duration_minutes = 30.0
        
        # æ ¹æ“šè¤‡é›œåº¦èª¿æ•´
        complexity_multipliers = {
            TaskComplexity.SIMPLE: 0.5,
            TaskComplexity.MODERATE: 1.0,
            TaskComplexity.COMPLEX: 2.0,
            TaskComplexity.ADVANCED: 4.0
        }
        
        multiplier = complexity_multipliers[complexity]
        base_requirements.gpu_memory_gb *= multiplier
        base_requirements.system_memory_gb *= multiplier
        base_requirements.estimated_duration_minutes *= multiplier
        
        # åŸºæ–¼é—œéµè©èª¿æ•´
        for pattern_type, keywords in self.resource_patterns.items():
            if any(keyword in description for keyword in keywords):
                if pattern_type == 'gpu_intensive':
                    base_requirements.gpu_memory_gb *= 1.5
                elif pattern_type == 'cpu_intensive':
                    base_requirements.cpu_cores *= 2
                elif pattern_type == 'memory_intensive':
                    base_requirements.system_memory_gb *= 2
                elif pattern_type == 'storage_intensive':
                    base_requirements.storage_gb *= 5
        
        # åŸºæ–¼ä¸Šä¸‹æ–‡èª¿æ•´
        if context:
            if context.get('model_size', 0) > 0:
                # æ ¹æ“šæ¨¡å‹å¤§å°èª¿æ•´GPUè¨˜æ†¶é«”éœ€æ±‚
                model_size_gb = context['model_size'] / 1000000000 * 4  # ç²—ç•¥ä¼°ç®—
                base_requirements.gpu_memory_gb = max(base_requirements.gpu_memory_gb, model_size_gb)
            
            if context.get('batch_size', 0) > 0:
                # æ ¹æ“šæ‰¹æ¬¡å¤§å°èª¿æ•´è¨˜æ†¶é«”éœ€æ±‚
                batch_multiplier = max(1, context['batch_size'] / 4)
                base_requirements.gpu_memory_gb *= batch_multiplier
                base_requirements.system_memory_gb *= batch_multiplier
        
        # ç¢ºä¿æœ€å°å€¼
        base_requirements.cpu_cores = max(1, base_requirements.cpu_cores)
        base_requirements.gpu_memory_gb = max(0, base_requirements.gpu_memory_gb)
        base_requirements.system_memory_gb = max(2, base_requirements.system_memory_gb)
        base_requirements.storage_gb = max(5, base_requirements.storage_gb)
        base_requirements.estimated_duration_minutes = max(1, base_requirements.estimated_duration_minutes)
        
        return base_requirements
    
    def _generate_tags(
        self,
        description: str,
        task_type: TaskType,
        complexity: TaskComplexity
    ) -> List[str]:
        """ç”Ÿæˆä»»å‹™æ¨™ç±¤"""
        tags = [task_type.value, complexity.value]
        
        # æŠ€è¡“æ¨™ç±¤
        tech_tags = {
            'machine_learning': ['ml', 'machine learning', 'æ©Ÿå™¨å­¸ç¿’', 'ai', 'äººå·¥æ™ºèƒ½'],
            'deep_learning': ['deep learning', 'æ·±åº¦å­¸ç¿’', 'neural', 'transformer'],
            'nlp': ['nlp', 'natural language', 'è‡ªç„¶èªè¨€', 'text', 'language'],
            'computer_vision': ['cv', 'computer vision', 'è¨ˆç®—æ©Ÿè¦–è¦º', 'image', 'vision'],
            'reinforcement_learning': ['rl', 'reinforcement', 'å¼·åŒ–å­¸ç¿’', 'policy', 'reward'],
            'data_science': ['data science', 'æ•¸æ“šç§‘å­¸', 'analytics', 'åˆ†æ', 'statistics'],
            'gpu_computing': ['gpu', 'cuda', 'parallel', 'ä¸¦è¡Œ', 'acceleration'],
            'cloud': ['cloud', 'é›²ç«¯', 'aws', 'gcp', 'azure', 'kubernetes'],
            'financial': ['financial', 'é‡‘è', 'trading', 'äº¤æ˜“', 'investment', 'æŠ•è³‡']
        }
        
        for tag, keywords in tech_tags.items():
            if any(keyword in description for keyword in keywords):
                tags.append(tag)
        
        return list(set(tags))  # å»é‡
    
    def _generate_recommendations(
        self,
        task_type: TaskType,
        complexity: TaskComplexity,
        time_sensitivity: TimeSensitivity,
        resource_requirements: ResourceRequirement
    ) -> List[str]:
        """ç”Ÿæˆå„ªåŒ–å»ºè­°"""
        recommendations = []
        
        # åŸºæ–¼ä»»å‹™é¡å‹çš„å»ºè­°
        if task_type == TaskType.TRAINING:
            recommendations.append("å»ºè­°ä½¿ç”¨GPUåŠ é€Ÿè¨“ç·´")
            recommendations.append("è€ƒæ…®ä½¿ç”¨æ··åˆç²¾åº¦è¨“ç·´ç¯€çœè¨˜æ†¶é«”")
            if complexity in [TaskComplexity.COMPLEX, TaskComplexity.ADVANCED]:
                recommendations.append("å»ºè­°ä½¿ç”¨æª¢æŸ¥é»ä¿å­˜æ©Ÿåˆ¶")
                recommendations.append("è€ƒæ…®åˆ†æ•£å¼è¨“ç·´æé«˜æ•ˆç‡")
        
        elif task_type == TaskType.INFERENCE:
            if time_sensitivity == TimeSensitivity.REAL_TIME:
                recommendations.append("å»ºè­°ä½¿ç”¨æ¨¡å‹é‡åŒ–åŠ é€Ÿæ¨ç†")
                recommendations.append("è€ƒæ…®ä½¿ç”¨TensorRTå„ªåŒ–")
            recommendations.append("å»ºè­°ä½¿ç”¨æ‰¹é‡æ¨ç†æé«˜ååé‡")
        
        # åŸºæ–¼è¤‡é›œåº¦çš„å»ºè­°
        if complexity == TaskComplexity.ADVANCED:
            recommendations.append("å»ºè­°ä½¿ç”¨å®¹å™¨åŒ–éƒ¨ç½²")
            recommendations.append("è€ƒæ…®ä½¿ç”¨Kubernetesé€²è¡Œç·¨æ’")
            recommendations.append("å»ºè­°å¯¦æ–½å…¨é¢çš„ç›£æ§å’Œæ—¥èªŒ")
        
        # åŸºæ–¼è³‡æºéœ€æ±‚çš„å»ºè­°
        if resource_requirements.gpu_memory_gb > 12:
            recommendations.append("å»ºè­°ä½¿ç”¨RTX 4090æˆ–æ›´é«˜ç´šGPU")
            recommendations.append("è€ƒæ…®ä½¿ç”¨æ¨¡å‹ä¸¦è¡ŒåŒ–")
        elif resource_requirements.gpu_memory_gb > 8:
            recommendations.append("RTX 4070é©åˆæ­¤ä»»å‹™")
        
        if resource_requirements.system_memory_gb > 32:
            recommendations.append("å»ºè­°å‡ç´šç³»çµ±è¨˜æ†¶é«”åˆ°64GB+")
        
        # åŸºæ–¼æ™‚é–“æ•æ„Ÿåº¦çš„å»ºè­°
        if time_sensitivity == TimeSensitivity.REAL_TIME:
            recommendations.append("å»ºè­°ä½¿ç”¨SSDå­˜å„²åŠ é€ŸI/O")
            recommendations.append("è€ƒæ…®ä½¿ç”¨è¨˜æ†¶é«”ç·©å­˜")
        
        return recommendations
    
    def _calculate_confidence_score(
        self,
        description: str,
        task_type: TaskType,
        complexity: TaskComplexity
    ) -> float:
        """è¨ˆç®—åˆ†é¡ä¿¡å¿ƒåˆ†æ•¸"""
        confidence = 0.5  # åŸºç¤ä¿¡å¿ƒåˆ†æ•¸
        
        # åŸºæ–¼æè¿°é•·åº¦
        if len(description) > 100:
            confidence += 0.1
        elif len(description) < 20:
            confidence -= 0.1
        
        # åŸºæ–¼é—œéµè©åŒ¹é…åº¦
        matched_keywords = 0
        total_keywords = 0
        
        for keywords in self.task_type_keywords.values():
            total_keywords += len(keywords)
            for keyword in keywords:
                if keyword in description:
                    matched_keywords += 1
        
        if total_keywords > 0:
            keyword_ratio = matched_keywords / total_keywords
            confidence += keyword_ratio * 0.3
        
        # åŸºæ–¼ä»»å‹™é¡å‹ç‰¹å®šæ€§
        type_keywords = self.task_type_keywords[task_type]
        type_matches = sum(1 for keyword in type_keywords if keyword in description)
        if type_matches > 0:
            confidence += min(type_matches * 0.1, 0.2)
        
        return min(max(confidence, 0.0), 1.0)
    
    def batch_analyze_tasks(
        self,
        tasks: List[Dict[str, Any]]
    ) -> List[TaskClassification]:
        """æ‰¹é‡åˆ†æä»»å‹™"""
        logger.info(f"ğŸ”„ æ‰¹é‡åˆ†æ {len(tasks)} å€‹ä»»å‹™...")
        
        results = []
        for i, task in enumerate(tasks):
            description = task.get('description', '')
            context = task.get('context', {})
            
            try:
                classification = self.analyze_task(description, context)
                results.append(classification)
                logger.info(f"âœ… ä»»å‹™ {i+1}/{len(tasks)} åˆ†æå®Œæˆ")
            except Exception as e:
                logger.error(f"âŒ ä»»å‹™ {i+1} åˆ†æå¤±æ•—: {e}")
                # å‰µå»ºé»˜èªåˆ†é¡
                default_classification = TaskClassification(
                    task_type=TaskType.ANALYSIS,
                    complexity=TaskComplexity.MODERATE,
                    time_sensitivity=TimeSensitivity.BATCH,
                    resource_requirements=ResourceRequirement(),
                    confidence_score=0.1,
                    tags=['unknown'],
                    recommendations=['éœ€è¦æ›´è©³ç´°çš„ä»»å‹™æè¿°']
                )
                results.append(default_classification)
        
        logger.info(f"ğŸ‰ æ‰¹é‡åˆ†æå®Œæˆï¼Œå…±è™•ç† {len(results)} å€‹ä»»å‹™")
        return results
    
    def get_task_statistics(
        self,
        classifications: List[TaskClassification]
    ) -> Dict[str, Any]:
        """ç²å–ä»»å‹™çµ±è¨ˆä¿¡æ¯"""
        if not classifications:
            return {}
        
        # ä»»å‹™é¡å‹åˆ†ä½ˆ
        type_counts = {}
        for classification in classifications:
            task_type = classification.task_type.value
            type_counts[task_type] = type_counts.get(task_type, 0) + 1
        
        # è¤‡é›œåº¦åˆ†ä½ˆ
        complexity_counts = {}
        for classification in classifications:
            complexity = classification.complexity.value
            complexity_counts[complexity] = complexity_counts.get(complexity, 0) + 1
        
        # å¹³å‡è³‡æºéœ€æ±‚
        total_gpu_memory = sum(c.resource_requirements.gpu_memory_gb for c in classifications)
        total_system_memory = sum(c.resource_requirements.system_memory_gb for c in classifications)
        total_duration = sum(c.resource_requirements.estimated_duration_minutes for c in classifications)
        
        avg_gpu_memory = total_gpu_memory / len(classifications)
        avg_system_memory = total_system_memory / len(classifications)
        avg_duration = total_duration / len(classifications)
        
        # å¹³å‡ä¿¡å¿ƒåˆ†æ•¸
        avg_confidence = sum(c.confidence_score for c in classifications) / len(classifications)
        
        return {
            'total_tasks': len(classifications),
            'task_type_distribution': type_counts,
            'complexity_distribution': complexity_counts,
            'average_resources': {
                'gpu_memory_gb': round(avg_gpu_memory, 2),
                'system_memory_gb': round(avg_system_memory, 2),
                'estimated_duration_minutes': round(avg_duration, 2)
            },
            'average_confidence_score': round(avg_confidence, 3),
            'timestamp': datetime.now().isoformat()
        }