#!/usr/bin/env python3
"""
Model Capability Service
æ¨¡å‹èƒ½åŠ›æœå‹™ - GPT-OSSæ•´åˆä»»å‹™1.2.2
çµ±ä¸€LLMClientæ¥å£æ•´åˆ
"""

import asyncio
import logging
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime, timezone

from ..database.model_capability_db import ModelCapabilityDB, ModelCapabilityDBError
from ..database.task_metadata_db import TaskMetadataDB, TaskMetadataDBError
from ..benchmarks.benchmark_runner import BenchmarkRunner
from ..monitoring.performance_monitor import PerformanceMonitor, MetricType
from ..utils.llm_client import LLMClient, LLMRequest, LLMResponse, AnalysisType

logger = logging.getLogger(__name__)

class ModelCapabilityServiceError(Exception):
    """æ¨¡å‹èƒ½åŠ›æœå‹™éŒ¯èª¤"""
    pass

class ModelCapabilityService:
    """
    æ¨¡å‹èƒ½åŠ›æœå‹™
    
    çµ±ä¸€ç®¡ç†ï¼š
    1. æ¨¡å‹èƒ½åŠ›æ•¸æ“šåº«æ“ä½œ
    2. åŸºæº–æ¸¬è©¦åŸ·è¡Œ
    3. æ€§èƒ½ç›£æ§
    4. LLMå®¢æˆ¶ç«¯é›†æˆ
    5. æ™ºèƒ½è·¯ç”±æ±ºç­–
    """
    
    def __init__(
        self,
        llm_client: Optional[LLMClient] = None,
        model_db: Optional[ModelCapabilityDB] = None,
        task_db: Optional[TaskMetadataDB] = None,
        benchmark_runner: Optional[BenchmarkRunner] = None,
        performance_monitor: Optional[PerformanceMonitor] = None
    ):
        """
        åˆå§‹åŒ–æ¨¡å‹èƒ½åŠ›æœå‹™
        
        Args:
            llm_client: LLMå®¢æˆ¶ç«¯
            model_db: æ¨¡å‹èƒ½åŠ›æ•¸æ“šåº«
            task_db: ä»»å‹™å…ƒæ•¸æ“šæ•¸æ“šåº«
            benchmark_runner: åŸºæº–æ¸¬è©¦åŸ·è¡Œå™¨
            performance_monitor: æ€§èƒ½ç›£æ§å™¨
        """
        self.llm_client = llm_client or LLMClient()
        self.model_db = model_db or ModelCapabilityDB()
        self.task_db = task_db or TaskMetadataDB()
        self.benchmark_runner = benchmark_runner or BenchmarkRunner(
            llm_client=self.llm_client,
            model_db=self.model_db
        )
        self.performance_monitor = performance_monitor or PerformanceMonitor(
            model_db=self.model_db,
            task_db=self.task_db
        )
        
        self.logger = logger
        self._initialized = False
    
    async def initialize(self) -> bool:
        """
        åˆå§‹åŒ–æœå‹™
        
        Returns:
            æ˜¯å¦åˆå§‹åŒ–æˆåŠŸ
        """
        try:
            self.logger.info("ğŸš€ Initializing Model Capability Service...")
            
            # åˆå§‹åŒ–æ•¸æ“šåº«
            await self._initialize_databases()
            
            # å•Ÿå‹•æ€§èƒ½ç›£æ§
            await self.performance_monitor.start()
            
            # æ·»åŠ æ€§èƒ½ç›£æ§çš„å‘Šè­¦å›èª¿
            self.performance_monitor.add_alert_callback(self._handle_performance_alert)
            
            self._initialized = True
            self.logger.info("âœ… Model Capability Service initialized successfully")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Failed to initialize Model Capability Service: {e}")
            return False
    
    async def shutdown(self):
        """é—œé–‰æœå‹™"""
        try:
            self.logger.info("ğŸ”„ Shutting down Model Capability Service...")
            
            # åœæ­¢æ€§èƒ½ç›£æ§
            await self.performance_monitor.stop()
            
            # é—œé–‰LLMå®¢æˆ¶ç«¯
            await self.llm_client.close()
            
            self._initialized = False
            self.logger.info("âœ… Model Capability Service shut down successfully")
            
        except Exception as e:
            self.logger.error(f"âŒ Error during service shutdown: {e}")
    
    async def _initialize_databases(self):
        """åˆå§‹åŒ–æ•¸æ“šåº«"""
        try:
            # åˆå§‹åŒ–æ¨™æº–ä»»å‹™é¡å‹
            task_types = await self.task_db.initialize_standard_task_types()
            self.logger.info(f"âœ… Initialized {len(task_types)} standard task types")
            
            # åˆå§‹åŒ–æ¨™æº–æ¨¡å‹
            models = await self.model_db.initialize_standard_models()
            self.logger.info(f"âœ… Initialized {len(models)} standard models")
            
        except Exception as e:
            self.logger.error(f"âŒ Error initializing databases: {e}")
            raise ModelCapabilityServiceError(f"Database initialization failed: {e}")
    
    def _handle_performance_alert(self, message: str, alert_data: Dict[str, Any]):
        """è™•ç†æ€§èƒ½å‘Šè­¦"""
        self.logger.warning(f"âš ï¸ Performance Alert: {message}")
        
        # é€™è£¡å¯ä»¥æ·»åŠ æ›´å¤šå‘Šè­¦è™•ç†é‚è¼¯
        # ä¾‹å¦‚ï¼šç™¼é€é€šçŸ¥ã€è‡ªå‹•èª¿æ•´è·¯ç”±ç­–ç•¥ç­‰
    
    # ==================== æ™ºèƒ½åˆ†ææ¥å£ ====================
    
    async def analyze_with_optimal_model(
        self,
        prompt: str,
        task_type: str,
        context: Optional[Dict[str, Any]] = None,
        user_id: Optional[str] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨æœ€å„ªæ¨¡å‹é€²è¡Œåˆ†æ
        
        Args:
            prompt: åˆ†ææç¤ºè©
            task_type: ä»»å‹™é¡å‹
            context: åˆ†æä¸Šä¸‹æ–‡
            user_id: ç”¨æˆ¶ID
            **kwargs: å…¶ä»–åƒæ•¸
            
        Returns:
            åˆ†æçµæœï¼ŒåŒ…å«è·¯ç”±æ±ºç­–ä¿¡æ¯
        """
        if not self._initialized:
            raise ModelCapabilityServiceError("Service not initialized")
        
        try:
            # ç²å–ä»»å‹™å…ƒæ•¸æ“š
            task_metadata = await self.task_db.get_task_metadata(task_type)
            if not task_metadata:
                raise ModelCapabilityServiceError(f"Task type '{task_type}' not found")
            
            # é€²è¡Œè·¯ç”±æ±ºç­–
            from ..database.task_metadata_models import RoutingDecisionRequest
            
            routing_request = RoutingDecisionRequest(
                task_type=task_type,
                user_tier=kwargs.get('user_tier', 'free'),
                estimated_tokens=len(prompt.split()) * 1.3,  # ä¼°ç®—tokenæ•¸
                priority=kwargs.get('priority', 'standard'),
                requires_high_quality=kwargs.get('requires_high_quality', False),
                max_acceptable_latency=kwargs.get('max_acceptable_latency'),
                max_acceptable_cost=kwargs.get('max_acceptable_cost'),
                user_preferences=kwargs.get('user_preferences')
            )
            
            routing_decision = await self.task_db.make_routing_decision(routing_request)
            
            # ä½¿ç”¨æ€§èƒ½ç›£æ§å™¨æ¸¬é‡è«‹æ±‚
            async with self.performance_monitor.measure_request(
                provider=routing_decision.selected_provider,
                model_id=routing_decision.selected_model,
                task_type=task_type,
                user_id=user_id
            ) as perf_ctx:
                
                # åŸ·è¡ŒLLMåˆ†æ
                analysis_type = self._map_task_type_to_analysis_type(task_type)
                
                request = LLMRequest(
                    prompt=prompt,
                    context=context or {},
                    analysis_type=analysis_type,
                    analyst_id=kwargs.get('analyst_id', 'general'),
                    stock_id=kwargs.get('stock_id'),
                    user_id=user_id,
                    max_tokens=kwargs.get('max_tokens'),
                    temperature=kwargs.get('temperature')
                )
                
                response = await self.llm_client._execute_request(request)
                
                # æ›´æ–°æ€§èƒ½ä¸Šä¸‹æ–‡
                perf_ctx.set_success(response.success)
                
                if response.success:
                    # è©•ä¼°å›æ‡‰å“è³ªï¼ˆç°¡åŒ–å¯¦ç¾ï¼‰
                    quality_score = self._evaluate_response_quality(response.content, task_type)
                    perf_ctx.set_accuracy(quality_score)
                
                    # ä¼°ç®—æˆæœ¬
                    if response.usage:
                        total_tokens = response.usage.get('total_tokens', 0)
                        cost = self._estimate_cost(
                            routing_decision.selected_provider,
                            routing_decision.selected_model,
                            total_tokens
                        )
                        perf_ctx.set_cost(cost)
                
                # è¨˜éŒ„ä»»å‹™æ€§èƒ½åˆ°æ•¸æ“šåº«
                await self.task_db.record_task_performance(
                    task_type=task_type,
                    provider=routing_decision.selected_provider,
                    model=routing_decision.selected_model,
                    latency_ms=response.response_time * 1000 if response.response_time else 0,
                    success=response.success,
                    quality_score=perf_ctx.accuracy if hasattr(perf_ctx, 'accuracy') else None,
                    cost=perf_ctx.cost if hasattr(perf_ctx, 'cost') else 0.0,
                    metadata={
                        'user_id': user_id,
                        'estimated_tokens': routing_request.estimated_tokens,
                        'routing_confidence': routing_decision.confidence_score
                    }
                )
                
                # æ§‹å»ºå®Œæ•´å›æ‡‰
                return {
                    'success': response.success,
                    'content': response.content,
                    'error': response.error,
                    'routing_decision': {
                        'selected_provider': routing_decision.selected_provider,
                        'selected_model': routing_decision.selected_model,
                        'reasoning': routing_decision.reasoning,
                        'expected_cost': routing_decision.expected_cost,
                        'expected_latency_ms': routing_decision.expected_latency_ms,
                        'confidence_score': routing_decision.confidence_score,
                        'fallback_options': routing_decision.fallback_options
                    },
                    'performance': {
                        'actual_latency_ms': response.response_time * 1000 if response.response_time else 0,
                        'tokens_used': response.usage.get('total_tokens') if response.usage else None,
                        'quality_score': perf_ctx.accuracy if hasattr(perf_ctx, 'accuracy') else None
                    },
                    'metadata': response.metadata,
                    'request_id': response.request_id,
                    'timestamp': datetime.now(timezone.utc).isoformat()
                }
                
        except Exception as e:
            self.logger.error(f"âŒ Error in optimal model analysis: {e}")
            raise ModelCapabilityServiceError(f"Analysis failed: {e}")
    
    def _map_task_type_to_analysis_type(self, task_type: str) -> AnalysisType:
        """å°‡ä»»å‹™é¡å‹æ˜ å°„ç‚ºåˆ†æé¡å‹"""
        mapping = {
            'financial_summary': AnalysisType.FUNDAMENTAL,
            'news_classification': AnalysisType.NEWS,
            'investment_reasoning': AnalysisType.REASONING,
            'report_generation': AnalysisType.INVESTMENT,
            'market_sentiment': AnalysisType.SENTIMENT,
            'technical_analysis': AnalysisType.TECHNICAL,
            'risk_assessment': AnalysisType.RISK
        }
        return mapping.get(task_type, AnalysisType.TECHNICAL)
    
    def _evaluate_response_quality(self, content: str, task_type: str) -> float:
        """è©•ä¼°å›æ‡‰å“è³ªï¼ˆç°¡åŒ–å¯¦ç¾ï¼‰"""
        if not content or len(content.strip()) < 20:
            return 0.0
        
        # åŸºæœ¬å“è³ªè©•åˆ†
        base_score = 0.5
        
        # é•·åº¦æª¢æŸ¥
        if len(content) > 100:
            base_score += 0.2
        
        # JSONæ ¼å¼æª¢æŸ¥
        if content.strip().startswith('{') and content.strip().endswith('}'):
            base_score += 0.2
        
        # ä»»å‹™ç‰¹å®šè©•åˆ†
        task_keywords = {
            'financial_summary': ['è²¡å‹™', 'ç‡Ÿæ”¶', 'ç²åˆ©', 'æˆé•·'],
            'investment_reasoning': ['åˆ†æ', 'å»ºè­°', 'é¢¨éšª', 'è©•ä¼°'],
            'market_sentiment': ['æƒ…ç·’', 'å¸‚å ´', 'æŠ•è³‡äºº', 'æ…‹åº¦']
        }
        
        if task_type in task_keywords:
            keywords = task_keywords[task_type]
            keyword_count = sum(1 for keyword in keywords if keyword in content)
            base_score += (keyword_count / len(keywords)) * 0.1
        
        return min(1.0, base_score)
    
    def _estimate_cost(self, provider: str, model: str, tokens: int) -> float:
        """ä¼°ç®—æˆæœ¬"""
        # ç°¡åŒ–çš„æˆæœ¬ä¼°ç®—
        cost_per_1k = {
            'gpt_oss': 0.0,  # æœ¬åœ°å…è²»
            'openai': {
                'gpt-4': 0.03,
                'gpt-3.5-turbo': 0.001
            },
            'anthropic': {
                'claude-3-sonnet-20240229': 0.003
            }
        }
        
        if provider in cost_per_1k:
            if isinstance(cost_per_1k[provider], dict):
                model_cost = cost_per_1k[provider].get(model, 0.002)
            else:
                model_cost = cost_per_1k[provider]
            
            return (tokens / 1000) * model_cost
        
        return 0.002 * (tokens / 1000)  # é»˜èªæˆæœ¬
    
    # ==================== åŸºæº–æ¸¬è©¦æ¥å£ ====================
    
    async def run_model_benchmark(
        self,
        provider: str,
        model_id: str,
        suite_name: str = "standard"
    ) -> Dict[str, Any]:
        """åŸ·è¡Œæ¨¡å‹åŸºæº–æ¸¬è©¦"""
        if not self._initialized:
            raise ModelCapabilityServiceError("Service not initialized")
        
        try:
            return await self.benchmark_runner.run_model_benchmark(
                provider=provider,
                model_id=model_id,
                suite_name=suite_name,
                llm_client=self.llm_client,
                update_database=True
            )
        except Exception as e:
            self.logger.error(f"âŒ Benchmark failed for {provider}/{model_id}: {e}")
            raise ModelCapabilityServiceError(f"Benchmark execution failed: {e}")
    
    async def run_batch_benchmarks(
        self,
        models: List[Tuple[str, str]],
        suite_name: str = "standard",
        parallel: bool = False
    ) -> Dict[str, Any]:
        """åŸ·è¡Œæ‰¹é‡åŸºæº–æ¸¬è©¦"""
        if not self._initialized:
            raise ModelCapabilityServiceError("Service not initialized")
        
        try:
            return await self.benchmark_runner.run_batch_benchmarks(
                models=models,
                suite_name=suite_name,
                llm_client=self.llm_client,
                update_database=True,
                parallel_models=parallel
            )
        except Exception as e:
            self.logger.error(f"âŒ Batch benchmark failed: {e}")
            raise ModelCapabilityServiceError(f"Batch benchmark execution failed: {e}")
    
    async def run_all_model_benchmarks(self, suite_name: str = "standard") -> Dict[str, Any]:
        """å°æ‰€æœ‰è¨»å†Šæ¨¡å‹åŸ·è¡ŒåŸºæº–æ¸¬è©¦"""
        if not self._initialized:
            raise ModelCapabilityServiceError("Service not initialized")
        
        try:
            return await self.benchmark_runner.run_all_registered_models(
                suite_name=suite_name,
                llm_client=self.llm_client,
                update_database=True
            )
        except Exception as e:
            self.logger.error(f"âŒ All models benchmark failed: {e}")
            raise ModelCapabilityServiceError(f"All models benchmark execution failed: {e}")
    
    # ==================== æ¨¡å‹ç®¡ç†æ¥å£ ====================
    
    async def register_model(
        self,
        provider: str,
        model_id: str,
        model_name: str,
        **model_properties
    ) -> Dict[str, Any]:
        """è¨»å†Šæ–°æ¨¡å‹"""
        if not self._initialized:
            raise ModelCapabilityServiceError("Service not initialized")
        
        try:
            model_capability = await self.model_db.create_model_capability(
                provider=provider,
                model_id=model_id,
                model_name=model_name,
                **model_properties
            )
            
            self.logger.info(f"âœ… Registered model: {provider}/{model_id}")
            return model_capability.dict()
            
        except ModelCapabilityDBError as e:
            self.logger.error(f"âŒ Failed to register model {provider}/{model_id}: {e}")
            raise ModelCapabilityServiceError(f"Model registration failed: {e}")
    
    async def list_available_models(
        self,
        provider: Optional[str] = None,
        privacy_level: Optional[str] = None,
        min_capability_score: Optional[float] = None
    ) -> List[Dict[str, Any]]:
        """åˆ—å‡ºå¯ç”¨æ¨¡å‹"""
        if not self._initialized:
            raise ModelCapabilityServiceError("Service not initialized")
        
        try:
            models = await self.model_db.list_model_capabilities(
                provider=provider,
                privacy_level=privacy_level,
                min_capability_score=min_capability_score,
                is_available=True
            )
            
            return [model.dict() for model in models]
            
        except ModelCapabilityDBError as e:
            self.logger.error(f"âŒ Failed to list models: {e}")
            raise ModelCapabilityServiceError(f"Failed to list models: {e}")
    
    async def get_model_recommendations(
        self,
        task_requirements: Dict[str, Any],
        limit: int = 5
    ) -> List[Dict[str, Any]]:
        """ç²å–æ¨¡å‹æ¨è–¦"""
        if not self._initialized:
            raise ModelCapabilityServiceError("Service not initialized")
        
        try:
            recommendations = await self.model_db.recommend_models(
                task_requirements=task_requirements,
                limit=limit
            )
            
            return [
                {
                    'model': model.dict(),
                    'match_score': score,
                    'recommendation_reason': self._generate_recommendation_reason(model, score)
                }
                for model, score in recommendations
            ]
            
        except ModelCapabilityDBError as e:
            self.logger.error(f"âŒ Failed to get recommendations: {e}")
            raise ModelCapabilityServiceError(f"Failed to get recommendations: {e}")
    
    def _generate_recommendation_reason(self, model, score: float) -> str:
        """ç”Ÿæˆæ¨è–¦ç†ç”±"""
        reasons = []
        
        if score >= 0.9:
            reasons.append("Excellent match for requirements")
        elif score >= 0.7:
            reasons.append("Good match for requirements")
        elif score >= 0.5:
            reasons.append("Moderate match for requirements")
        else:
            reasons.append("Low match for requirements")
        
        if model.privacy_level == 'local':
            reasons.append("High privacy protection")
        
        if model.cost_per_1k_input_tokens == 0.0:
            reasons.append("Free to use")
        elif model.cost_per_1k_input_tokens < 0.01:
            reasons.append("Low cost")
        
        if model.avg_latency_ms < 1000:
            reasons.append("Fast response")
        
        return "; ".join(reasons)
    
    # ==================== æ€§èƒ½ç›£æ§æ¥å£ ====================
    
    async def get_performance_report(
        self,
        provider: Optional[str] = None,
        model_id: Optional[str] = None,
        hours_back: int = 24
    ) -> Dict[str, Any]:
        """ç²å–æ€§èƒ½å ±å‘Š"""
        if not self._initialized:
            raise ModelCapabilityServiceError("Service not initialized")
        
        try:
            return await self.performance_monitor.get_performance_report(
                provider=provider,
                model_id=model_id,
                hours_back=hours_back
            )
        except Exception as e:
            self.logger.error(f"âŒ Failed to get performance report: {e}")
            raise ModelCapabilityServiceError(f"Failed to get performance report: {e}")
    
    def get_current_metrics(
        self,
        provider: Optional[str] = None,
        model_id: Optional[str] = None
    ) -> Dict[str, Dict[str, float]]:
        """ç²å–ç•¶å‰æ€§èƒ½æŒ‡æ¨™"""
        if not self._initialized:
            raise ModelCapabilityServiceError("Service not initialized")
        
        return self.performance_monitor.get_current_metrics(
            provider=provider,
            model_id=model_id
        )
    
    # ==================== çµ±è¨ˆå’Œå ±å‘Šæ¥å£ ====================
    
    async def get_system_statistics(self) -> Dict[str, Any]:
        """ç²å–ç³»çµ±çµ±è¨ˆä¿¡æ¯"""
        if not self._initialized:
            raise ModelCapabilityServiceError("Service not initialized")
        
        try:
            # æ¨¡å‹çµ±è¨ˆ
            model_stats = await self.model_db.get_performance_statistics()
            
            # ä»»å‹™çµ±è¨ˆï¼ˆç°¡åŒ–å¯¦ç¾ï¼‰
            task_count = len(await self.task_db.list_task_metadata())
            
            # LLMå®¢æˆ¶ç«¯çµ±è¨ˆ
            llm_stats = self.llm_client.get_stats()
            
            # åŸºæº–æ¸¬è©¦çµ±è¨ˆ
            benchmark_history = self.benchmark_runner.framework.get_history(limit=50)
            
            return {
                'models': model_stats,
                'tasks': {
                    'total_task_types': task_count
                },
                'llm_client': llm_stats,
                'benchmarks': {
                    'total_runs': len(benchmark_history),
                    'recent_runs': benchmark_history[:5] if benchmark_history else []
                },
                'performance_monitoring': {
                    'active': self.performance_monitor._running if hasattr(self.performance_monitor, '_running') else False,
                    'current_metrics_count': len(self.get_current_metrics())
                },
                'timestamp': datetime.now(timezone.utc).isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Failed to get system statistics: {e}")
            return {
                'error': str(e),
                'timestamp': datetime.now(timezone.utc).isoformat()
            }
    
    async def health_check(self) -> Dict[str, Any]:
        """ç³»çµ±å¥åº·æª¢æŸ¥"""
        health_status = {
            'service_initialized': self._initialized,
            'timestamp': datetime.now(timezone.utc).isoformat(),
            'components': {}
        }
        
        try:
            # æª¢æŸ¥LLMå®¢æˆ¶ç«¯
            llm_health = await self.llm_client.health_check()
            health_status['components']['llm_client'] = llm_health
            
            # æª¢æŸ¥æ•¸æ“šåº«ï¼ˆç°¡åŒ–æª¢æŸ¥ï¼‰
            try:
                models = await self.model_db.list_model_capabilities(limit=1)
                health_status['components']['model_db'] = {
                    'status': 'healthy',
                    'accessible': True,
                    'model_count': len(models)
                }
            except Exception as e:
                health_status['components']['model_db'] = {
                    'status': 'unhealthy',
                    'error': str(e)
                }
            
            # æª¢æŸ¥æ€§èƒ½ç›£æ§
            health_status['components']['performance_monitor'] = {
                'status': 'healthy' if hasattr(self.performance_monitor, '_running') and self.performance_monitor._running else 'stopped',
                'metrics_count': len(self.get_current_metrics())
            }
            
            # æ•´é«”ç‹€æ…‹
            component_statuses = [comp.get('status', 'unknown') for comp in health_status['components'].values()]
            if all(status == 'healthy' for status in component_statuses):
                health_status['overall_status'] = 'healthy'
            elif any(status == 'unhealthy' for status in component_statuses):
                health_status['overall_status'] = 'unhealthy'
            else:
                health_status['overall_status'] = 'degraded'
            
            return health_status
            
        except Exception as e:
            health_status['overall_status'] = 'error'
            health_status['error'] = str(e)
            return health_status