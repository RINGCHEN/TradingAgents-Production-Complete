#!/usr/bin/env python3
"""
Financial Model Training Script
é‡‘èæ¨¡å‹è¨“ç·´è…³æœ¬

å®Œæ•´çš„GRPO/PPOè¨“ç·´æµç¨‹ï¼Œæ•´åˆæ‰€æœ‰è¨“ç·´çµ„ä»¶
"""

import os
import sys
import json
import logging
import argparse
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°è·¯å¾‘
sys.path.append(str(Path(__file__).parent.parent.parent))

from tradingagents.training.grpo_trainer import GRPOTrainer
from tradingagents.training.ppo_trainer import PPOTrainer
from tradingagents.training.reward_models import create_reward_model
from tradingagents.training.training_config import (
    TrainingConfig, GRPOConfig, PPOConfig, ConfigManager
)
from tradingagents.training.data_manager import TrainingDataManager
from tradingagents.training.evaluator import ModelEvaluator

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('training.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)


class FinancialModelTrainer:
    """
    é‡‘èæ¨¡å‹è¨“ç·´å™¨
    
    æ•´åˆGRPO/PPOè¨“ç·´æµç¨‹çš„ä¸»è¦é¡åˆ¥
    """
    
    def __init__(self, config: TrainingConfig):
        self.config = config
        self.data_manager = TrainingDataManager(config.dataset_path)
        self.evaluator = ModelEvaluator()
        
        # å‰µå»ºè¼¸å‡ºç›®éŒ„
        self.output_dir = Path(config.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # è¨“ç·´çµ±è¨ˆ
        self.training_stats = {
            'start_time': None,
            'end_time': None,
            'total_epochs': 0,
            'best_score': 0.0,
            'training_history': []
        }
        
    def prepare_data(self) -> tuple:
        """æº–å‚™è¨“ç·´æ•¸æ“š"""
        logger.info("ğŸ”„ æº–å‚™è¨“ç·´æ•¸æ“š...")
        
        # æª¢æŸ¥æ˜¯å¦æœ‰ç¾æœ‰æ•¸æ“š
        dataset_path = Path(self.config.dataset_path)
        
        if not dataset_path.exists() or not any(dataset_path.glob("*.jsonl")):
            logger.info("æœªæ‰¾åˆ°è¨“ç·´æ•¸æ“šï¼Œå‰µå»ºæ¼”ç¤ºæ•¸æ“šé›†...")
            demo_dir = self.data_manager.create_demo_dataset(str(dataset_path))
            logger.info(f"æ¼”ç¤ºæ•¸æ“šé›†å·²å‰µå»ºï¼š{demo_dir}")
        
        # è¼‰å…¥æ•¸æ“š
        data_files = list(dataset_path.glob("*.jsonl"))
        if not data_files:
            raise FileNotFoundError(f"åœ¨ {dataset_path} ä¸­æœªæ‰¾åˆ° .jsonl æ•¸æ“šæ–‡ä»¶")
        
        queries, responses, contexts = self.data_manager.load_financial_data(
            str(data_files[0]), "jsonl"
        )
        
        logger.info(f"è¼‰å…¥äº† {len(queries)} å€‹è¨“ç·´æ¨£æœ¬")
        
        # æ•¸æ“šå“è³ªæª¢æŸ¥
        quality_report = self.data_manager.validate_data_quality(queries, responses, contexts)
        logger.info(f"æ•¸æ“šå“è³ªæª¢æŸ¥å®Œæˆï¼š{len(quality_report['issues'])} å€‹å•é¡Œ")
        
        if quality_report['issues']:
            for issue in quality_report['issues']:
                logger.warning(f"æ•¸æ“šå“è³ªå•é¡Œï¼š{issue}")
        
        # åˆ†å‰²æ•¸æ“š
        train_data, val_data, test_data = self.data_manager.split_data(
            queries, responses, contexts,
            train_ratio=0.8, val_ratio=0.1, test_ratio=0.1
        )
        
        return train_data, val_data, test_data
    
    def setup_model_and_trainer(self):
        """è¨­ç½®æ¨¡å‹å’Œè¨“ç·´å™¨"""
        logger.info(f"ğŸ¤– è¨­ç½®æ¨¡å‹å’Œè¨“ç·´å™¨ (é¡å‹: {self.config.training_type})...")
        
        # å‰µå»ºçå‹µæ¨¡å‹
        reward_model = create_reward_model(
            self.config.reward_model_type,
            self.config.reward_model_config
        )
        
        # æ ¹æ“šè¨“ç·´é¡å‹å‰µå»ºè¨“ç·´å™¨
        if self.config.training_type == "grpo":
            grpo_config = self.config.get_algorithm_config()
            trainer = GRPOTrainer(
                config=grpo_config,
                model_name=self.config.model_name,
                reward_model=reward_model,
                device=self.config.device
            )
        elif self.config.training_type == "ppo":
            ppo_config = self.config.get_algorithm_config()
            trainer = PPOTrainer(
                config=ppo_config,
                model_name=self.config.model_name,
                reward_model=reward_model,
                device=self.config.device
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è¨“ç·´é¡å‹: {self.config.training_type}")
        
        return trainer
    
    def train_model(self, trainer, train_data, val_data):
        """åŸ·è¡Œæ¨¡å‹è¨“ç·´"""
        logger.info("ğŸš€ é–‹å§‹æ¨¡å‹è¨“ç·´...")
        
        self.training_stats['start_time'] = datetime.now().isoformat()
        
        train_queries, train_responses, train_contexts = train_data
        val_queries, val_responses, val_contexts = val_data
        
        best_val_score = 0.0
        patience_counter = 0
        
        for epoch in range(self.config.num_train_epochs):
            logger.info(f"ğŸ“š Epoch {epoch + 1}/{self.config.num_train_epochs}")
            
            # è¨“ç·´ä¸€å€‹epoch
            if self.config.training_type == "grpo":
                # GRPOéœ€è¦(query, response)å°
                training_pairs = list(zip(train_queries, train_responses))
                epoch_stats = trainer.train_epoch(training_pairs, train_contexts)
            else:
                # PPOä½¿ç”¨æŸ¥è©¢ç”Ÿæˆå›æ‡‰
                epoch_stats = trainer.train_epoch(train_queries, train_contexts)
            
            # è¨˜éŒ„è¨“ç·´çµ±è¨ˆ
            self.training_stats['training_history'].append({
                'epoch': epoch + 1,
                'stats': epoch_stats,
                'timestamp': datetime.now().isoformat()
            })
            
            # é©—è­‰è©•ä¼°
            if (epoch + 1) % self.config.eval_steps == 0:
                logger.info("ğŸ“Š åŸ·è¡Œé©—è­‰è©•ä¼°...")
                
                val_result = self.evaluator.evaluate_model(
                    trainer.model,
                    trainer.tokenizer,
                    val_queries[:10],  # ä½¿ç”¨å‰10å€‹æ¨£æœ¬å¿«é€Ÿè©•ä¼°
                    val_responses[:10],
                    val_contexts[:10] if val_contexts else None,
                    f"{self.config.model_name}_epoch_{epoch + 1}"
                )
                
                val_score = val_result.metrics.overall_score
                logger.info(f"é©—è­‰åˆ†æ•¸: {val_score:.3f}")
                
                # æª¢æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
                if val_score > best_val_score:
                    best_val_score = val_score
                    patience_counter = 0
                    
                    # ä¿å­˜æœ€ä½³æ¨¡å‹
                    best_model_path = self.output_dir / "best_model"
                    trainer.save_checkpoint(str(best_model_path), epoch + 1, epoch_stats)
                    logger.info(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (åˆ†æ•¸: {val_score:.3f})")
                else:
                    patience_counter += 1
                
                # æ—©åœæª¢æŸ¥
                if hasattr(self.config, 'early_stopping_patience') and patience_counter >= self.config.early_stopping_patience:
                    logger.info(f"â¹ï¸ æ—©åœè§¸ç™¼ (patience: {patience_counter})")
                    break
            
            # å®šæœŸä¿å­˜æª¢æŸ¥é»
            if (epoch + 1) % self.config.save_steps == 0:
                checkpoint_path = self.output_dir / f"checkpoint_epoch_{epoch + 1}"
                trainer.save_checkpoint(str(checkpoint_path), epoch + 1, epoch_stats)
                logger.info(f"ğŸ’¾ ä¿å­˜æª¢æŸ¥é»: epoch {epoch + 1}")
        
        self.training_stats['end_time'] = datetime.now().isoformat()
        self.training_stats['total_epochs'] = epoch + 1
        self.training_stats['best_score'] = best_val_score
        
        logger.info("âœ… è¨“ç·´å®Œæˆï¼")
        return trainer
    
    def evaluate_final_model(self, trainer, test_data):
        """è©•ä¼°æœ€çµ‚æ¨¡å‹"""
        logger.info("ğŸ“ˆ åŸ·è¡Œæœ€çµ‚æ¨¡å‹è©•ä¼°...")
        
        test_queries, test_responses, test_contexts = test_data
        
        # è¼‰å…¥æœ€ä½³æ¨¡å‹é€²è¡Œè©•ä¼°
        best_model_path = self.output_dir / "best_model"
        if best_model_path.exists():
            logger.info("è¼‰å…¥æœ€ä½³æ¨¡å‹é€²è¡Œè©•ä¼°...")
            trainer.load_checkpoint(str(best_model_path))
        
        # åŸ·è¡Œå®Œæ•´è©•ä¼°
        final_result = self.evaluator.evaluate_model(
            trainer.model,
            trainer.tokenizer,
            test_queries,
            test_responses,
            test_contexts,
            f"{self.config.model_name}_final"
        )
        
        # ç”Ÿæˆè©•ä¼°å ±å‘Š
        report_dir = self.output_dir / "evaluation_report"
        self.evaluator.generate_evaluation_report(
            final_result,
            str(report_dir),
            include_plots=True
        )
        
        logger.info(f"ğŸ“Š æœ€çµ‚è©•ä¼°åˆ†æ•¸: {final_result.metrics.overall_score:.3f}")
        logger.info(f"ğŸ“„ è©•ä¼°å ±å‘Šå·²ä¿å­˜åˆ°: {report_dir}")
        
        return final_result
    
    def save_training_summary(self, final_result):
        """ä¿å­˜è¨“ç·´ç¸½çµ"""
        summary = {
            'config': self.config.to_dict(),
            'training_stats': self.training_stats,
            'final_evaluation': {
                'overall_score': final_result.metrics.overall_score,
                'accuracy_score': final_result.metrics.accuracy_score,
                'relevance_score': final_result.metrics.relevance_score,
                'risk_awareness_score': final_result.metrics.risk_awareness_score,
                'actionability_score': final_result.metrics.actionability_score,
                'compliance_score': final_result.metrics.compliance_score
            },
            'recommendations': final_result.recommendations,
            'hardware_info': self._get_hardware_info()
        }
        
        summary_path = self.output_dir / "training_summary.json"
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“‹ è¨“ç·´ç¸½çµå·²ä¿å­˜åˆ°: {summary_path}")
    
    def _get_hardware_info(self) -> Dict[str, Any]:
        """ç²å–ç¡¬é«”ä¿¡æ¯"""
        info = {
            'cuda_available': torch.cuda.is_available(),
            'device_count': torch.cuda.device_count() if torch.cuda.is_available() else 0
        }
        
        if torch.cuda.is_available():
            info['gpu_name'] = torch.cuda.get_device_name(0)
            info['gpu_memory_total'] = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        
        return info
    
    def run_full_training_pipeline(self):
        """åŸ·è¡Œå®Œæ•´çš„è¨“ç·´æµç¨‹"""
        try:
            logger.info("ğŸ¯ é–‹å§‹é‡‘èæ¨¡å‹è¨“ç·´æµç¨‹...")
            
            # 1. æº–å‚™æ•¸æ“š
            train_data, val_data, test_data = self.prepare_data()
            
            # 2. è¨­ç½®æ¨¡å‹å’Œè¨“ç·´å™¨
            trainer = self.setup_model_and_trainer()
            
            # 3. åŸ·è¡Œè¨“ç·´
            trained_trainer = self.train_model(trainer, train_data, val_data)
            
            # 4. æœ€çµ‚è©•ä¼°
            final_result = self.evaluate_final_model(trained_trainer, test_data)
            
            # 5. ä¿å­˜ç¸½çµ
            self.save_training_summary(final_result)
            
            logger.info("ğŸ‰ è¨“ç·´æµç¨‹å®Œæˆï¼")
            return final_result
            
        except Exception as e:
            logger.error(f"âŒ è¨“ç·´éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
            raise


def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description="é‡‘èæ¨¡å‹è¨“ç·´è…³æœ¬")
    parser.add_argument("--config", type=str, help="é…ç½®æ–‡ä»¶è·¯å¾‘")
    parser.add_argument("--training-type", choices=["grpo", "ppo"], default="grpo", help="è¨“ç·´é¡å‹")
    parser.add_argument("--model-name", type=str, default="microsoft/DialoGPT-medium", help="åŸºç¤æ¨¡å‹åç¨±")
    parser.add_argument("--dataset-path", type=str, default="./data/training_data", help="æ•¸æ“šé›†è·¯å¾‘")
    parser.add_argument("--output-dir", type=str, default="./models/trained_model", help="è¼¸å‡ºç›®éŒ„")
    parser.add_argument("--num-epochs", type=int, default=3, help="è¨“ç·´è¼ªæ•¸")
    parser.add_argument("--batch-size", type=int, default=4, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--learning-rate", type=float, default=5e-5, help="å­¸ç¿’ç‡")
    parser.add_argument("--reward-model", choices=["financial", "trading", "risk_adjusted", "multi_factor"], 
                       default="financial", help="çå‹µæ¨¡å‹é¡å‹")
    
    args = parser.parse_args()
    
    # è¼‰å…¥æˆ–å‰µå»ºé…ç½®
    if args.config:
        config_manager = ConfigManager()
        config = config_manager.load_config(args.config)
    else:
        # ä½¿ç”¨å‘½ä»¤è¡Œåƒæ•¸å‰µå»ºé…ç½®
        config = TrainingConfig(
            training_type=args.training_type,
            model_name=args.model_name,
            dataset_path=args.dataset_path,
            output_dir=args.output_dir,
            num_train_epochs=args.num_epochs,
            batch_size=args.batch_size,
            learning_rate=args.learning_rate,
            reward_model_type=args.reward_model
        )
    
    # é¡¯ç¤ºé…ç½®ä¿¡æ¯
    logger.info("ğŸ“‹ è¨“ç·´é…ç½®:")
    logger.info(f"  - è¨“ç·´é¡å‹: {config.training_type}")
    logger.info(f"  - æ¨¡å‹åç¨±: {config.model_name}")
    logger.info(f"  - æ•¸æ“šé›†è·¯å¾‘: {config.dataset_path}")
    logger.info(f"  - è¼¸å‡ºç›®éŒ„: {config.output_dir}")
    logger.info(f"  - è¨“ç·´è¼ªæ•¸: {config.num_train_epochs}")
    logger.info(f"  - æ‰¹æ¬¡å¤§å°: {config.batch_size}")
    logger.info(f"  - å­¸ç¿’ç‡: {config.learning_rate}")
    logger.info(f"  - çå‹µæ¨¡å‹: {config.reward_model_type}")
    
    # ç¡¬é«”å»ºè­°
    hardware_rec = config.get_hardware_recommendations()
    logger.info("ğŸ’» ç¡¬é«”å»ºè­°:")
    for key, value in hardware_rec.items():
        logger.info(f"  - {key}: {value}")
    
    # å‰µå»ºè¨“ç·´å™¨ä¸¦åŸ·è¡Œè¨“ç·´
    trainer = FinancialModelTrainer(config)
    result = trainer.run_full_training_pipeline()
    
    logger.info(f"ğŸ† æœ€çµ‚è©•ä¼°åˆ†æ•¸: {result.metrics.overall_score:.3f}")
    logger.info("ğŸ¯ è¨“ç·´å®Œæˆï¼æª¢æŸ¥è¼¸å‡ºç›®éŒ„ä»¥ç²å–è©³ç´°çµæœã€‚")



# è‡ªå‹•æª¢æ¸¬å’Œåˆ‡æ›åˆ° TradingAgents ç›®éŒ„
def ensure_tradingagents_directory():
    """ç¢ºä¿ç•¶å‰å·¥ä½œç›®éŒ„åœ¨ TradingAgents/ ä¸‹ï¼Œä»¥æ­£ç¢ºè¨ªå•é…ç½®æ–‡ä»¶"""
    current_dir = Path.cwd()
    
    # å¦‚æœç•¶å‰ç›®éŒ„æ˜¯ TradingAgents çš„çˆ¶ç›®éŒ„ï¼Œåˆ‡æ›åˆ° TradingAgents
    if (current_dir / 'TradingAgents').exists():
        os.chdir(current_dir / 'TradingAgents')
        print(f"[DIR] è‡ªå‹•åˆ‡æ›å·¥ä½œç›®éŒ„åˆ°: {Path.cwd()}")
    
    # é©—è­‰å¿…è¦çš„ç›®éŒ„å­˜åœ¨
    required_dirs = ['configs', 'training', 'tradingagents']
    missing_dirs = [d for d in required_dirs if not Path(d).exists()]
    
    if missing_dirs:
        raise FileNotFoundError(f"ç¼ºå°‘å¿…è¦ç›®éŒ„: {missing_dirs}. è«‹ç¢ºä¿å¾ TradingAgents/ ç›®éŒ„åŸ·è¡Œæ­¤è…³æœ¬")

# ç›®éŒ„æª¢æŸ¥å‡½æ•¸å·²æº–å‚™å¥½ï¼Œä½†ä¸åœ¨æ¨¡çµ„å°å…¥æ™‚è‡ªå‹•åŸ·è¡Œ
# åªåœ¨éœ€è¦æ™‚æ‰‹å‹•èª¿ç”¨ ensure_tradingagents_directory()

if __name__ == "__main__":
    main()