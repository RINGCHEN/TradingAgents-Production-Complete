"""
Validation System for Financial AI Training
é‡‘èAIè¨“ç·´é©—è­‰ç³»çµ±

ä»»å‹™4.3: é©—è­‰é›†å’Œè©•ä¼°æŒ‡æ¨™ç³»çµ±
è² è²¬äºº: å°k (AIè¨“ç·´å°ˆå®¶åœ˜éšŠ)

æä¾›ï¼š
- é©—è­‰é›†ç®¡ç†
- å¤šç¶­åº¦è©•ä¼°æŒ‡æ¨™
- è‡ªå‹•åŒ–é©—è­‰æµç¨‹
- æ€§èƒ½åŸºæº–æ¸¬è©¦
- é©—è­‰å ±å‘Šç”Ÿæˆ
"""

import os
import json
import logging
import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import torch

from .reward_models import create_reward_model, FinancialRewardModel
from .data_manager import TrainingDataManager

logger = logging.getLogger(__name__)


@dataclass
class ValidationMetrics:
    """é©—è­‰æŒ‡æ¨™æ•¸æ“šçµæ§‹"""
    
    # æ ¸å¿ƒè©•ä¼°æŒ‡æ¨™
    accuracy_score: float = 0.0
    relevance_score: float = 0.0
    risk_awareness_score: float = 0.0
    actionability_score: float = 0.0
    compliance_score: float = 0.0
    overall_score: float = 0.0
    
    # èªè¨€å“è³ªæŒ‡æ¨™
    fluency_score: float = 0.0
    coherence_score: float = 0.0
    informativeness_score: float = 0.0
    
    # é‡‘èå°ˆæ¥­æŒ‡æ¨™
    financial_accuracy: float = 0.0
    risk_assessment_quality: float = 0.0
    investment_logic_score: float = 0.0
    
    # çµ±è¨ˆæŒ‡æ¨™
    response_length_avg: float = 0.0
    response_length_std: float = 0.0
    financial_terms_density: float = 0.0
    risk_mentions_ratio: float = 0.0
    compliance_coverage: float = 0.0
    
    # æ€§èƒ½æŒ‡æ¨™
    inference_time_avg: float = 0.0
    tokens_per_second: float = 0.0
    memory_usage_mb: float = 0.0
    
    def to_dict(self) -> Dict[str, float]:
        """è½‰æ›ç‚ºå­—å…¸æ ¼å¼"""
        return asdict(self)


@dataclass
class ValidationResult:
    """é©—è­‰çµæœæ•¸æ“šçµæ§‹"""
    
    model_name: str
    validation_date: str
    dataset_name: str
    sample_count: int
    metrics: ValidationMetrics
    detailed_scores: List[Dict[str, Any]]
    benchmark_comparison: Dict[str, float]
    recommendations: List[str]
    validation_config: Dict[str, Any]


class ValidationDatasetManager:
    """é©—è­‰æ•¸æ“šé›†ç®¡ç†å™¨"""
    
    def __init__(self, validation_dir: str = "./data/validation"):
        self.validation_dir = Path(validation_dir)
        self.validation_dir.mkdir(parents=True, exist_ok=True)
        
        # é å®šç¾©çš„é©—è­‰æ•¸æ“šé›†
        self.validation_datasets = {
            'financial_analysis': self._create_financial_analysis_dataset(),
            'investment_advice': self._create_investment_advice_dataset(),
            'risk_assessment': self._create_risk_assessment_dataset(),
            'market_commentary': self._create_market_commentary_dataset()
        }
    
    def _create_financial_analysis_dataset(self) -> List[Dict[str, Any]]:
        """å‰µå»ºé‡‘èåˆ†æé©—è­‰æ•¸æ“šé›†"""
        return [
            {
                'query': 'åˆ†æå°ç©é›»(2330)çš„æŠ•è³‡åƒ¹å€¼',
                'expected_response': 'å°ç©é›»ä½œç‚ºå…¨çƒåŠå°é«”é¾é ­ï¼Œå…·æœ‰å…ˆé€²è£½ç¨‹æŠ€è¡“å„ªå‹¢å’Œç©©å®šå®¢æˆ¶é—œä¿‚ã€‚æŠ•è³‡äº®é»åŒ…æ‹¬AIæ™¶ç‰‡éœ€æ±‚å¢é•·å’Œé•·æœŸæŠ€è¡“é ˜å…ˆåœ°ä½ã€‚é¢¨éšªå› ç´ éœ€è€ƒæ…®åœ°ç·£æ”¿æ²»å½±éŸ¿å’Œæ™¯æ°£å¾ªç’°ã€‚å»ºè­°é•·æœŸæŠ•è³‡è€…å¯åˆ†æ‰¹å¸ƒå±€ï¼Œä½†è«‹æ³¨æ„æŠ•è³‡é¢¨éšªï¼Œå»ºè­°è«®è©¢å°ˆæ¥­æŠ•è³‡é¡§å•ã€‚',
                'context': {
                    'stock_code': '2330',
                    'company': 'å°ç©é›»',
                    'sector': 'åŠå°é«”',
                    'analysis_type': 'fundamental'
                },
                'evaluation_criteria': {
                    'accuracy_weight': 0.3,
                    'risk_awareness_weight': 0.25,
                    'actionability_weight': 0.25,
                    'compliance_weight': 0.2
                }
            },
            {
                'query': 'é´»æµ·(2317)çš„è²¡å‹™ç‹€æ³å¦‚ä½•ï¼Ÿ',
                'expected_response': 'é´»æµ·ç‚ºå…¨çƒæœ€å¤§é›»å­ä»£å·¥å» ï¼Œç‡Ÿæ”¶è¦æ¨¡é¾å¤§ä½†æ¯›åˆ©ç‡ç›¸å°è¼ƒä½ã€‚è¿‘å¹´ç©æ¥µè½‰å‹ç™¼å±•é›»å‹•è»Šã€åŠå°é«”ç­‰æ–°äº‹æ¥­ã€‚è²¡å‹™é«”è³ªç©©å¥ï¼Œç¾é‡‘æµå……æ²›ï¼Œä½†éœ€é—œæ³¨è½‰å‹æˆæ•ˆå’Œç«¶çˆ­å£“åŠ›ã€‚æŠ•è³‡å‰è«‹è©³ç´°è©•ä¼°å€‹äººé¢¨éšªæ‰¿å—åº¦ï¼Œæ­¤ç‚ºåˆ†æè§€é»éæŠ•è³‡å»ºè­°ã€‚',
                'context': {
                    'stock_code': '2317',
                    'company': 'é´»æµ·',
                    'sector': 'é›»å­ä»£å·¥',
                    'analysis_type': 'financial'
                },
                'evaluation_criteria': {
                    'accuracy_weight': 0.35,
                    'risk_awareness_weight': 0.2,
                    'actionability_weight': 0.25,
                    'compliance_weight': 0.2
                }
            },
            {
                'query': 'è¯ç™¼ç§‘(2454)èˆ‡é«˜é€šçš„ç«¶çˆ­å„ªå‹¢æ¯”è¼ƒ',
                'expected_response': 'è¯ç™¼ç§‘åœ¨ä¸­ä½éšæ™¶ç‰‡å¸‚å ´å…·æœ‰æˆæœ¬å„ªå‹¢ï¼Œé«˜é€šåœ¨é«˜éšè™•ç†å™¨å’Œå°ˆåˆ©æŠ€è¡“é ˜å…ˆã€‚è¯ç™¼ç§‘è¿‘å¹´åœ¨5Gå’ŒAIæ™¶ç‰‡é ˜åŸŸå¿«é€Ÿè¿½è¶•ï¼Œä½†é«˜é€šåœ¨æ——è‰¦æ‰‹æ©Ÿå¸‚å ´ä»å ä¸»å°åœ°ä½ã€‚å…©å®¶å…¬å¸å„æœ‰å„ªå‹¢ï¼ŒæŠ•è³‡éœ€è€ƒæ…®æŠ€è¡“ç™¼å±•è¶¨å‹¢å’Œå¸‚å ´ç«¶çˆ­è®ŠåŒ–ã€‚æŠ•è³‡æœ‰é¢¨éšªï¼Œè«‹è¬¹æ…è©•ä¼°ã€‚',
                'context': {
                    'stock_code': '2454',
                    'company': 'è¯ç™¼ç§‘',
                    'sector': 'åŠå°é«”',
                    'analysis_type': 'competitive'
                },
                'evaluation_criteria': {
                    'accuracy_weight': 0.3,
                    'risk_awareness_weight': 0.2,
                    'actionability_weight': 0.3,
                    'compliance_weight': 0.2
                }
            }
        ]
    
    def _create_investment_advice_dataset(self) -> List[Dict[str, Any]]:
        """å‰µå»ºæŠ•è³‡å»ºè­°é©—è­‰æ•¸æ“šé›†"""
        return [
            {
                'query': 'ç¾åœ¨é©åˆæŠ•è³‡å°è‚¡å—ï¼Ÿ',
                'expected_response': 'å°è‚¡æŠ•è³‡éœ€è€ƒæ…®å¤šé …å› ç´ ï¼šç¶“æ¿ŸåŸºæœ¬é¢ã€åœ‹éš›æƒ…å‹¢ã€è³‡é‡‘æµå‘ç­‰ã€‚å»ºè­°åˆ†æ•£æŠ•è³‡ã€å®šæœŸå®šé¡ã€é•·æœŸæŒæœ‰çš„ç­–ç•¥ã€‚æŠ•è³‡å‰æ‡‰è©•ä¼°å€‹äººè²¡å‹™ç‹€æ³å’Œé¢¨éšªæ‰¿å—åº¦ï¼Œå»ºè­°è«®è©¢å°ˆæ¥­è²¡å‹™é¡§å•ã€‚æ­¤ç‚ºä¸€èˆ¬æ€§åˆ†æï¼Œéå€‹äººåŒ–æŠ•è³‡å»ºè­°ï¼ŒæŠ•è³‡æœ‰é¢¨éšªè«‹è¬¹æ…è©•ä¼°ã€‚',
                'context': {
                    'market': 'å°è‚¡',
                    'investment_type': 'general_advice',
                    'risk_level': 'medium'
                },
                'evaluation_criteria': {
                    'accuracy_weight': 0.25,
                    'risk_awareness_weight': 0.3,
                    'actionability_weight': 0.25,
                    'compliance_weight': 0.2
                }
            },
            {
                'query': 'æ–°æ‰‹æŠ•è³‡è€…æ‡‰è©²å¦‚ä½•é–‹å§‹ï¼Ÿ',
                'expected_response': 'æ–°æ‰‹æŠ•è³‡å»ºè­°ï¼š1.å»ºç«‹ç·Šæ€¥é å‚™é‡‘ 2.å­¸ç¿’åŸºæœ¬æŠ•è³‡çŸ¥è­˜ 3.å¾ä½é¢¨éšªç”¢å“é–‹å§‹ 4.åˆ†æ•£æŠ•è³‡é™ä½é¢¨éšª 5.å®šæœŸæª¢è¦–èª¿æ•´ã€‚å¯è€ƒæ…®ETFã€å®šæœŸå®šé¡ç­‰æ–¹å¼å…¥é–€ã€‚æœ€é‡è¦æ˜¯é¢¨éšªç®¡ç†ï¼Œä¸è¦æŠ•å…¥ç„¡æ³•æ‰¿å—æå¤±çš„è³‡é‡‘ã€‚å»ºè­°å°‹æ±‚å°ˆæ¥­ç†è²¡é¡§å•å”åŠ©åˆ¶å®šå€‹äººåŒ–æŠ•è³‡è¨ˆåŠƒã€‚',
                'context': {
                    'investor_type': 'beginner',
                    'investment_type': 'education',
                    'risk_level': 'low'
                },
                'evaluation_criteria': {
                    'accuracy_weight': 0.2,
                    'risk_awareness_weight': 0.35,
                    'actionability_weight': 0.3,
                    'compliance_weight': 0.15
                }
            }
        ]
    
    def _create_risk_assessment_dataset(self) -> List[Dict[str, Any]]:
        """å‰µå»ºé¢¨éšªè©•ä¼°é©—è­‰æ•¸æ“šé›†"""
        return [
            {
                'query': 'æŠ•è³‡é«˜è‚¡æ¯è‚¡ç¥¨æœ‰ä»€éº¼é¢¨éšªï¼Ÿ',
                'expected_response': 'é«˜è‚¡æ¯è‚¡ç¥¨é¢¨éšªåŒ…æ‹¬ï¼š1.è‚¡æ¯å‰Šæ¸›é¢¨éšª-å…¬å¸ç²åˆ©ä¸‹é™å¯èƒ½æ¸›å°‘é…æ¯ 2.è‚¡åƒ¹æ³¢å‹•é¢¨éšª-é«˜è‚¡æ¯ä¸ä¿è­‰è‚¡åƒ¹ç©©å®š 3.ç”¢æ¥­é›†ä¸­é¢¨éšª-å‚³çµ±é«˜è‚¡æ¯å¤šé›†ä¸­ç‰¹å®šç”¢æ¥­ 4.åˆ©ç‡é¢¨éšª-å‡æ¯æ™‚å¸å¼•åŠ›ä¸‹é™ã€‚æŠ•è³‡å‰éœ€è©•ä¼°å…¬å¸åŸºæœ¬é¢ã€è‚¡æ¯ç©©å®šæ€§å’Œå€‹äººé¢¨éšªæ‰¿å—åº¦ã€‚å»ºè­°åˆ†æ•£æŠ•è³‡ï¼Œè«®è©¢å°ˆæ¥­æ„è¦‹ã€‚',
                'context': {
                    'investment_type': 'dividend_stocks',
                    'risk_focus': 'comprehensive',
                    'analysis_depth': 'detailed'
                },
                'evaluation_criteria': {
                    'accuracy_weight': 0.25,
                    'risk_awareness_weight': 0.4,
                    'actionability_weight': 0.2,
                    'compliance_weight': 0.15
                }
            }
        ]
    
    def _create_market_commentary_dataset(self) -> List[Dict[str, Any]]:
        """å‰µå»ºå¸‚å ´è©•è«–é©—è­‰æ•¸æ“šé›†"""
        return [
            {
                'query': 'å¦‚ä½•çœ‹å¾…ç•¶å‰çš„å¸‚å ´æ³¢å‹•ï¼Ÿ',
                'expected_response': 'å¸‚å ´æ³¢å‹•æ˜¯æ­£å¸¸ç¾è±¡ï¼Œåæ˜ æŠ•è³‡è€…å°ç¶“æ¿Ÿã€æ”¿ç­–ã€åœ°ç·£æ”¿æ²»ç­‰å› ç´ çš„åæ‡‰ã€‚é¢å°æ³¢å‹•å»ºè­°ï¼š1.ä¿æŒå†·éœç†æ€§ 2.å …æŒé•·æœŸæŠ•è³‡ç­–ç•¥ 3.é©åº¦åˆ†æ•£é¢¨éšª 4.é¿å…æƒ…ç·’åŒ–æ±ºç­–ã€‚çŸ­æœŸæ³¢å‹•é›£ä»¥é æ¸¬ï¼Œé‡è¦çš„æ˜¯åŸºæ–¼åŸºæœ¬é¢åšæŠ•è³‡æ±ºç­–ã€‚å¸‚å ´æ³¢å‹•ä¹Ÿå‰µé€ æŠ•è³‡æ©Ÿæœƒï¼Œä½†éœ€è¬¹æ…è©•ä¼°é¢¨éšªã€‚',
                'context': {
                    'market_condition': 'volatile',
                    'commentary_type': 'general',
                    'time_horizon': 'mixed'
                },
                'evaluation_criteria': {
                    'accuracy_weight': 0.3,
                    'risk_awareness_weight': 0.25,
                    'actionability_weight': 0.25,
                    'compliance_weight': 0.2
                }
            }
        ]
    
    def get_validation_dataset(self, dataset_name: str) -> List[Dict[str, Any]]:
        """ç²å–æŒ‡å®šçš„é©—è­‰æ•¸æ“šé›†"""
        if dataset_name not in self.validation_datasets:
            raise ValueError(f"æœªçŸ¥çš„é©—è­‰æ•¸æ“šé›†: {dataset_name}")
        
        return self.validation_datasets[dataset_name]
    
    def get_all_datasets(self) -> Dict[str, List[Dict[str, Any]]]:
        """ç²å–æ‰€æœ‰é©—è­‰æ•¸æ“šé›†"""
        return self.validation_datasets.copy()
    
    def save_validation_dataset(self, dataset_name: str, output_path: Optional[str] = None):
        """ä¿å­˜é©—è­‰æ•¸æ“šé›†åˆ°æ–‡ä»¶"""
        if output_path is None:
            output_path = self.validation_dir / f"{dataset_name}_validation.json"
        
        dataset = self.get_validation_dataset(dataset_name)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(dataset, f, ensure_ascii=False, indent=2)
        
        logger.info(f"é©—è­‰æ•¸æ“šé›†å·²ä¿å­˜: {output_path}")


class FinancialValidationSystem:
    """é‡‘èAIæ¨¡å‹é©—è­‰ç³»çµ±"""
    
    def __init__(
        self,
        reward_model: Optional[FinancialRewardModel] = None,
        validation_dir: str = "./data/validation"
    ):
        self.reward_model = reward_model or create_reward_model("financial")
        self.dataset_manager = ValidationDatasetManager(validation_dir)
        self.validation_history = []
        
        # åŸºæº–åˆ†æ•¸ï¼ˆç”¨æ–¼æ¯”è¼ƒï¼‰
        self.benchmark_scores = {
            'accuracy_score': 0.70,
            'relevance_score': 0.75,
            'risk_awareness_score': 0.65,
            'actionability_score': 0.70,
            'compliance_score': 0.80,
            'overall_score': 0.72
        }
    
    def validate_model(
        self,
        model,
        tokenizer,
        dataset_name: str = 'financial_analysis',
        model_name: str = 'unknown_model',
        validation_config: Optional[Dict[str, Any]] = None
    ) -> ValidationResult:
        """
        åŸ·è¡Œæ¨¡å‹é©—è­‰
        
        Args:
            model: è¦é©—è­‰çš„æ¨¡å‹
            tokenizer: å°æ‡‰çš„tokenizer
            dataset_name: é©—è­‰æ•¸æ“šé›†åç¨±
            model_name: æ¨¡å‹åç¨±
            validation_config: é©—è­‰é…ç½®
            
        Returns:
            ValidationResult: é©—è­‰çµæœ
        """
        logger.info(f"ğŸ” é–‹å§‹é©—è­‰æ¨¡å‹: {model_name} (æ•¸æ“šé›†: {dataset_name})")
        
        # ç²å–é©—è­‰æ•¸æ“šé›†
        validation_data = self.dataset_manager.get_validation_dataset(dataset_name)
        
        # é»˜èªé©—è­‰é…ç½®
        if validation_config is None:
            validation_config = {
                'max_length': 512,
                'temperature': 0.7,
                'do_sample': True,
                'num_return_sequences': 1
            }
        
        # åŸ·è¡Œé©—è­‰
        detailed_scores = []
        all_metrics = []
        performance_metrics = []
        
        model.eval()
        
        for i, item in enumerate(validation_data):
            logger.info(f"è™•ç†é©—è­‰æ¨£æœ¬ {i+1}/{len(validation_data)}")
            
            query = item['query']
            expected_response = item['expected_response']
            context = item.get('context', {})
            evaluation_criteria = item.get('evaluation_criteria', {})
            
            # ç”Ÿæˆæ¨¡å‹å›æ‡‰ä¸¦æ¸¬é‡æ€§èƒ½
            generated_response, performance = self._generate_with_performance_tracking(
                model, tokenizer, query, validation_config
            )
            
            # è¨ˆç®—è©•ä¼°åˆ†æ•¸
            scores = self._compute_detailed_scores(
                query, generated_response, expected_response, context, evaluation_criteria
            )
            
            # è¨˜éŒ„è©³ç´°çµæœ
            detailed_result = {
                'sample_id': i + 1,
                'query': query,
                'expected_response': expected_response,
                'generated_response': generated_response,
                'context': context,
                'scores': scores,
                'performance': performance
            }
            
            detailed_scores.append(detailed_result)
            all_metrics.append(scores)
            performance_metrics.append(performance)
        
        # è¨ˆç®—å¹³å‡æŒ‡æ¨™
        avg_metrics = self._compute_average_metrics(all_metrics, performance_metrics)
        
        # èˆ‡åŸºæº–æ¯”è¼ƒ
        benchmark_comparison = self._compare_with_benchmark(avg_metrics)
        
        # ç”Ÿæˆå»ºè­°
        recommendations = self._generate_validation_recommendations(avg_metrics, detailed_scores)
        
        # å‰µå»ºé©—è­‰çµæœ
        result = ValidationResult(
            model_name=model_name,
            validation_date=datetime.now().isoformat(),
            dataset_name=dataset_name,
            sample_count=len(validation_data),
            metrics=avg_metrics,
            detailed_scores=detailed_scores,
            benchmark_comparison=benchmark_comparison,
            recommendations=recommendations,
            validation_config=validation_config
        )
        
        # æ·»åŠ åˆ°æ­·å²è¨˜éŒ„
        self.validation_history.append(result)
        
        logger.info(f"âœ… é©—è­‰å®Œæˆ - ç¸½é«”åˆ†æ•¸: {avg_metrics.overall_score:.3f}")
        
        return result
    
    def _generate_with_performance_tracking(
        self,
        model,
        tokenizer,
        query: str,
        config: Dict[str, Any]
    ) -> Tuple[str, Dict[str, float]]:
        """ç”Ÿæˆå›æ‡‰ä¸¦è¿½è¹¤æ€§èƒ½æŒ‡æ¨™"""
        
        import time
        import psutil
        import os
        
        # è¨˜éŒ„é–‹å§‹ç‹€æ…‹
        start_time = time.time()
        process = psutil.Process(os.getpid())
        start_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # ç·¨ç¢¼è¼¸å…¥
        inputs = tokenizer(
            query,
            return_tensors="pt",
            max_length=256,
            truncation=True
        )
        
        if torch.cuda.is_available():
            inputs = {k: v.cuda() for k, v in inputs.items()}
        
        # ç”Ÿæˆå›æ‡‰
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=config.get('max_length', 512),
                temperature=config.get('temperature', 0.7),
                do_sample=config.get('do_sample', True),
                num_return_sequences=config.get('num_return_sequences', 1),
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        
        # è§£ç¢¼å›æ‡‰
        response = tokenizer.decode(
            outputs[0][inputs['input_ids'].shape[1]:],
            skip_special_tokens=True
        ).strip()
        
        # è¨ˆç®—æ€§èƒ½æŒ‡æ¨™
        end_time = time.time()
        end_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        inference_time = end_time - start_time
        tokens_generated = len(outputs[0]) - inputs['input_ids'].shape[1]
        tokens_per_second = tokens_generated / inference_time if inference_time > 0 else 0
        memory_usage = end_memory - start_memory
        
        performance = {
            'inference_time': inference_time,
            'tokens_per_second': tokens_per_second,
            'memory_usage_mb': memory_usage,
            'tokens_generated': tokens_generated
        }
        
        return response, performance
    
    def _compute_detailed_scores(
        self,
        query: str,
        generated_response: str,
        expected_response: str,
        context: Dict[str, Any],
        evaluation_criteria: Dict[str, float]
    ) -> Dict[str, float]:
        """è¨ˆç®—è©³ç´°è©•ä¼°åˆ†æ•¸"""
        
        # ä½¿ç”¨çå‹µæ¨¡å‹è¨ˆç®—åŸºç¤åˆ†æ•¸
        reward_components = self.reward_model._compute_reward_components(
            query, generated_response, context
        )
        
        # è¨ˆç®—èˆ‡æœŸæœ›å›æ‡‰çš„ç›¸ä¼¼åº¦
        similarity_score = self._compute_response_similarity(expected_response, generated_response)
        
        # è¨ˆç®—èªè¨€å“è³ªåˆ†æ•¸
        fluency_score = self._evaluate_fluency(generated_response)
        coherence_score = self._evaluate_coherence(generated_response)
        informativeness_score = self._evaluate_informativeness(generated_response)
        
        # è¨ˆç®—é‡‘èå°ˆæ¥­åˆ†æ•¸
        financial_accuracy = self._evaluate_financial_accuracy(generated_response, context)
        risk_assessment_quality = self._evaluate_risk_assessment_quality(generated_response)
        investment_logic_score = self._evaluate_investment_logic(generated_response)
        
        # çµ±è¨ˆæŒ‡æ¨™
        response_length = len(generated_response)
        financial_terms_count = self._count_financial_terms(generated_response)
        financial_terms_density = financial_terms_count / len(generated_response.split()) if generated_response else 0
        risk_mentions = self._count_risk_mentions(generated_response)
        risk_mentions_ratio = risk_mentions / len(generated_response.split()) if generated_response else 0
        compliance_mentions = self._count_compliance_mentions(generated_response)
        compliance_coverage = min(compliance_mentions / 2, 1.0)  # å‡è¨­éœ€è¦è‡³å°‘2å€‹åˆè¦æåŠ
        
        return {
            'accuracy_score': reward_components.accuracy_score,
            'relevance_score': reward_components.relevance_score,
            'risk_awareness_score': reward_components.risk_awareness_score,
            'actionability_score': reward_components.actionability_score,
            'compliance_score': reward_components.compliance_score,
            'similarity_score': similarity_score,
            'fluency_score': fluency_score,
            'coherence_score': coherence_score,
            'informativeness_score': informativeness_score,
            'financial_accuracy': financial_accuracy,
            'risk_assessment_quality': risk_assessment_quality,
            'investment_logic_score': investment_logic_score,
            'response_length': response_length,
            'financial_terms_density': financial_terms_density,
            'risk_mentions_ratio': risk_mentions_ratio,
            'compliance_coverage': compliance_coverage
        }
    
    def _compute_response_similarity(self, expected: str, generated: str) -> float:
        """è¨ˆç®—å›æ‡‰ç›¸ä¼¼åº¦"""
        expected_words = set(expected.lower().split())
        generated_words = set(generated.lower().split())
        
        if not expected_words and not generated_words:
            return 1.0
        
        intersection = expected_words.intersection(generated_words)
        union = expected_words.union(generated_words)
        
        return len(intersection) / len(union) if union else 0.0
    
    def _evaluate_fluency(self, text: str) -> float:
        """è©•ä¼°èªè¨€æµæš¢åº¦"""
        # ç°¡åŒ–çš„æµæš¢åº¦è©•ä¼°
        score = 0.5
        
        # æª¢æŸ¥å¥å­çµæ§‹
        sentences = text.split('ã€‚')
        if len(sentences) >= 2:
            score += 0.2
        
        # æª¢æŸ¥æ¨™é»ç¬¦è™Ÿä½¿ç”¨
        punctuation_count = sum(1 for char in text if char in 'ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š')
        if punctuation_count >= len(sentences):
            score += 0.2
        
        # æª¢æŸ¥é‡è¤‡è©èª
        words = text.split()
        unique_words = set(words)
        if len(unique_words) / len(words) > 0.8:
            score += 0.1
        
        return min(score, 1.0)
    
    def _evaluate_coherence(self, text: str) -> float:
        """è©•ä¼°é‚è¼¯é€£è²«æ€§"""
        score = 0.5
        
        # æª¢æŸ¥é‚è¼¯é€£æ¥è©
        logical_connectors = ['å› ç‚º', 'æ‰€ä»¥', 'ä½†æ˜¯', 'ç„¶è€Œ', 'æ­¤å¤–', 'å¦å¤–', 'å› æ­¤', 'ç”±æ–¼']
        connector_count = sum(1 for connector in logical_connectors if connector in text)
        if connector_count >= 2:
            score += 0.3
        elif connector_count >= 1:
            score += 0.2
        
        # æª¢æŸ¥ä¸»é¡Œä¸€è‡´æ€§ï¼ˆç°¡åŒ–ç‰ˆï¼‰
        if len(text) > 50 and not any(word in text for word in ['çªç„¶', 'å¿½ç„¶', 'è«å']):
            score += 0.2
        
        return min(score, 1.0)
    
    def _evaluate_informativeness(self, text: str) -> float:
        """è©•ä¼°ä¿¡æ¯è±å¯Œåº¦"""
        score = 0.3
        
        # æª¢æŸ¥å…·é«”æ•¸æ“š
        import re
        numbers = re.findall(r'\d+', text)
        if len(numbers) >= 3:
            score += 0.3
        elif len(numbers) >= 1:
            score += 0.2
        
        # æª¢æŸ¥å…·é«”äº‹å¯¦
        fact_indicators = ['æ ¹æ“š', 'æ•¸æ“šé¡¯ç¤º', 'ç ”ç©¶è¡¨æ˜', 'çµ±è¨ˆ', 'å ±å‘Š', 'åˆ†æå¸«']
        fact_count = sum(1 for indicator in fact_indicators if indicator in text)
        if fact_count >= 2:
            score += 0.2
        elif fact_count >= 1:
            score += 0.1
        
        # æª¢æŸ¥è©³ç´°ç¨‹åº¦
        if len(text) >= 100:
            score += 0.2
        elif len(text) >= 50:
            score += 0.1
        
        return min(score, 1.0)
    
    def _evaluate_financial_accuracy(self, text: str, context: Dict[str, Any]) -> float:
        """è©•ä¼°é‡‘èæº–ç¢ºæ€§"""
        score = 0.5
        
        # æª¢æŸ¥æ˜¯å¦æåŠç›¸é—œå…¬å¸/è‚¡ç¥¨
        if 'stock_code' in context:
            stock_code = context['stock_code']
            if stock_code in text:
                score += 0.2
        
        if 'company' in context:
            company = context['company']
            if company in text:
                score += 0.2
        
        # æª¢æŸ¥è¡Œæ¥­ç›¸é—œè¡“èª
        if 'sector' in context:
            sector = context['sector']
            sector_terms = {
                'åŠå°é«”': ['æ™¶ç‰‡', 'è£½ç¨‹', 'ä»£å·¥', 'å°æ¸¬'],
                'é‡‘è': ['éŠ€è¡Œ', 'ä¿éšª', 'è­‰åˆ¸', 'åˆ©ç‡'],
                'é›»å­': ['ä»£å·¥', 'çµ„è£', 'ä¾›æ‡‰éˆ']
            }
            
            if sector in sector_terms:
                relevant_terms = sum(1 for term in sector_terms[sector] if term in text)
                if relevant_terms >= 2:
                    score += 0.1
        
        return min(score, 1.0)
    
    def _evaluate_risk_assessment_quality(self, text: str) -> float:
        """è©•ä¼°é¢¨éšªè©•ä¼°å“è³ª"""
        score = 0.3
        
        # æª¢æŸ¥é¢¨éšªé¡å‹æåŠ
        risk_types = ['å¸‚å ´é¢¨éšª', 'æµå‹•æ€§é¢¨éšª', 'ä¿¡ç”¨é¢¨éšª', 'æ“ä½œé¢¨éšª', 'æ”¿ç­–é¢¨éšª', 'åŒ¯ç‡é¢¨éšª']
        risk_type_mentions = sum(1 for risk_type in risk_types if risk_type in text)
        
        if risk_type_mentions >= 2:
            score += 0.3
        elif risk_type_mentions >= 1:
            score += 0.2
        
        # æª¢æŸ¥é¢¨éšªé‡åŒ–
        quantitative_terms = ['æ³¢å‹•', 'ä¸‹è·Œ', 'æå¤±', 'æ©Ÿç‡', 'å¯èƒ½æ€§']
        quant_mentions = sum(1 for term in quantitative_terms if term in text)
        
        if quant_mentions >= 2:
            score += 0.2
        elif quant_mentions >= 1:
            score += 0.1
        
        # æª¢æŸ¥é¢¨éšªç®¡ç†å»ºè­°
        management_terms = ['åˆ†æ•£', 'åœæ', 'æ§åˆ¶', 'ç®¡ç†', 'é™ä½']
        mgmt_mentions = sum(1 for term in management_terms if term in text)
        
        if mgmt_mentions >= 2:
            score += 0.2
        elif mgmt_mentions >= 1:
            score += 0.1
        
        return min(score, 1.0)
    
    def _evaluate_investment_logic(self, text: str) -> float:
        """è©•ä¼°æŠ•è³‡é‚è¼¯"""
        score = 0.4
        
        # æª¢æŸ¥åˆ†ææ¡†æ¶
        analysis_terms = ['åŸºæœ¬é¢', 'æŠ€è¡“é¢', 'ä¼°å€¼', 'æˆé•·æ€§', 'ç²åˆ©èƒ½åŠ›']
        analysis_mentions = sum(1 for term in analysis_terms if term in text)
        
        if analysis_mentions >= 3:
            score += 0.3
        elif analysis_mentions >= 1:
            score += 0.2
        
        # æª¢æŸ¥æŠ•è³‡å»ºè­°é‚è¼¯
        logic_indicators = ['å› ç‚º', 'ç”±æ–¼', 'åŸºæ–¼', 'è€ƒæ…®åˆ°', 'é‘‘æ–¼']
        logic_count = sum(1 for indicator in logic_indicators if indicator in text)
        
        if logic_count >= 2:
            score += 0.2
        elif logic_count >= 1:
            score += 0.1
        
        # æª¢æŸ¥æ™‚é–“æ¡†æ¶
        time_frames = ['çŸ­æœŸ', 'ä¸­æœŸ', 'é•·æœŸ', 'æœªä¾†', 'ç›®å‰']
        time_mentions = sum(1 for frame in time_frames if frame in text)
        
        if time_mentions >= 2:
            score += 0.1
        
        return min(score, 1.0)
    
    def _count_financial_terms(self, text: str) -> int:
        """è¨ˆç®—é‡‘èè¡“èªæ•¸é‡"""
        financial_terms = [
            'æŠ•è³‡', 'è‚¡ç¥¨', 'åŸºé‡‘', 'å‚µåˆ¸', 'é¢¨éšª', 'å ±é…¬', 'ç²åˆ©', 'è™§æ',
            'å¸‚å ´', 'ç¶“æ¿Ÿ', 'é‡‘è', 'éŠ€è¡Œ', 'ä¿éšª', 'è­‰åˆ¸', 'æœŸè²¨', 'é¸æ“‡æ¬Š',
            'æœ¬ç›Šæ¯”', 'è‚¡æ¯', 'æ®–åˆ©ç‡', 'ç¾é‡‘æµ', 'è³‡ç”¢', 'è² å‚µ', 'ç‡Ÿæ”¶', 'æ·¨åˆ©',
            'æˆé•·', 'åƒ¹å€¼', 'å‹•èƒ½', 'è¶¨å‹¢', 'æ”¯æ’', 'å£“åŠ›', 'çªç ´', 'æ•´ç†'
        ]
        
        return sum(1 for term in financial_terms if term in text)
    
    def _count_risk_mentions(self, text: str) -> int:
        """è¨ˆç®—é¢¨éšªæåŠæ¬¡æ•¸"""
        risk_terms = ['é¢¨éšª', 'ä¸ç¢ºå®š', 'æ³¢å‹•', 'è¬¹æ…', 'å°å¿ƒ', 'æ³¨æ„', 'è­¦å‘Š', 'å±éšª']
        return sum(1 for term in risk_terms if term in text)
    
    def _count_compliance_mentions(self, text: str) -> int:
        """è¨ˆç®—åˆè¦æåŠæ¬¡æ•¸"""
        compliance_terms = ['åƒ…ä¾›åƒè€ƒ', 'æŠ•è³‡æœ‰é¢¨éšª', 'è«‹è¬¹æ…', 'éæŠ•è³‡å»ºè­°', 'å°ˆæ¥­è«®è©¢', 'é¢¨éšªè‡ªè² ']
        return sum(1 for term in compliance_terms if term in text)
    
    def _compute_average_metrics(
        self,
        all_scores: List[Dict[str, float]],
        performance_metrics: List[Dict[str, float]]
    ) -> ValidationMetrics:
        """è¨ˆç®—å¹³å‡æŒ‡æ¨™"""
        
        # è¨ˆç®—å„é …å¹³å‡åˆ†æ•¸
        avg_scores = {}
        for key in all_scores[0].keys():
            if key != 'response_length':  # é•·åº¦å–®ç¨è™•ç†
                avg_scores[key] = np.mean([scores[key] for scores in all_scores])
        
        # è¨ˆç®—é•·åº¦çµ±è¨ˆ
        lengths = [scores['response_length'] for scores in all_scores]
        response_length_avg = np.mean(lengths)
        response_length_std = np.std(lengths)
        
        # è¨ˆç®—æ€§èƒ½å¹³å‡å€¼
        avg_performance = {}
        for key in performance_metrics[0].keys():
            avg_performance[key] = np.mean([perf[key] for perf in performance_metrics])
        
        # è¨ˆç®—ç¸½é«”åˆ†æ•¸
        core_metrics = ['accuracy_score', 'relevance_score', 'risk_awareness_score', 
                       'actionability_score', 'compliance_score']
        overall_score = np.mean([avg_scores[metric] for metric in core_metrics])
        
        return ValidationMetrics(
            accuracy_score=avg_scores['accuracy_score'],
            relevance_score=avg_scores['relevance_score'],
            risk_awareness_score=avg_scores['risk_awareness_score'],
            actionability_score=avg_scores['actionability_score'],
            compliance_score=avg_scores['compliance_score'],
            overall_score=overall_score,
            fluency_score=avg_scores['fluency_score'],
            coherence_score=avg_scores['coherence_score'],
            informativeness_score=avg_scores['informativeness_score'],
            financial_accuracy=avg_scores['financial_accuracy'],
            risk_assessment_quality=avg_scores['risk_assessment_quality'],
            investment_logic_score=avg_scores['investment_logic_score'],
            response_length_avg=response_length_avg,
            response_length_std=response_length_std,
            financial_terms_density=avg_scores['financial_terms_density'],
            risk_mentions_ratio=avg_scores['risk_mentions_ratio'],
            compliance_coverage=avg_scores['compliance_coverage'],
            inference_time_avg=avg_performance['inference_time'],
            tokens_per_second=avg_performance['tokens_per_second'],
            memory_usage_mb=avg_performance['memory_usage_mb']
        )
    
    def _compare_with_benchmark(self, metrics: ValidationMetrics) -> Dict[str, float]:
        """èˆ‡åŸºæº–åˆ†æ•¸æ¯”è¼ƒ"""
        comparison = {}
        
        for key, benchmark_value in self.benchmark_scores.items():
            current_value = getattr(metrics, key)
            improvement = current_value - benchmark_value
            comparison[key] = {
                'current': current_value,
                'benchmark': benchmark_value,
                'improvement': improvement,
                'improvement_percent': (improvement / benchmark_value * 100) if benchmark_value > 0 else 0
            }
        
        return comparison
    
    def _generate_validation_recommendations(
        self,
        metrics: ValidationMetrics,
        detailed_scores: List[Dict[str, Any]]
    ) -> List[str]:
        """ç”Ÿæˆé©—è­‰å»ºè­°"""
        recommendations = []
        
        # åŸºæ–¼åˆ†æ•¸çš„å»ºè­°
        if metrics.accuracy_score < 0.7:
            recommendations.append("æº–ç¢ºæ€§éœ€è¦æ”¹é€²ï¼šå»ºè­°å¢åŠ æ›´å¤šé«˜å“è³ªè¨“ç·´æ•¸æ“šï¼Œç‰¹åˆ¥æ˜¯é‡‘èå°ˆæ¥­å…§å®¹")
        
        if metrics.risk_awareness_score < 0.6:
            recommendations.append("é¢¨éšªæ„è­˜ä¸è¶³ï¼šéœ€è¦å¼·åŒ–é¢¨éšªæç¤ºå’Œè­¦å‘Šçš„è¨“ç·´")
        
        if metrics.compliance_score < 0.8:
            recommendations.append("åˆè¦æ€§éœ€è¦æå‡ï¼šç¢ºä¿è¼¸å‡ºåŒ…å«é©ç•¶çš„å…è²¬è²æ˜")
        
        if metrics.fluency_score < 0.7:
            recommendations.append("èªè¨€æµæš¢åº¦æœ‰å¾…æ”¹å–„ï¼šå»ºè­°æ”¹é€²èªè¨€æ¨¡å‹çš„è¡¨é”èƒ½åŠ›")
        
        if metrics.financial_accuracy < 0.7:
            recommendations.append("é‡‘èå°ˆæ¥­æº–ç¢ºæ€§ä¸è¶³ï¼šéœ€è¦æ›´å¤šé ˜åŸŸå°ˆæ¥­çŸ¥è­˜è¨“ç·´")
        
        # åŸºæ–¼æ€§èƒ½çš„å»ºè­°
        if metrics.inference_time_avg > 2.0:
            recommendations.append("æ¨ç†é€Ÿåº¦è¼ƒæ…¢ï¼šè€ƒæ…®æ¨¡å‹å„ªåŒ–æˆ–ç¡¬é«”å‡ç´š")
        
        if metrics.tokens_per_second < 10:
            recommendations.append("ç”Ÿæˆæ•ˆç‡åä½ï¼šå»ºè­°æª¢æŸ¥æ¨¡å‹é…ç½®å’Œç¡¬é«”æ€§èƒ½")
        
        # åŸºæ–¼çµ±è¨ˆçš„å»ºè­°
        if metrics.response_length_avg < 50:
            recommendations.append("å›æ‡‰éæ–¼ç°¡çŸ­ï¼šè¨“ç·´æ¨¡å‹ç”Ÿæˆæ›´è©³ç´°çš„å›æ‡‰")
        
        if metrics.financial_terms_density < 0.1:
            recommendations.append("é‡‘èè¡“èªä½¿ç”¨ä¸è¶³ï¼šå¢åŠ å°ˆæ¥­è¡“èªçš„ä½¿ç”¨è¨“ç·´")
        
        return recommendations
    
    def generate_validation_report(
        self,
        result: ValidationResult,
        output_dir: str,
        include_plots: bool = True
    ):
        """ç”Ÿæˆé©—è­‰å ±å‘Š"""
        
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ç”ŸæˆJSONå ±å‘Š
        report_data = {
            'model_name': result.model_name,
            'validation_date': result.validation_date,
            'dataset_name': result.dataset_name,
            'sample_count': result.sample_count,
            'metrics': result.metrics.to_dict(),
            'benchmark_comparison': result.benchmark_comparison,
            'recommendations': result.recommendations,
            'validation_config': result.validation_config,
            'detailed_scores': result.detailed_scores[:3]  # åªä¿å­˜å‰3å€‹è©³ç´°çµæœ
        }
        
        with open(output_dir / "validation_report.json", 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
        
        # ç”Ÿæˆåœ–è¡¨
        if include_plots:
            self._generate_validation_plots(result, output_dir)
        
        # ç”ŸæˆMarkdownå ±å‘Š
        self._generate_validation_markdown_report(result, output_dir)
        
        logger.info(f"é©—è­‰å ±å‘Šå·²ç”Ÿæˆ: {output_dir}")
    
    def _generate_validation_plots(self, result: ValidationResult, output_dir: Path):
        """ç”Ÿæˆé©—è­‰åœ–è¡¨"""
        
        # è¨­ç½®ä¸­æ–‡å­—é«”
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS']
        plt.rcParams['axes.unicode_minus'] = False
        
        # 1. æ ¸å¿ƒæŒ‡æ¨™é›·é”åœ–
        fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(projection='polar'))
        
        metrics_names = ['æº–ç¢ºæ€§', 'ç›¸é—œæ€§', 'é¢¨éšªæ„è­˜', 'å¯æ“ä½œæ€§', 'åˆè¦æ€§']
        metrics_values = [
            result.metrics.accuracy_score,
            result.metrics.relevance_score,
            result.metrics.risk_awareness_score,
            result.metrics.actionability_score,
            result.metrics.compliance_score
        ]
        
        angles = np.linspace(0, 2 * np.pi, len(metrics_names), endpoint=False)
        metrics_values += metrics_values[:1]
        angles = np.concatenate((angles, [angles[0]]))
        
        ax.plot(angles, metrics_values, 'o-', linewidth=2, label='ç•¶å‰æ¨¡å‹')
        ax.fill(angles, metrics_values, alpha=0.25)
        
        # æ·»åŠ åŸºæº–ç·š
        benchmark_values = [
            self.benchmark_scores['accuracy_score'],
            self.benchmark_scores['relevance_score'],
            self.benchmark_scores['risk_awareness_score'],
            self.benchmark_scores['actionability_score'],
            self.benchmark_scores['compliance_score']
        ]
        benchmark_values += benchmark_values[:1]
        
        ax.plot(angles, benchmark_values, '--', linewidth=2, label='åŸºæº–ç·š', alpha=0.7)
        
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(metrics_names)
        ax.set_ylim(0, 1)
        ax.set_title(f'{result.model_name} é©—è­‰çµæœ', size=16, pad=20)
        ax.legend()
        
        plt.savefig(output_dir / "validation_radar.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. è©³ç´°æŒ‡æ¨™å°æ¯”åœ–
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # èªè¨€å“è³ªæŒ‡æ¨™
        language_metrics = ['æµæš¢åº¦', 'é€£è²«æ€§', 'ä¿¡æ¯é‡']
        language_scores = [
            result.metrics.fluency_score,
            result.metrics.coherence_score,
            result.metrics.informativeness_score
        ]
        
        axes[0, 0].bar(language_metrics, language_scores, color='skyblue', alpha=0.7)
        axes[0, 0].set_title('èªè¨€å“è³ªæŒ‡æ¨™')
        axes[0, 0].set_ylim(0, 1)
        
        # é‡‘èå°ˆæ¥­æŒ‡æ¨™
        financial_metrics = ['é‡‘èæº–ç¢ºæ€§', 'é¢¨éšªè©•ä¼°', 'æŠ•è³‡é‚è¼¯']
        financial_scores = [
            result.metrics.financial_accuracy,
            result.metrics.risk_assessment_quality,
            result.metrics.investment_logic_score
        ]
        
        axes[0, 1].bar(financial_metrics, financial_scores, color='lightgreen', alpha=0.7)
        axes[0, 1].set_title('é‡‘èå°ˆæ¥­æŒ‡æ¨™')
        axes[0, 1].set_ylim(0, 1)
        
        # æ€§èƒ½æŒ‡æ¨™
        performance_metrics = ['æ¨ç†æ™‚é–“(ç§’)', 'Token/ç§’', 'è¨˜æ†¶é«”ä½¿ç”¨(MB)']
        performance_values = [
            result.metrics.inference_time_avg,
            result.metrics.tokens_per_second,
            result.metrics.memory_usage_mb / 100  # ç¸®æ”¾ä»¥ä¾¿é¡¯ç¤º
        ]
        
        axes[1, 0].bar(performance_metrics, performance_values, color='orange', alpha=0.7)
        axes[1, 0].set_title('æ€§èƒ½æŒ‡æ¨™')
        
        # çµ±è¨ˆæŒ‡æ¨™
        stats_metrics = ['å¹³å‡é•·åº¦', 'é‡‘èè¡“èªå¯†åº¦', 'é¢¨éšªæåŠæ¯”ä¾‹', 'åˆè¦è¦†è“‹ç‡']
        stats_values = [
            result.metrics.response_length_avg / 100,  # ç¸®æ”¾
            result.metrics.financial_terms_density,
            result.metrics.risk_mentions_ratio,
            result.metrics.compliance_coverage
        ]
        
        axes[1, 1].bar(stats_metrics, stats_values, color='purple', alpha=0.7)
        axes[1, 1].set_title('çµ±è¨ˆæŒ‡æ¨™')
        
        plt.tight_layout()
        plt.savefig(output_dir / "detailed_metrics.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    def _generate_validation_markdown_report(self, result: ValidationResult, output_dir: Path):
        """ç”ŸæˆMarkdowné©—è­‰å ±å‘Š"""
        
        markdown_content = f"""# æ¨¡å‹é©—è­‰å ±å‘Š

## åŸºæœ¬ä¿¡æ¯
- **æ¨¡å‹åç¨±**: {result.model_name}
- **é©—è­‰æ—¥æœŸ**: {result.validation_date}
- **é©—è­‰æ•¸æ“šé›†**: {result.dataset_name}
- **æ¨£æœ¬æ•¸é‡**: {result.sample_count}

## æ ¸å¿ƒæŒ‡æ¨™

| æŒ‡æ¨™ | åˆ†æ•¸ | åŸºæº– | æ”¹å–„ | æ”¹å–„ç‡ |
|------|------|------|------|--------|
| æº–ç¢ºæ€§ | {result.metrics.accuracy_score:.3f} | {self.benchmark_scores['accuracy_score']:.3f} | {result.benchmark_comparison['accuracy_score']['improvement']:+.3f} | {result.benchmark_comparison['accuracy_score']['improvement_percent']:+.1f}% |
| ç›¸é—œæ€§ | {result.metrics.relevance_score:.3f} | {self.benchmark_scores['relevance_score']:.3f} | {result.benchmark_comparison['relevance_score']['improvement']:+.3f} | {result.benchmark_comparison['relevance_score']['improvement_percent']:+.1f}% |
| é¢¨éšªæ„è­˜ | {result.metrics.risk_awareness_score:.3f} | {self.benchmark_scores['risk_awareness_score']:.3f} | {result.benchmark_comparison['risk_awareness_score']['improvement']:+.3f} | {result.benchmark_comparison['risk_awareness_score']['improvement_percent']:+.1f}% |
| å¯æ“ä½œæ€§ | {result.metrics.actionability_score:.3f} | {self.benchmark_scores['actionability_score']:.3f} | {result.benchmark_comparison['actionability_score']['improvement']:+.3f} | {result.benchmark_comparison['actionability_score']['improvement_percent']:+.1f}% |
| åˆè¦æ€§ | {result.metrics.compliance_score:.3f} | {self.benchmark_scores['compliance_score']:.3f} | {result.benchmark_comparison['compliance_score']['improvement']:+.3f} | {result.benchmark_comparison['compliance_score']['improvement_percent']:+.1f}% |
| **ç¸½é«”åˆ†æ•¸** | **{result.metrics.overall_score:.3f}** | **{self.benchmark_scores['overall_score']:.3f}** | **{result.benchmark_comparison['overall_score']['improvement']:+.3f}** | **{result.benchmark_comparison['overall_score']['improvement_percent']:+.1f}%** |

## è©³ç´°æŒ‡æ¨™

### èªè¨€å“è³ª
- **æµæš¢åº¦**: {result.metrics.fluency_score:.3f}
- **é€£è²«æ€§**: {result.metrics.coherence_score:.3f}
- **ä¿¡æ¯é‡**: {result.metrics.informativeness_score:.3f}

### é‡‘èå°ˆæ¥­
- **é‡‘èæº–ç¢ºæ€§**: {result.metrics.financial_accuracy:.3f}
- **é¢¨éšªè©•ä¼°å“è³ª**: {result.metrics.risk_assessment_quality:.3f}
- **æŠ•è³‡é‚è¼¯**: {result.metrics.investment_logic_score:.3f}

### æ€§èƒ½æŒ‡æ¨™
- **å¹³å‡æ¨ç†æ™‚é–“**: {result.metrics.inference_time_avg:.2f} ç§’
- **ç”Ÿæˆé€Ÿåº¦**: {result.metrics.tokens_per_second:.1f} tokens/ç§’
- **è¨˜æ†¶é«”ä½¿ç”¨**: {result.metrics.memory_usage_mb:.1f} MB

### çµ±è¨ˆæŒ‡æ¨™
- **å¹³å‡å›æ‡‰é•·åº¦**: {result.metrics.response_length_avg:.0f} å­—ç¬¦ (Â±{result.metrics.response_length_std:.0f})
- **é‡‘èè¡“èªå¯†åº¦**: {result.metrics.financial_terms_density:.3f}
- **é¢¨éšªæåŠæ¯”ä¾‹**: {result.metrics.risk_mentions_ratio:.3f}
- **åˆè¦è¦†è“‹ç‡**: {result.metrics.compliance_coverage:.3f}

## æ”¹é€²å»ºè­°

"""
        
        for i, recommendation in enumerate(result.recommendations, 1):
            markdown_content += f"{i}. {recommendation}\n"
        
        markdown_content += f"""
## é©—è­‰æ¨£æœ¬ç¤ºä¾‹

ä»¥ä¸‹æ˜¯å‰2å€‹é©—è­‰æ¨£æœ¬çš„è©³ç´°çµæœï¼š

"""
        
        for i, sample in enumerate(result.detailed_scores[:2], 1):
            markdown_content += f"""
### æ¨£æœ¬ {i}

**æŸ¥è©¢**: {sample['query']}

**æœŸæœ›å›æ‡‰**: {sample['expected_response'][:100]}...

**ç”Ÿæˆå›æ‡‰**: {sample['generated_response'][:100]}...

**è©•åˆ†**:
- æº–ç¢ºæ€§: {sample['scores']['accuracy_score']:.2f}
- ç›¸é—œæ€§: {sample['scores']['relevance_score']:.2f}
- é¢¨éšªæ„è­˜: {sample['scores']['risk_awareness_score']:.2f}
- å¯æ“ä½œæ€§: {sample['scores']['actionability_score']:.2f}
- åˆè¦æ€§: {sample['scores']['compliance_score']:.2f}

**æ€§èƒ½**:
- æ¨ç†æ™‚é–“: {sample['performance']['inference_time']:.2f}ç§’
- ç”Ÿæˆé€Ÿåº¦: {sample['performance']['tokens_per_second']:.1f} tokens/ç§’

---
"""
        
        with open(output_dir / "validation_report.md", 'w', encoding='utf-8') as f:
            f.write(markdown_content)
    
    def get_validation_history(self) -> List[ValidationResult]:
        """ç²å–é©—è­‰æ­·å²"""
        return self.validation_history.copy()
    
    def compare_validation_results(
        self,
        results: List[ValidationResult]
    ) -> Dict[str, Any]:
        """æ¯”è¼ƒå¤šå€‹é©—è­‰çµæœ"""
        
        if len(results) < 2:
            raise ValueError("éœ€è¦è‡³å°‘2å€‹é©—è­‰çµæœé€²è¡Œæ¯”è¼ƒ")
        
        comparison = {
            'models': [r.model_name for r in results],
            'metrics_comparison': {},
            'best_model': {},
            'performance_trends': {}
        }
        
        # æ¯”è¼ƒæ ¸å¿ƒæŒ‡æ¨™
        core_metrics = ['accuracy_score', 'relevance_score', 'risk_awareness_score',
                       'actionability_score', 'compliance_score', 'overall_score']
        
        for metric in core_metrics:
            scores = [getattr(r.metrics, metric) for r in results]
            best_idx = np.argmax(scores)
            
            comparison['metrics_comparison'][metric] = {
                'scores': scores,
                'best_model': results[best_idx].model_name,
                'best_score': scores[best_idx],
                'improvement_range': max(scores) - min(scores)
            }
        
        # ç¢ºå®šæœ€ä½³æ¨¡å‹
        overall_scores = [r.metrics.overall_score for r in results]
        best_overall_idx = np.argmax(overall_scores)
        
        comparison['best_model'] = {
            'name': results[best_overall_idx].model_name,
            'overall_score': overall_scores[best_overall_idx],
            'validation_date': results[best_overall_idx].validation_date
        }
        
        return comparison


# è‡ªå‹•æª¢æ¸¬å’Œåˆ‡æ›åˆ° TradingAgents ç›®éŒ„
def ensure_tradingagents_directory():
    """ç¢ºä¿ç•¶å‰å·¥ä½œç›®éŒ„åœ¨ TradingAgents/ ä¸‹ï¼Œä»¥æ­£ç¢ºè¨ªå•é…ç½®æ–‡ä»¶"""
    current_dir = Path.cwd()
    
    # å¦‚æœç•¶å‰ç›®éŒ„æ˜¯ TradingAgents çš„çˆ¶ç›®éŒ„ï¼Œåˆ‡æ›åˆ° TradingAgents
    if (current_dir / 'TradingAgents').exists():
        os.chdir(current_dir / 'TradingAgents')
        print(f"[DIR] è‡ªå‹•åˆ‡æ›å·¥ä½œç›®éŒ„åˆ°: {Path.cwd()}")
    
    # é©—è­‰å¿…è¦çš„ç›®éŒ„å­˜åœ¨
    required_dirs = ['configs', 'training', 'tradingagents']
    missing_dirs = [d for d in required_dirs if not Path(d).exists()]
    
    if missing_dirs:
        raise FileNotFoundError(f"ç¼ºå°‘å¿…è¦ç›®éŒ„: {missing_dirs}. è«‹ç¢ºä¿å¾ TradingAgents/ ç›®éŒ„åŸ·è¡Œæ­¤è…³æœ¬")

# ç›®éŒ„æª¢æŸ¥å‡½æ•¸å·²æº–å‚™å¥½ï¼Œä½†ä¸åœ¨æ¨¡çµ„å°å…¥æ™‚è‡ªå‹•åŸ·è¡Œ
# åªåœ¨éœ€è¦æ™‚æ‰‹å‹•èª¿ç”¨ ensure_tradingagents_directory()

