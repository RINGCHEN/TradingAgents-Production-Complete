"""
Advanced Training Configuration and Optimization
é«˜ç´šè¨“ç·´é…ç½®å’Œå„ªåŒ–ç³»çµ±

ä»»å‹™4.4: é«˜ç´šè¨“ç·´é…ç½®å’Œå„ªåŒ–
è² è²¬äºº: å°k (AIè¨“ç·´å°ˆå®¶åœ˜éšŠ)

æä¾›ï¼š
- YAMLé…ç½®æ–‡ä»¶æ”¯æ´
- å‹•æ…‹é…ç½®èª¿æ•´
- è¶…åƒæ•¸å„ªåŒ–
- è¨“ç·´ç­–ç•¥è‡ªå‹•é¸æ“‡
- æ€§èƒ½å„ªåŒ–å»ºè­°
"""

import os
import yaml
import json
import logging
from typing import Dict, List, Optional, Any, Union, Tuple
from dataclasses import dataclass, field, asdict
from pathlib import Path
from datetime import datetime
import numpy as np
import torch

from .training_config import TrainingConfig, GRPOConfig, PPOConfig

logger = logging.getLogger(__name__)


@dataclass
class HyperparameterRange:
    """è¶…åƒæ•¸ç¯„åœå®šç¾©"""
    min_value: float
    max_value: float
    step: Optional[float] = None
    scale: str = "linear"  # "linear", "log", "categorical"
    values: Optional[List[Any]] = None  # ç”¨æ–¼categoricalé¡å‹


@dataclass
class OptimizationConfig:
    """å„ªåŒ–é…ç½®"""
    method: str = "grid_search"  # "grid_search", "random_search", "bayesian"
    max_trials: int = 20
    timeout_minutes: int = 60
    early_stopping_patience: int = 3
    optimization_metric: str = "overall_score"
    minimize: bool = False  # Falseè¡¨ç¤ºæœ€å¤§åŒ–æŒ‡æ¨™


@dataclass
class AdvancedTrainingConfig(TrainingConfig):
    """é«˜ç´šè¨“ç·´é…ç½®"""
    
    # è¶…åƒæ•¸å„ªåŒ–é…ç½®
    hyperparameter_optimization: bool = False
    optimization_config: OptimizationConfig = field(default_factory=OptimizationConfig)
    hyperparameter_ranges: Dict[str, HyperparameterRange] = field(default_factory=dict)
    
    # å‹•æ…‹èª¿æ•´é…ç½®
    dynamic_adjustment: bool = False
    adjustment_frequency: int = 100  # æ¯å¤šå°‘æ­¥æª¢æŸ¥ä¸€æ¬¡
    adjustment_metrics: List[str] = field(default_factory=lambda: ["loss", "accuracy"])
    
    # é«˜ç´šå„ªåŒ–ç­–ç•¥
    mixed_precision_level: str = "O1"  # "O0", "O1", "O2", "O3"
    gradient_compression: bool = False
    gradient_clipping_type: str = "norm"  # "norm", "value"
    
    # å­¸ç¿’ç‡èª¿åº¦ç­–ç•¥
    lr_scheduler_type: str = "cosine"  # "cosine", "linear", "polynomial", "constant"
    lr_warmup_type: str = "linear"  # "linear", "constant"
    lr_scheduler_params: Dict[str, Any] = field(default_factory=dict)
    
    # æ•¸æ“šå¢å¼·é…ç½®
    data_augmentation: bool = False
    augmentation_strategies: List[str] = field(default_factory=list)
    augmentation_probability: float = 0.5
    
    # æ­£å‰‡åŒ–é…ç½®
    dropout_rate: float = 0.1
    layer_dropout: bool = False
    attention_dropout: float = 0.1
    
    # æ¨¡å‹æ¶æ§‹å„ªåŒ–
    model_parallelism: bool = False
    pipeline_parallelism: bool = False
    tensor_parallelism: bool = False
    
    # è¨˜æ†¶é«”å„ªåŒ–
    activation_checkpointing: bool = True
    cpu_offload: bool = False
    zero_optimization_stage: int = 1  # 0, 1, 2, 3
    
    # å¯¦é©—è¿½è¹¤é…ç½®
    experiment_tracking: Dict[str, Any] = field(default_factory=dict)
    
    # è‡ªå‹•åŒ–é…ç½®
    auto_config: bool = False
    auto_config_target: str = "performance"  # "performance", "memory", "speed"
    
    def __post_init__(self):
        super().__post_init__()
        self._setup_advanced_defaults()
        self._validate_advanced_config()
    
    def _setup_advanced_defaults(self):
        """è¨­ç½®é«˜ç´šé…ç½®é»˜èªå€¼"""
        
        # è¨­ç½®é»˜èªè¶…åƒæ•¸ç¯„åœ
        if not self.hyperparameter_ranges and self.hyperparameter_optimization:
            self.hyperparameter_ranges = {
                'learning_rate': HyperparameterRange(1e-6, 1e-3, scale="log"),
                'batch_size': HyperparameterRange(2, 16, values=[2, 4, 8, 16]),
                'warmup_steps': HyperparameterRange(50, 500, step=50),
                'weight_decay': HyperparameterRange(0.0, 0.1, step=0.01)
            }
        
        # è¨­ç½®é»˜èªå­¸ç¿’ç‡èª¿åº¦åƒæ•¸
        if not self.lr_scheduler_params:
            if self.lr_scheduler_type == "cosine":
                self.lr_scheduler_params = {
                    'T_max': self.max_steps if self.max_steps > 0 else 1000,
                    'eta_min': self.learning_rate * 0.1
                }
            elif self.lr_scheduler_type == "polynomial":
                self.lr_scheduler_params = {
                    'power': 1.0,
                    'total_iters': self.max_steps if self.max_steps > 0 else 1000
                }
        
        # è¨­ç½®é»˜èªå¯¦é©—è¿½è¹¤
        if not self.experiment_tracking:
            self.experiment_tracking = {
                'enabled': False,
                'project_name': 'financial_ai_training',
                'experiment_name': f'training_{datetime.now().strftime("%Y%m%d_%H%M%S")}',
                'tags': ['financial', 'ai', self.training_type],
                'log_frequency': 10
            }
    
    def _validate_advanced_config(self):
        """é©—è­‰é«˜ç´šé…ç½®"""
        
        # é©—è­‰æ··åˆç²¾åº¦é…ç½®
        valid_precision_levels = ["O0", "O1", "O2", "O3"]
        if self.mixed_precision_level not in valid_precision_levels:
            raise ValueError(f"Invalid mixed_precision_level: {self.mixed_precision_level}")
        
        # é©—è­‰å­¸ç¿’ç‡èª¿åº¦å™¨
        valid_schedulers = ["cosine", "linear", "polynomial", "constant"]
        if self.lr_scheduler_type not in valid_schedulers:
            raise ValueError(f"Invalid lr_scheduler_type: {self.lr_scheduler_type}")
        
        # é©—è­‰ZeROå„ªåŒ–éšæ®µ
        if not 0 <= self.zero_optimization_stage <= 3:
            raise ValueError("zero_optimization_stage must be between 0 and 3")
        
        # é©—è­‰dropoutç‡
        if not 0 <= self.dropout_rate <= 1:
            raise ValueError("dropout_rate must be between 0 and 1")


class AdvancedConfigManager:
    """é«˜ç´šé…ç½®ç®¡ç†å™¨"""
    
    def __init__(self, config_dir: str = "./configs/advanced"):
        self.config_dir = Path(config_dir)
        self.config_dir.mkdir(parents=True, exist_ok=True)
        
        # é å®šç¾©é…ç½®æ¨¡æ¿
        self.config_templates = {
            'performance_optimized': self._create_performance_template(),
            'memory_optimized': self._create_memory_template(),
            'speed_optimized': self._create_speed_template(),
            'research_optimized': self._create_research_template()
        }
    
    def _create_performance_template(self) -> Dict[str, Any]:
        """å‰µå»ºæ€§èƒ½å„ªåŒ–æ¨¡æ¿"""
        return {
            'training_type': 'grpo',
            'batch_size': 8,
            'gradient_accumulation_steps': 4,
            'learning_rate': 3e-5,
            'mixed_precision_level': 'O2',
            'gradient_checkpointing': True,
            'lr_scheduler_type': 'cosine',
            'zero_optimization_stage': 2,
            'activation_checkpointing': True,
            'dynamic_adjustment': True,
            'adjustment_frequency': 50,
            'experiment_tracking': {
                'enabled': True,
                'log_frequency': 10
            }
        }
    
    def _create_memory_template(self) -> Dict[str, Any]:
        """å‰µå»ºè¨˜æ†¶é«”å„ªåŒ–æ¨¡æ¿"""
        return {
            'training_type': 'grpo',
            'batch_size': 2,
            'gradient_accumulation_steps': 16,
            'learning_rate': 5e-5,
            'mixed_precision_level': 'O3',
            'gradient_checkpointing': True,
            'cpu_offload': True,
            'zero_optimization_stage': 3,
            'activation_checkpointing': True,
            'model_parallelism': True,
            'experiment_tracking': {
                'enabled': True,
                'log_frequency': 20
            }
        }
    
    def _create_speed_template(self) -> Dict[str, Any]:
        """å‰µå»ºé€Ÿåº¦å„ªåŒ–æ¨¡æ¿"""
        return {
            'training_type': 'ppo',
            'batch_size': 16,
            'gradient_accumulation_steps': 2,
            'learning_rate': 1e-4,
            'mixed_precision_level': 'O1',
            'gradient_checkpointing': False,
            'zero_optimization_stage': 1,
            'activation_checkpointing': False,
            'lr_scheduler_type': 'linear',
            'experiment_tracking': {
                'enabled': True,
                'log_frequency': 5
            }
        }
    
    def _create_research_template(self) -> Dict[str, Any]:
        """å‰µå»ºç ”ç©¶å„ªåŒ–æ¨¡æ¿"""
        return {
            'training_type': 'grpo',
            'batch_size': 4,
            'gradient_accumulation_steps': 8,
            'learning_rate': 5e-5,
            'hyperparameter_optimization': True,
            'optimization_config': {
                'method': 'bayesian',
                'max_trials': 50,
                'timeout_minutes': 120
            },
            'dynamic_adjustment': True,
            'data_augmentation': True,
            'experiment_tracking': {
                'enabled': True,
                'log_frequency': 1
            }
        }
    
    def create_config_from_template(
        self,
        template_name: str,
        overrides: Optional[Dict[str, Any]] = None
    ) -> AdvancedTrainingConfig:
        """å¾æ¨¡æ¿å‰µå»ºé…ç½®"""
        
        if template_name not in self.config_templates:
            raise ValueError(f"Unknown template: {template_name}")
        
        # ç²å–æ¨¡æ¿é…ç½®
        template_config = self.config_templates[template_name].copy()
        
        # æ‡‰ç”¨è¦†è“‹è¨­ç½®
        if overrides:
            template_config.update(overrides)
        
        return AdvancedTrainingConfig(**template_config)
    
    def save_config_yaml(self, config: AdvancedTrainingConfig, filename: str):
        """ä¿å­˜é…ç½®ç‚ºYAMLæ–‡ä»¶"""
        
        config_path = self.config_dir / filename
        config_dict = asdict(config)
        
        # è™•ç†ç‰¹æ®Šå°è±¡
        config_dict = self._serialize_config(config_dict)
        
        with open(config_path, 'w', encoding='utf-8') as f:
            yaml.dump(config_dict, f, default_flow_style=False, allow_unicode=True, indent=2)
        
        logger.info(f"é…ç½®å·²ä¿å­˜åˆ°: {config_path}")
    
    def load_config_yaml(self, filename: str) -> AdvancedTrainingConfig:
        """å¾YAMLæ–‡ä»¶è¼‰å…¥é…ç½®"""
        
        config_path = self.config_dir / filename
        
        if not config_path.exists():
            raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        
        with open(config_path, 'r', encoding='utf-8') as f:
            config_dict = yaml.safe_load(f)
        
        # ååºåˆ—åŒ–ç‰¹æ®Šå°è±¡
        config_dict = self._deserialize_config(config_dict)
        
        return AdvancedTrainingConfig(**config_dict)
    
    def _serialize_config(self, config_dict: Dict[str, Any]) -> Dict[str, Any]:
        """åºåˆ—åŒ–é…ç½®å­—å…¸"""
        
        # è™•ç†HyperparameterRangeå°è±¡
        if 'hyperparameter_ranges' in config_dict:
            ranges = config_dict['hyperparameter_ranges']
            for key, range_obj in ranges.items():
                if hasattr(range_obj, '__dict__'):
                    ranges[key] = asdict(range_obj)
        
        # è™•ç†OptimizationConfigå°è±¡
        if 'optimization_config' in config_dict:
            opt_config = config_dict['optimization_config']
            if hasattr(opt_config, '__dict__'):
                config_dict['optimization_config'] = asdict(opt_config)
        
        return config_dict
    
    def _deserialize_config(self, config_dict: Dict[str, Any]) -> Dict[str, Any]:
        """ååºåˆ—åŒ–é…ç½®å­—å…¸"""
        
        # è™•ç†HyperparameterRangeå°è±¡
        if 'hyperparameter_ranges' in config_dict:
            ranges = config_dict['hyperparameter_ranges']
            for key, range_dict in ranges.items():
                if isinstance(range_dict, dict):
                    ranges[key] = HyperparameterRange(**range_dict)
        
        # è™•ç†OptimizationConfigå°è±¡
        if 'optimization_config' in config_dict:
            opt_config = config_dict['optimization_config']
            if isinstance(opt_config, dict):
                config_dict['optimization_config'] = OptimizationConfig(**opt_config)
        
        return config_dict
    
    def auto_configure(
        self,
        target: str = "performance",
        hardware_info: Optional[Dict[str, Any]] = None
    ) -> AdvancedTrainingConfig:
        """è‡ªå‹•é…ç½®ç”Ÿæˆ"""
        
        logger.info(f"ğŸ¤– è‡ªå‹•é…ç½®ç”Ÿæˆ (ç›®æ¨™: {target})")
        
        # ç²å–ç¡¬é«”ä¿¡æ¯
        if hardware_info is None:
            hardware_info = self._detect_hardware()
        
        # æ ¹æ“šç›®æ¨™é¸æ“‡åŸºç¤æ¨¡æ¿
        if target == "performance":
            base_config = self._create_performance_template()
        elif target == "memory":
            base_config = self._create_memory_template()
        elif target == "speed":
            base_config = self._create_speed_template()
        else:
            base_config = self._create_performance_template()
        
        # æ ¹æ“šç¡¬é«”èª¿æ•´é…ç½®
        optimized_config = self._optimize_for_hardware(base_config, hardware_info)
        
        return AdvancedTrainingConfig(**optimized_config)
    
    def _detect_hardware(self) -> Dict[str, Any]:
        """æª¢æ¸¬ç¡¬é«”é…ç½®"""
        
        hardware_info = {
            'gpu_available': torch.cuda.is_available(),
            'gpu_count': torch.cuda.device_count() if torch.cuda.is_available() else 0,
            'gpu_memory_gb': 0,
            'gpu_name': '',
            'cpu_cores': os.cpu_count(),
            'system_memory_gb': 0
        }
        
        if torch.cuda.is_available():
            gpu_props = torch.cuda.get_device_properties(0)
            hardware_info['gpu_memory_gb'] = gpu_props.total_memory / (1024**3)
            hardware_info['gpu_name'] = gpu_props.name
        
        # ç²å–ç³»çµ±è¨˜æ†¶é«”
        try:
            import psutil
            hardware_info['system_memory_gb'] = psutil.virtual_memory().total / (1024**3)
        except ImportError:
            hardware_info['system_memory_gb'] = 16  # é»˜èªå€¼
        
        return hardware_info
    
    def _optimize_for_hardware(
        self,
        base_config: Dict[str, Any],
        hardware_info: Dict[str, Any]
    ) -> Dict[str, Any]:
        """æ ¹æ“šç¡¬é«”å„ªåŒ–é…ç½®"""
        
        config = base_config.copy()
        
        gpu_memory = hardware_info.get('gpu_memory_gb', 0)
        gpu_name = hardware_info.get('gpu_name', '').lower()
        
        # RTX 4070 å„ªåŒ–
        if '4070' in gpu_name:
            config.update({
                'batch_size': 4,
                'gradient_accumulation_steps': 8,
                'mixed_precision_level': 'O2',
                'gradient_checkpointing': True,
                'zero_optimization_stage': 2
            })
            logger.info("ğŸ¯ RTX 4070 å„ªåŒ–é…ç½®å·²æ‡‰ç”¨")
        
        # RTX 4090 å„ªåŒ–
        elif '4090' in gpu_name:
            config.update({
                'batch_size': 8,
                'gradient_accumulation_steps': 4,
                'mixed_precision_level': 'O1',
                'gradient_checkpointing': False,
                'zero_optimization_stage': 1
            })
            logger.info("ğŸ¯ RTX 4090 å„ªåŒ–é…ç½®å·²æ‡‰ç”¨")
        
        # ä½è¨˜æ†¶é«”GPUå„ªåŒ–
        elif gpu_memory < 8:
            config.update({
                'batch_size': 2,
                'gradient_accumulation_steps': 16,
                'mixed_precision_level': 'O3',
                'cpu_offload': True,
                'zero_optimization_stage': 3
            })
            logger.info("ğŸ¯ ä½è¨˜æ†¶é«”GPUå„ªåŒ–é…ç½®å·²æ‡‰ç”¨")
        
        # é«˜è¨˜æ†¶é«”GPUå„ªåŒ–
        elif gpu_memory > 20:
            config.update({
                'batch_size': 16,
                'gradient_accumulation_steps': 2,
                'mixed_precision_level': 'O1',
                'model_parallelism': True
            })
            logger.info("ğŸ¯ é«˜è¨˜æ†¶é«”GPUå„ªåŒ–é…ç½®å·²æ‡‰ç”¨")
        
        return config
    
    def generate_hyperparameter_sweep_config(
        self,
        base_config: AdvancedTrainingConfig,
        sweep_parameters: List[str]
    ) -> Dict[str, Any]:
        """ç”Ÿæˆè¶…åƒæ•¸æƒæé…ç½®"""
        
        sweep_config = {
            'method': base_config.optimization_config.method,
            'metric': {
                'name': base_config.optimization_config.optimization_metric,
                'goal': 'minimize' if base_config.optimization_config.minimize else 'maximize'
            },
            'parameters': {}
        }
        
        # æ·»åŠ æƒæåƒæ•¸
        for param_name in sweep_parameters:
            if param_name in base_config.hyperparameter_ranges:
                param_range = base_config.hyperparameter_ranges[param_name]
                
                if param_range.values:
                    # åˆ†é¡åƒæ•¸
                    sweep_config['parameters'][param_name] = {
                        'values': param_range.values
                    }
                else:
                    # æ•¸å€¼åƒæ•¸
                    if param_range.scale == 'log':
                        sweep_config['parameters'][param_name] = {
                            'distribution': 'log_uniform',
                            'min': np.log(param_range.min_value),
                            'max': np.log(param_range.max_value)
                        }
                    else:
                        sweep_config['parameters'][param_name] = {
                            'distribution': 'uniform',
                            'min': param_range.min_value,
                            'max': param_range.max_value
                        }
        
        return sweep_config
    
    def validate_config_compatibility(
        self,
        config: AdvancedTrainingConfig
    ) -> Dict[str, Any]:
        """é©—è­‰é…ç½®å…¼å®¹æ€§"""
        
        validation_result = {
            'valid': True,
            'warnings': [],
            'errors': [],
            'suggestions': []
        }
        
        # æª¢æŸ¥è¨˜æ†¶é«”éœ€æ±‚
        estimated_memory = self._estimate_memory_usage(config)
        available_memory = self._get_available_gpu_memory()
        
        if estimated_memory > available_memory * 0.9:
            validation_result['errors'].append(
                f"ä¼°è¨ˆè¨˜æ†¶é«”ä½¿ç”¨ ({estimated_memory:.1f}GB) è¶…éå¯ç”¨è¨˜æ†¶é«” ({available_memory:.1f}GB)"
            )
            validation_result['valid'] = False
        
        # æª¢æŸ¥æ‰¹æ¬¡å¤§å°å’Œæ¢¯åº¦ç´¯ç©
        effective_batch_size = config.batch_size * config.gradient_accumulation_steps
        if effective_batch_size < 8:
            validation_result['warnings'].append(
                "æœ‰æ•ˆæ‰¹æ¬¡å¤§å°éå°ï¼Œå¯èƒ½å½±éŸ¿è¨“ç·´ç©©å®šæ€§"
            )
        
        # æª¢æŸ¥å­¸ç¿’ç‡è¨­ç½®
        if config.learning_rate > 1e-3:
            validation_result['warnings'].append(
                "å­¸ç¿’ç‡è¼ƒé«˜ï¼Œå»ºè­°ç›£æ§è¨“ç·´ç©©å®šæ€§"
            )
        
        # æª¢æŸ¥æ··åˆç²¾åº¦å’ŒZeROå…¼å®¹æ€§
        if config.mixed_precision_level == 'O3' and config.zero_optimization_stage < 2:
            validation_result['suggestions'].append(
                "O3æ··åˆç²¾åº¦å»ºè­°é…åˆZeRO Stage 2+ä½¿ç”¨"
            )
        
        return validation_result
    
    def _estimate_memory_usage(self, config: AdvancedTrainingConfig) -> float:
        """ä¼°ç®—è¨˜æ†¶é«”ä½¿ç”¨é‡"""
        
        # ç°¡åŒ–çš„è¨˜æ†¶é«”ä¼°ç®—
        base_memory = 2.0  # åŸºç¤è¨˜æ†¶é«” (GB)
        
        # æ¨¡å‹è¨˜æ†¶é«”
        model_memory = 1.5  # å‡è¨­ä¸­ç­‰å¤§å°æ¨¡å‹
        
        # æ‰¹æ¬¡è¨˜æ†¶é«”
        batch_memory = config.batch_size * 0.1
        
        # å„ªåŒ–å™¨è¨˜æ†¶é«”
        optimizer_memory = model_memory * 2  # Adaméœ€è¦2å€æ¨¡å‹åƒæ•¸
        
        # ZeROå„ªåŒ–æ¸›å°‘
        if config.zero_optimization_stage >= 2:
            optimizer_memory *= 0.5
        if config.zero_optimization_stage >= 3:
            model_memory *= 0.5
        
        # æ··åˆç²¾åº¦æ¸›å°‘
        if config.mixed_precision_level in ['O2', 'O3']:
            model_memory *= 0.7
            batch_memory *= 0.7
        
        total_memory = base_memory + model_memory + batch_memory + optimizer_memory
        
        return total_memory
    
    def _get_available_gpu_memory(self) -> float:
        """ç²å–å¯ç”¨GPUè¨˜æ†¶é«”"""
        
        if not torch.cuda.is_available():
            return 0.0
        
        gpu_props = torch.cuda.get_device_properties(0)
        total_memory = gpu_props.total_memory / (1024**3)
        
        return total_memory
    
    def create_all_templates(self):
        """å‰µå»ºæ‰€æœ‰é…ç½®æ¨¡æ¿æ–‡ä»¶"""
        
        for template_name, template_config in self.config_templates.items():
            config = AdvancedTrainingConfig(**template_config)
            filename = f"{template_name}_config.yaml"
            self.save_config_yaml(config, filename)
            logger.info(f"å‰µå»ºæ¨¡æ¿é…ç½®: {filename}")
    
    def get_config_recommendations(
        self,
        training_goal: str,
        dataset_size: int,
        time_budget_hours: int
    ) -> Dict[str, Any]:
        """ç²å–é…ç½®å»ºè­°"""
        
        recommendations = {
            'suggested_template': 'performance_optimized',
            'config_adjustments': {},
            'training_strategy': '',
            'estimated_time_hours': 0
        }
        
        # æ ¹æ“šè¨“ç·´ç›®æ¨™é¸æ“‡æ¨¡æ¿
        if training_goal == 'research':
            recommendations['suggested_template'] = 'research_optimized'
            recommendations['training_strategy'] = 'ç ”ç©¶å°å‘ï¼šå•Ÿç”¨è¶…åƒæ•¸å„ªåŒ–å’Œè©³ç´°å¯¦é©—è¿½è¹¤'
        elif training_goal == 'production':
            recommendations['suggested_template'] = 'performance_optimized'
            recommendations['training_strategy'] = 'ç”Ÿç”¢å°å‘ï¼šå¹³è¡¡æ€§èƒ½å’Œç©©å®šæ€§'
        elif training_goal == 'quick_test':
            recommendations['suggested_template'] = 'speed_optimized'
            recommendations['training_strategy'] = 'å¿«é€Ÿæ¸¬è©¦ï¼šå„ªåŒ–è¨“ç·´é€Ÿåº¦'
        
        # æ ¹æ“šæ•¸æ“šé›†å¤§å°èª¿æ•´
        if dataset_size < 1000:
            recommendations['config_adjustments']['num_train_epochs'] = 5
            recommendations['config_adjustments']['save_steps'] = 100
        elif dataset_size > 10000:
            recommendations['config_adjustments']['num_train_epochs'] = 2
            recommendations['config_adjustments']['save_steps'] = 1000
        
        # æ ¹æ“šæ™‚é–“é ç®—èª¿æ•´
        if time_budget_hours < 2:
            recommendations['config_adjustments']['batch_size'] = 8
            recommendations['config_adjustments']['gradient_accumulation_steps'] = 2
        elif time_budget_hours > 12:
            recommendations['config_adjustments']['hyperparameter_optimization'] = True
        
        # ä¼°ç®—è¨“ç·´æ™‚é–“
        base_time_per_epoch = dataset_size / 1000 * 0.5  # ç°¡åŒ–ä¼°ç®—
        epochs = recommendations['config_adjustments'].get('num_train_epochs', 3)
        recommendations['estimated_time_hours'] = base_time_per_epoch * epochs
        
        return recommendations


# è‡ªå‹•æª¢æ¸¬å’Œåˆ‡æ›åˆ° TradingAgents ç›®éŒ„
def ensure_tradingagents_directory():
    """ç¢ºä¿ç•¶å‰å·¥ä½œç›®éŒ„åœ¨ TradingAgents/ ä¸‹ï¼Œä»¥æ­£ç¢ºè¨ªå•é…ç½®æ–‡ä»¶"""
    current_dir = Path.cwd()
    
    # å¦‚æœç•¶å‰ç›®éŒ„æ˜¯ TradingAgents çš„çˆ¶ç›®éŒ„ï¼Œåˆ‡æ›åˆ° TradingAgents
    if (current_dir / 'TradingAgents').exists():
        os.chdir(current_dir / 'TradingAgents')
        print(f"[DIR] è‡ªå‹•åˆ‡æ›å·¥ä½œç›®éŒ„åˆ°: {Path.cwd()}")
    
    # é©—è­‰å¿…è¦çš„ç›®éŒ„å­˜åœ¨
    required_dirs = ['configs', 'training', 'tradingagents']
    missing_dirs = [d for d in required_dirs if not Path(d).exists()]
    
    if missing_dirs:
        raise FileNotFoundError(f"ç¼ºå°‘å¿…è¦ç›®éŒ„: {missing_dirs}. è«‹ç¢ºä¿å¾ TradingAgents/ ç›®éŒ„åŸ·è¡Œæ­¤è…³æœ¬")

# ç›®éŒ„æª¢æŸ¥å‡½æ•¸å·²æº–å‚™å¥½ï¼Œä½†ä¸åœ¨æ¨¡çµ„å°å…¥æ™‚è‡ªå‹•åŸ·è¡Œ
# åªåœ¨éœ€è¦æ™‚æ‰‹å‹•èª¿ç”¨ ensure_tradingagents_directory()

