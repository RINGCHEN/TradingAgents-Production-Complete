"""
AI Task Analysis and Classification System
AIä»»å‹™åˆ†æå’Œåˆ†é¡ç³»çµ±

ä»»å‹™6.1: AIä»»å‹™åˆ†æå’Œåˆ†é¡
è² è²¬äºº: å°k (AIè¨“ç·´å°ˆå®¶åœ˜éšŠ)

æä¾›ï¼š
- AIä»»å‹™è‡ªå‹•åˆ†æ
- ä»»å‹™é¡å‹åˆ†é¡
- è¤‡é›œåº¦è©•ä¼°
- è³‡æºéœ€æ±‚é æ¸¬
- å„ªå…ˆç´šå»ºè­°
"""

import os
import json
import logging
import re
from typing import Dict, List, Optional, Any, Tuple, Union
from dataclasses import dataclass, asdict
from datetime import datetime
from pathlib import Path
from enum import Enum
import numpy as np

logger = logging.getLogger(__name__)

class TaskType(Enum):
    """ä»»å‹™é¡å‹æšèˆ‰"""
    TRAINING = "training"           # è¨“ç·´ä»»å‹™
    INFERENCE = "inference"         # æ¨ç†ä»»å‹™
    EVALUATION = "evaluation"       # è©•ä¼°ä»»å‹™
    DATA_PROCESSING = "data_processing"  # æ•¸æ“šè™•ç†ä»»å‹™
    MODEL_OPTIMIZATION = "model_optimization"  # æ¨¡å‹å„ªåŒ–ä»»å‹™
    SYSTEM_MAINTENANCE = "system_maintenance"  # ç³»çµ±ç¶­è­·ä»»å‹™
    RESEARCH = "research"           # ç ”ç©¶ä»»å‹™
    DEPLOYMENT = "deployment"       # éƒ¨ç½²ä»»å‹™

class TaskComplexity(Enum):
    """ä»»å‹™è¤‡é›œåº¦æšèˆ‰"""
    LOW = "low"                    # ä½è¤‡é›œåº¦
    MEDIUM = "medium"              # ä¸­ç­‰è¤‡é›œåº¦
    HIGH = "high"                  # é«˜è¤‡é›œåº¦
    CRITICAL = "critical"          # é—œéµè¤‡é›œåº¦

class TaskPriority(Enum):
    """ä»»å‹™å„ªå…ˆç´šæšèˆ‰"""
    LOW = 1
    MEDIUM = 2
    HIGH = 3
    URGENT = 4
    CRITICAL = 5

@dataclass
class ResourceRequirement:
    """è³‡æºéœ€æ±‚æ•¸æ“šçµæ§‹"""
    gpu_memory_gb: float = 0.0
    cpu_cores: int = 1
    system_memory_gb: float = 4.0
    storage_gb: float = 10.0
    estimated_time_hours: float = 1.0
    network_bandwidth_mbps: float = 100.0
    
    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)

@dataclass
class TaskAnalysisResult:
    """ä»»å‹™åˆ†æçµæœæ•¸æ“šçµæ§‹"""
    task_id: str
    task_name: str
    task_type: TaskType
    complexity: TaskComplexity
    priority: TaskPriority
    resource_requirements: ResourceRequirement
    dependencies: List[str]
    estimated_duration: float  # å°æ™‚
    risk_factors: List[str]
    optimization_suggestions: List[str]
    analysis_confidence: float  # 0-1ä¹‹é–“
    analysis_timestamp: str
    
    def to_dict(self) -> Dict[str, Any]:
        result = asdict(self)
        result['task_type'] = self.task_type.value
        result['complexity'] = self.complexity.value
        result['priority'] = self.priority.value
        return result

class TaskPatternMatcher:
    """ä»»å‹™æ¨¡å¼åŒ¹é…å™¨"""
    
    def __init__(self):
        # ä»»å‹™é¡å‹é—œéµè©æ¨¡å¼
        self.type_patterns = {
            TaskType.TRAINING: [
                r'train|training|è¨“ç·´|å­¸ç¿’',
                r'grpo|ppo|sft|fine.*tun',
                r'model.*train|è¨“ç·´.*æ¨¡å‹',
                r'epoch|batch|gradient'
            ],
            TaskType.INFERENCE: [
                r'infer|inference|æ¨ç†|é æ¸¬',
                r'predict|generate|ç”Ÿæˆ',
                r'model.*run|é‹è¡Œ.*æ¨¡å‹',
                r'api.*call|èª¿ç”¨.*api'
            ],
            TaskType.EVALUATION: [
                r'eval|evaluation|è©•ä¼°|æ¸¬è©¦',
                r'benchmark|åŸºæº–|é©—è­‰',
                r'metric|æŒ‡æ¨™|performance',
                r'validate|validation'
            ],
            TaskType.DATA_PROCESSING: [
                r'data.*process|æ•¸æ“š.*è™•ç†',
                r'preprocess|é è™•ç†|æ¸…æ´—',
                r'transform|è½‰æ›|æ ¼å¼åŒ–',
                r'dataset|æ•¸æ“šé›†'
            ],
            TaskType.MODEL_OPTIMIZATION: [
                r'optim|optimization|å„ªåŒ–',
                r'hyperparameter|è¶…åƒæ•¸',
                r'tune|tuning|èª¿å„ª',
                r'compress|å£“ç¸®|é‡åŒ–'
            ],
            TaskType.SYSTEM_MAINTENANCE: [
                r'maintain|maintenance|ç¶­è­·',
                r'update|æ›´æ–°|å‡ç´š',
                r'backup|å‚™ä»½|æ¢å¾©',
                r'monitor|ç›£æ§|æª¢æŸ¥'
            ],
            TaskType.RESEARCH: [
                r'research|ç ”ç©¶|å¯¦é©—',
                r'experiment|è©¦é©—|æ¢ç´¢',
                r'analysis|åˆ†æ|èª¿ç ”',
                r'prototype|åŸå‹|æ¦‚å¿µé©—è­‰'
            ],
            TaskType.DEPLOYMENT: [
                r'deploy|deployment|éƒ¨ç½²',
                r'release|ç™¼å¸ƒ|ä¸Šç·š',
                r'production|ç”Ÿç”¢ç’°å¢ƒ',
                r'docker|container|å®¹å™¨'
            ]
        }
        
        # è¤‡é›œåº¦é—œéµè©æ¨¡å¼
        self.complexity_patterns = {
            TaskComplexity.LOW: [
                r'simple|ç°¡å–®|åŸºç¤|basic',
                r'quick|å¿«é€Ÿ|è¼•é‡|light',
                r'test|æ¸¬è©¦|demo|æ¼”ç¤º'
            ],
            TaskComplexity.MEDIUM: [
                r'standard|æ¨™æº–|normal|æ­£å¸¸',
                r'moderate|ä¸­ç­‰|regular',
                r'typical|å…¸å‹|å¸¸è¦'
            ],
            TaskComplexity.HIGH: [
                r'complex|è¤‡é›œ|advanced|é«˜ç´š',
                r'sophisticated|ç²¾å¯†|è©³ç´°',
                r'comprehensive|å…¨é¢|å®Œæ•´'
            ],
            TaskComplexity.CRITICAL: [
                r'critical|é—œéµ|æ ¸å¿ƒ|core',
                r'mission.*critical|ä»»å‹™é—œéµ',
                r'production.*critical|ç”Ÿç”¢é—œéµ',
                r'system.*critical|ç³»çµ±é—œéµ'
            ]
        }
        
        # å„ªå…ˆç´šé—œéµè©æ¨¡å¼
        self.priority_patterns = {
            TaskPriority.LOW: [
                r'low.*priority|ä½å„ªå…ˆç´š',
                r'optional|å¯é¸|éå¿…éœ€',
                r'nice.*to.*have|éŒ¦ä¸Šæ·»èŠ±'
            ],
            TaskPriority.MEDIUM: [
                r'medium.*priority|ä¸­ç­‰å„ªå…ˆç´š',
                r'normal|æ­£å¸¸|æ¨™æº–',
                r'regular|å¸¸è¦'
            ],
            TaskPriority.HIGH: [
                r'high.*priority|é«˜å„ªå…ˆç´š',
                r'important|é‡è¦|é—œéµ',
                r'must.*have|å¿…é ˆ'
            ],
            TaskPriority.URGENT: [
                r'urgent|ç·Šæ€¥|æ€¥è¿«',
                r'asap|ç›¡å¿«|ç«‹å³',
                r'time.*critical|æ™‚é–“é—œéµ'
            ],
            TaskPriority.CRITICAL: [
                r'critical.*priority|é—œéµå„ªå…ˆç´š',
                r'blocker|é˜»å¡|blocking',
                r'emergency|ç·Šæ€¥æƒ…æ³'
            ]
        }
    
    def match_task_type(self, task_description: str) -> Tuple[TaskType, float]:
        """åŒ¹é…ä»»å‹™é¡å‹"""
        scores = {}
        for task_type, patterns in self.type_patterns.items():
            score = 0
            for pattern in patterns:
                matches = len(re.findall(pattern, task_description, re.IGNORECASE))
                score += matches
            if score > 0:
                scores[task_type] = score / len(patterns)
        
        if not scores:
            return TaskType.SYSTEM_MAINTENANCE, 0.1
        
        best_type = max(scores, key=scores.get)
        confidence = min(scores[best_type], 1.0)
        return best_type, confidence
    
    def match_complexity(self, task_description: str) -> Tuple[TaskComplexity, float]:
        """åŒ¹é…ä»»å‹™è¤‡é›œåº¦"""
        scores = {}
        for complexity, patterns in self.complexity_patterns.items():
            score = 0
            for pattern in patterns:
                matches = len(re.findall(pattern, task_description, re.IGNORECASE))
                score += matches
            if score > 0:
                scores[complexity] = score / len(patterns)
        
        # æ ¹æ“šä»»å‹™æè¿°é•·åº¦å’ŒæŠ€è¡“è©å½™å¯†åº¦èª¿æ•´è¤‡é›œåº¦
        tech_terms = [
            'algorithm', 'optimization', 'neural', 'deep', 'learning',
            'transformer', 'attention', 'gradient', 'backprop',
            'ç®—æ³•', 'å„ªåŒ–', 'ç¥ç¶“', 'æ·±åº¦', 'å­¸ç¿’', 'æ³¨æ„åŠ›', 'æ¢¯åº¦'
        ]
        tech_score = sum(1 for term in tech_terms if term.lower() in task_description.lower())
        length_score = len(task_description) / 100  # æ¯100å­—ç¬¦å¢åŠ è¤‡é›œåº¦
        
        # å¦‚æœæ²’æœ‰æ˜ç¢ºçš„è¤‡é›œåº¦é—œéµè©ï¼Œæ ¹æ“šæŠ€è¡“è¤‡é›œåº¦æ¨æ–·
        if not scores:
            if tech_score >= 3 or length_score >= 2:
                return TaskComplexity.HIGH, 0.6
            elif tech_score >= 1 or length_score >= 1:
                return TaskComplexity.MEDIUM, 0.5
            else:
                return TaskComplexity.LOW, 0.4
        
        best_complexity = max(scores, key=scores.get)
        confidence = min(scores[best_complexity] + tech_score * 0.1, 1.0)
        return best_complexity, confidence
    
    def match_priority(self, task_description: str) -> Tuple[TaskPriority, float]:
        """åŒ¹é…ä»»å‹™å„ªå…ˆç´š"""
        scores = {}
        for priority, patterns in self.priority_patterns.items():
            score = 0
            for pattern in patterns:
                matches = len(re.findall(pattern, task_description, re.IGNORECASE))
                score += matches
            if score > 0:
                scores[priority] = score / len(patterns)
        
        if not scores:
            return TaskPriority.MEDIUM, 0.3
        
        best_priority = max(scores, key=scores.get)
        confidence = min(scores[best_priority], 1.0)
        return best_priority, confidence

class ResourceEstimator:
    """è³‡æºéœ€æ±‚ä¼°ç®—å™¨"""
    
    def __init__(self):
        # åŸºæ–¼ä»»å‹™é¡å‹çš„åŸºç¤è³‡æºéœ€æ±‚
        self.base_requirements = {
            TaskType.TRAINING: ResourceRequirement(
                gpu_memory_gb=8.0,
                cpu_cores=4,
                system_memory_gb=16.0,
                storage_gb=50.0,
                estimated_time_hours=4.0,
                network_bandwidth_mbps=100.0
            ),
            TaskType.INFERENCE: ResourceRequirement(
                gpu_memory_gb=4.0,
                cpu_cores=2,
                system_memory_gb=8.0,
                storage_gb=10.0,
                estimated_time_hours=0.5,
                network_bandwidth_mbps=50.0
            ),
            TaskType.EVALUATION: ResourceRequirement(
                gpu_memory_gb=6.0,
                cpu_cores=2,
                system_memory_gb=12.0,
                storage_gb=20.0,
                estimated_time_hours=2.0,
                network_bandwidth_mbps=100.0
            ),
            TaskType.DATA_PROCESSING: ResourceRequirement(
                gpu_memory_gb=2.0,
                cpu_cores=4,
                system_memory_gb=16.0,
                storage_gb=100.0,
                estimated_time_hours=3.0,
                network_bandwidth_mbps=200.0
            ),
            TaskType.MODEL_OPTIMIZATION: ResourceRequirement(
                gpu_memory_gb=12.0,
                cpu_cores=6,
                system_memory_gb=24.0,
                storage_gb=30.0,
                estimated_time_hours=8.0,
                network_bandwidth_mbps=100.0
            ),
            TaskType.SYSTEM_MAINTENANCE: ResourceRequirement(
                gpu_memory_gb=1.0,
                cpu_cores=2,
                system_memory_gb=4.0,
                storage_gb=20.0,
                estimated_time_hours=1.0,
                network_bandwidth_mbps=50.0
            ),
            TaskType.RESEARCH: ResourceRequirement(
                gpu_memory_gb=10.0,
                cpu_cores=4,
                system_memory_gb=20.0,
                storage_gb=40.0,
                estimated_time_hours=12.0,
                network_bandwidth_mbps=100.0
            ),
            TaskType.DEPLOYMENT: ResourceRequirement(
                gpu_memory_gb=4.0,
                cpu_cores=2,
                system_memory_gb=8.0,
                storage_gb=30.0,
                estimated_time_hours=2.0,
                network_bandwidth_mbps=500.0
            )
        }
        
        # è¤‡é›œåº¦èª¿æ•´ä¿‚æ•¸
        self.complexity_multipliers = {
            TaskComplexity.LOW: 0.5,
            TaskComplexity.MEDIUM: 1.0,
            TaskComplexity.HIGH: 2.0,
            TaskComplexity.CRITICAL: 3.0
        }
    
    def estimate_resources(
        self,
        task_type: TaskType,
        complexity: TaskComplexity,
        task_description: str
    ) -> ResourceRequirement:
        """ä¼°ç®—è³‡æºéœ€æ±‚"""
        # ç²å–åŸºç¤éœ€æ±‚
        base_req = self.base_requirements.get(task_type, self.base_requirements[TaskType.SYSTEM_MAINTENANCE])
        
        # è¤‡é›œåº¦èª¿æ•´
        multiplier = self.complexity_multipliers[complexity]
        
        # å‰µå»ºèª¿æ•´å¾Œçš„è³‡æºéœ€æ±‚
        estimated_req = ResourceRequirement(
            gpu_memory_gb=base_req.gpu_memory_gb * multiplier,
            cpu_cores=max(1, int(base_req.cpu_cores * multiplier)),
            system_memory_gb=base_req.system_memory_gb * multiplier,
            storage_gb=base_req.storage_gb * multiplier,
            estimated_time_hours=base_req.estimated_time_hours * multiplier,
            network_bandwidth_mbps=base_req.network_bandwidth_mbps
        )
        
        # åŸºæ–¼ä»»å‹™æè¿°çš„ç‰¹æ®Šèª¿æ•´
        estimated_req = self._adjust_for_specific_requirements(estimated_req, task_description)
        
        return estimated_req
    
    def _adjust_for_specific_requirements(
        self,
        base_req: ResourceRequirement,
        task_description: str
    ) -> ResourceRequirement:
        """åŸºæ–¼ä»»å‹™æè¿°èª¿æ•´ç‰¹æ®Šéœ€æ±‚"""
        adjusted_req = ResourceRequirement(**asdict(base_req))
        
        # å¤§æ¨¡å‹ç›¸é—œèª¿æ•´
        large_model_terms = ['large.*model', 'llm', 'gpt', 'bert', 'transformer']
        if any(re.search(term, task_description, re.IGNORECASE) for term in large_model_terms):
            adjusted_req.gpu_memory_gb *= 1.5
            adjusted_req.system_memory_gb *= 1.3
        
        # æ‰¹é‡è™•ç†èª¿æ•´
        batch_terms = ['batch', 'bulk', 'æ‰¹é‡', 'å¤§é‡']
        if any(term in task_description.lower() for term in batch_terms):
            adjusted_req.cpu_cores = max(adjusted_req.cpu_cores, 4)
            adjusted_req.system_memory_gb *= 1.2
        
        # å¯¦æ™‚è™•ç†èª¿æ•´
        realtime_terms = ['real.*time', 'realtime', 'å¯¦æ™‚', 'streaming']
        if any(re.search(term, task_description, re.IGNORECASE) for term in realtime_terms):
            adjusted_req.network_bandwidth_mbps *= 2
            adjusted_req.cpu_cores = max(adjusted_req.cpu_cores, 4)
        
        # åˆ†æ•£å¼è™•ç†èª¿æ•´
        distributed_terms = ['distributed', 'parallel', 'åˆ†æ•£å¼', 'ä¸¦è¡Œ']
        if any(term in task_description.lower() for term in distributed_terms):
            adjusted_req.gpu_memory_gb *= 0.8  # åˆ†æ•£å¼å¯ä»¥æ¸›å°‘å–®æ©Ÿéœ€æ±‚
            adjusted_req.network_bandwidth_mbps *= 3
        
        return adjusted_req

class TaskAnalyzer:
    """AIä»»å‹™åˆ†æå™¨"""
    
    def __init__(self):
        self.pattern_matcher = TaskPatternMatcher()
        self.resource_estimator = ResourceEstimator()
        self.analysis_history = []
        
        # ä¾è³´é—œä¿‚æ¨¡å¼
        self.dependency_patterns = {
            'data_dependency': [
                r'éœ€è¦.*æ•¸æ“š|require.*data',
                r'ä¾è³´.*æ•¸æ“šé›†|depend.*dataset',
                r'åŸºæ–¼.*æ•¸æ“š|based.*on.*data'
            ],
            'model_dependency': [
                r'éœ€è¦.*æ¨¡å‹|require.*model',
                r'ä¾è³´.*æ¨¡å‹|depend.*model',
                r'åŸºæ–¼.*æ¨¡å‹|based.*on.*model'
            ],
            'system_dependency': [
                r'éœ€è¦.*ç³»çµ±|require.*system',
                r'ä¾è³´.*æœå‹™|depend.*service',
                r'éœ€è¦.*ç’°å¢ƒ|require.*environment'
            ]
        }
        
        # é¢¨éšªå› ç´ æ¨¡å¼
        self.risk_patterns = {
            'resource_risk': [
                r'å¤§é‡.*è³‡æº|large.*resource',
                r'é«˜.*è¨˜æ†¶é«”|high.*memory',
                r'é•·æ™‚é–“.*é‹è¡Œ|long.*running'
            ],
            'complexity_risk': [
                r'è¤‡é›œ.*ç®—æ³•|complex.*algorithm',
                r'å¯¦é©—æ€§|experimental',
                r'æœªç¶“.*æ¸¬è©¦|untested'
            ],
            'dependency_risk': [
                r'å¤šå€‹.*ä¾è³´|multiple.*depend',
                r'å¤–éƒ¨.*æœå‹™|external.*service',
                r'ç¬¬ä¸‰æ–¹.*api|third.*party.*api'
            ]
        }
    
    def analyze_task(
        self,
        task_id: str,
        task_name: str,
        task_description: str,
        additional_context: Optional[Dict[str, Any]] = None
    ) -> TaskAnalysisResult:
        """åˆ†æAIä»»å‹™"""
        logger.info(f"ğŸ” é–‹å§‹åˆ†æä»»å‹™: {task_name}")
        
        # ä»»å‹™é¡å‹åˆ†æ
        task_type, type_confidence = self.pattern_matcher.match_task_type(task_description)
        
        # è¤‡é›œåº¦åˆ†æ
        complexity, complexity_confidence = self.pattern_matcher.match_complexity(task_description)
        
        # å„ªå…ˆç´šåˆ†æ
        priority, priority_confidence = self.pattern_matcher.match_priority(task_description)
        
        # è³‡æºéœ€æ±‚ä¼°ç®—
        resource_requirements = self.resource_estimator.estimate_resources(
            task_type, complexity, task_description
        )
        
        # ä¾è³´é—œä¿‚åˆ†æ
        dependencies = self._analyze_dependencies(task_description, additional_context)
        
        # é¢¨éšªå› ç´ åˆ†æ
        risk_factors = self._analyze_risk_factors(task_description, task_type, complexity)
        
        # å„ªåŒ–å»ºè­°ç”Ÿæˆ
        optimization_suggestions = self._generate_optimization_suggestions(
            task_type, complexity, resource_requirements, risk_factors
        )
        
        # è¨ˆç®—ç¸½é«”åˆ†æä¿¡å¿ƒåº¦
        analysis_confidence = np.mean([
            type_confidence,
            complexity_confidence,
            priority_confidence
        ])
        
        # å‰µå»ºåˆ†æçµæœ
        result = TaskAnalysisResult(
            task_id=task_id,
            task_name=task_name,
            task_type=task_type,
            complexity=complexity,
            priority=priority,
            resource_requirements=resource_requirements,
            dependencies=dependencies,
            estimated_duration=resource_requirements.estimated_time_hours,
            risk_factors=risk_factors,
            optimization_suggestions=optimization_suggestions,
            analysis_confidence=analysis_confidence,
            analysis_timestamp=datetime.now().isoformat()
        )
        
        # æ·»åŠ åˆ°æ­·å²è¨˜éŒ„
        self.analysis_history.append(result)
        
        logger.info(f"âœ… ä»»å‹™åˆ†æå®Œæˆ: {task_type.value}, è¤‡é›œåº¦: {complexity.value}, ä¿¡å¿ƒåº¦: {analysis_confidence:.2f}")
        return result
    
    def _analyze_dependencies(
        self,
        task_description: str,
        additional_context: Optional[Dict[str, Any]]
    ) -> List[str]:
        """åˆ†æä»»å‹™ä¾è³´é—œä¿‚"""
        dependencies = []
        
        # åŸºæ–¼æ¨¡å¼åŒ¹é…çš„ä¾è³´åˆ†æ
        for dep_type, patterns in self.dependency_patterns.items():
            for pattern in patterns:
                if re.search(pattern, task_description, re.IGNORECASE):
                    dependencies.append(dep_type)
                    break
        
        # åŸºæ–¼ä¸Šä¸‹æ–‡çš„ä¾è³´åˆ†æ
        if additional_context:
            if 'prerequisites' in additional_context:
                dependencies.extend(additional_context['prerequisites'])
            if 'required_services' in additional_context:
                dependencies.extend(additional_context['required_services'])
        
        # å»é‡ä¸¦è¿”å›
        return list(set(dependencies))
    
    def _analyze_risk_factors(
        self,
        task_description: str,
        task_type: TaskType,
        complexity: TaskComplexity
    ) -> List[str]:
        """åˆ†æé¢¨éšªå› ç´ """
        risk_factors = []
        
        # åŸºæ–¼æ¨¡å¼åŒ¹é…çš„é¢¨éšªåˆ†æ
        for risk_type, patterns in self.risk_patterns.items():
            for pattern in patterns:
                if re.search(pattern, task_description, re.IGNORECASE):
                    risk_factors.append(risk_type)
                    break
        
        # åŸºæ–¼ä»»å‹™é¡å‹çš„é¢¨éšª
        if task_type == TaskType.TRAINING:
            risk_factors.append("training_instability")
        elif task_type == TaskType.DEPLOYMENT:
            risk_factors.append("production_impact")
        elif task_type == TaskType.RESEARCH:
            risk_factors.append("uncertain_outcome")
        
        # åŸºæ–¼è¤‡é›œåº¦çš„é¢¨éšª
        if complexity in [TaskComplexity.HIGH, TaskComplexity.CRITICAL]:
            risk_factors.append("high_complexity_risk")
        
        return list(set(risk_factors))
    
    def _generate_optimization_suggestions(
        self,
        task_type: TaskType,
        complexity: TaskComplexity,
        resource_requirements: ResourceRequirement,
        risk_factors: List[str]
    ) -> List[str]:
        """ç”Ÿæˆå„ªåŒ–å»ºè­°"""
        suggestions = []
        
        # åŸºæ–¼ä»»å‹™é¡å‹çš„å»ºè­°
        if task_type == TaskType.TRAINING:
            suggestions.append("è€ƒæ…®ä½¿ç”¨æ··åˆç²¾åº¦è¨“ç·´æ¸›å°‘GPUè¨˜æ†¶é«”ä½¿ç”¨")
            suggestions.append("å¯¦æ–½æª¢æŸ¥é»ä¿å­˜æ©Ÿåˆ¶é˜²æ­¢è¨“ç·´ä¸­æ–·")
            if complexity == TaskComplexity.HIGH:
                suggestions.append("è€ƒæ…®åˆ†éšæ®µè¨“ç·´ç­–ç•¥")
        elif task_type == TaskType.INFERENCE:
            suggestions.append("è€ƒæ…®æ¨¡å‹é‡åŒ–ä»¥æé«˜æ¨ç†é€Ÿåº¦")
            suggestions.append("å¯¦æ–½æ‰¹é‡æ¨ç†ä»¥æé«˜ååé‡")
        elif task_type == TaskType.DATA_PROCESSING:
            suggestions.append("è€ƒæ…®ä¸¦è¡Œè™•ç†ä»¥æé«˜æ•¸æ“šè™•ç†é€Ÿåº¦")
            suggestions.append("å¯¦æ–½æ•¸æ“šç®¡é“å„ªåŒ–")
        
        # åŸºæ–¼è³‡æºéœ€æ±‚çš„å»ºè­°
        if resource_requirements.gpu_memory_gb > 16:
            suggestions.append("è€ƒæ…®ä½¿ç”¨æ¨¡å‹ä¸¦è¡ŒåŒ–æˆ–æ¢¯åº¦æª¢æŸ¥é»")
        if resource_requirements.estimated_time_hours > 8:
            suggestions.append("å»ºè­°åˆ†è§£ç‚ºå¤šå€‹å­ä»»å‹™ä¸¦è¡ŒåŸ·è¡Œ")
        
        # åŸºæ–¼é¢¨éšªå› ç´ çš„å»ºè­°
        if "high_complexity_risk" in risk_factors:
            suggestions.append("å»ºè­°å…ˆé€²è¡Œå°è¦æ¨¡åŸå‹é©—è­‰")
        if "resource_risk" in risk_factors:
            suggestions.append("å»ºè­°å¯¦æ–½è³‡æºç›£æ§å’Œè‡ªå‹•æ“´ç¸®å®¹")
        if "dependency_risk" in risk_factors:
            suggestions.append("å»ºè­°å¯¦æ–½ä¾è³´å¥åº·æª¢æŸ¥å’Œå®¹éŒ¯æ©Ÿåˆ¶")
        
        return suggestions
    
    def batch_analyze_tasks(
        self,
        tasks: List[Dict[str, Any]]
    ) -> List[TaskAnalysisResult]:
        """æ‰¹é‡åˆ†æä»»å‹™"""
        results = []
        for task in tasks:
            result = self.analyze_task(
                task_id=task.get('id', ''),
                task_name=task.get('name', ''),
                task_description=task.get('description', ''),
                additional_context=task.get('context')
            )
            results.append(result)
        return results
    
    def get_analysis_summary(self) -> Dict[str, Any]:
        """ç²å–åˆ†ææ‘˜è¦"""
        if not self.analysis_history:
            return {"message": "No analysis history available"}
        
        # çµ±è¨ˆå„ç¨®ä»»å‹™é¡å‹
        type_counts = {}
        complexity_counts = {}
        priority_counts = {}
        total_gpu_memory = 0
        total_estimated_time = 0
        
        for result in self.analysis_history:
            # ä»»å‹™é¡å‹çµ±è¨ˆ
            task_type = result.task_type.value
            type_counts[task_type] = type_counts.get(task_type, 0) + 1
            
            # è¤‡é›œåº¦çµ±è¨ˆ
            complexity = result.complexity.value
            complexity_counts[complexity] = complexity_counts.get(complexity, 0) + 1
            
            # å„ªå…ˆç´šçµ±è¨ˆ
            priority = result.priority.value
            priority_counts[priority] = priority_counts.get(priority, 0) + 1
            
            # è³‡æºçµ±è¨ˆ
            total_gpu_memory += result.resource_requirements.gpu_memory_gb
            total_estimated_time += result.estimated_duration
        
        return {
            "total_tasks_analyzed": len(self.analysis_history),
            "task_type_distribution": type_counts,
            "complexity_distribution": complexity_counts,
            "priority_distribution": priority_counts,
            "resource_summary": {
                "total_gpu_memory_gb": total_gpu_memory,
                "average_gpu_memory_gb": total_gpu_memory / len(self.analysis_history),
                "total_estimated_time_hours": total_estimated_time,
                "average_estimated_time_hours": total_estimated_time / len(self.analysis_history)
            },
            "analysis_timestamp": datetime.now().isoformat()
        }
    
    def export_analysis_results(self, output_path: str):
        """å°å‡ºåˆ†æçµæœ"""
        output_path = Path(output_path)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # å°å‡ºè©³ç´°çµæœ
        detailed_results = [result.to_dict() for result in self.analysis_history]
        with open(output_path / "task_analysis_results.json", 'w', encoding='utf-8') as f:
            json.dump(detailed_results, f, ensure_ascii=False, indent=2)
        
        # å°å‡ºæ‘˜è¦
        summary = self.get_analysis_summary()
        with open(output_path / "task_analysis_summary.json", 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        logger.info(f"åˆ†æçµæœå·²å°å‡ºåˆ°: {output_path}")

# ä¾¿åˆ©å‡½æ•¸
def analyze_single_task(
    task_name: str,
    task_description: str,
    task_id: Optional[str] = None
) -> TaskAnalysisResult:
    """åˆ†æå–®å€‹ä»»å‹™çš„ä¾¿åˆ©å‡½æ•¸"""
    analyzer = TaskAnalyzer()
    if task_id is None:
        task_id = f"task_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    return analyzer.analyze_task(task_id, task_name, task_description)

def create_task_analysis_report(
    tasks: List[Dict[str, Any]],
    output_path: str = "./task_analysis_report"
) -> Dict[str, Any]:
    """å‰µå»ºä»»å‹™åˆ†æå ±å‘Šçš„ä¾¿åˆ©å‡½æ•¸"""
    analyzer = TaskAnalyzer()
    results = analyzer.batch_analyze_tasks(tasks)
    
    # å°å‡ºçµæœ
    analyzer.export_analysis_results(output_path)
    
    # è¿”å›æ‘˜è¦
    return analyzer.get_analysis_summary()