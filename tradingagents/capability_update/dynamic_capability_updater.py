#!/usr/bin/env python3
"""
Dynamic Capability Updater
å‹•æ…‹èƒ½åŠ›æ›´æ–°å™¨ - GPT-OSSæ•´åˆä»»å‹™1.2.3
"""

import asyncio
import logging
import time
import statistics
from datetime import datetime, timezone, timedelta
from typing import Dict, Any, List, Optional, Callable, Set
from dataclasses import dataclass, field
from enum import Enum
from concurrent.futures import ThreadPoolExecutor, as_completed

from ..database.model_capability_db import ModelCapabilityDB
from ..monitoring.performance_monitor import PerformanceMonitor, MetricType
from ..benchmarks.benchmark_runner import BenchmarkRunner
from .model_version_control import ModelVersionControl, VersionUpdateRequest, ChangeType

logger = logging.getLogger(__name__)

class UpdateTrigger(Enum):
    """æ›´æ–°è§¸ç™¼æ¢ä»¶"""
    PERFORMANCE_THRESHOLD = "performance_threshold"
    BENCHMARK_SCHEDULE = "benchmark_schedule"
    DRIFT_DETECTION = "drift_detection"
    MANUAL_REQUEST = "manual_request"
    TIME_BASED = "time_based"
    ALERT_BASED = "alert_based"

class UpdateStrategy(Enum):
    """æ›´æ–°ç­–ç•¥"""
    IMMEDIATE = "immediate"
    BATCHED = "batched"
    SCHEDULED = "scheduled"
    GRADUAL = "gradual"

@dataclass
class UpdateRule:
    """æ›´æ–°è¦å‰‡"""
    rule_id: str
    trigger_type: UpdateTrigger
    conditions: Dict[str, Any]
    strategy: UpdateStrategy
    enabled: bool = True
    priority: int = 1  # 1=high, 2=medium, 3=low
    cooldown_minutes: int = 60
    max_retries: int = 3
    
    # éæ¿¾æ¢ä»¶
    provider_filter: Optional[Set[str]] = None
    model_filter: Optional[Set[str]] = None
    
    # å›èª¿å‡½æ•¸
    pre_update_callback: Optional[Callable] = None
    post_update_callback: Optional[Callable] = None

@dataclass
class UpdateTask:
    """æ›´æ–°ä»»å‹™"""
    task_id: str
    provider: str
    model_id: str
    update_type: ChangeType
    trigger: UpdateTrigger
    priority: int
    created_at: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    scheduled_at: Optional[datetime] = None
    retry_count: int = 0
    context: Dict[str, Any] = field(default_factory=dict)

@dataclass
class UpdateResult:
    """æ›´æ–°çµæœ"""
    task_id: str
    success: bool
    version_id: Optional[str] = None
    changes_applied: List[str] = field(default_factory=list)
    error: Optional[str] = None
    execution_time_ms: float = 0
    performance_impact: Dict[str, float] = field(default_factory=dict)

class DynamicCapabilityUpdater:
    """
    å‹•æ…‹èƒ½åŠ›æ›´æ–°å™¨
    
    åŠŸèƒ½ï¼š
    1. è‡ªå‹•ç›£æ¸¬æ¨¡å‹æ€§èƒ½è®ŠåŒ–
    2. åŸºæ–¼è¦å‰‡çš„æ›´æ–°è§¸ç™¼
    3. æ™ºèƒ½æ›´æ–°ç­–ç•¥èª¿åº¦
    4. æ‰¹é‡è™•ç†å’Œå„ªåŒ–
    5. éŒ¯èª¤è™•ç†å’Œé‡è©¦æ©Ÿåˆ¶
    """
    
    def __init__(
        self,
        model_db: Optional[ModelCapabilityDB] = None,
        performance_monitor: Optional[PerformanceMonitor] = None,
        benchmark_runner: Optional[BenchmarkRunner] = None,
        version_control: Optional[ModelVersionControl] = None
    ):
        """åˆå§‹åŒ–å‹•æ…‹èƒ½åŠ›æ›´æ–°å™¨"""
        self.model_db = model_db or ModelCapabilityDB()
        self.performance_monitor = performance_monitor or PerformanceMonitor()
        self.benchmark_runner = benchmark_runner or BenchmarkRunner()
        self.version_control = version_control or ModelVersionControl()
        
        self.logger = logger
        
        # æ›´æ–°è¦å‰‡å’Œä»»å‹™éšŠåˆ—
        self.update_rules: Dict[str, UpdateRule] = {}
        self.task_queue: List[UpdateTask] = []
        self.active_tasks: Dict[str, UpdateTask] = {}
        
        # é‹è¡Œæ™‚ç‹€æ…‹
        self._running = False
        self._scheduler_task: Optional[asyncio.Task] = None
        self._worker_tasks: List[asyncio.Task] = []
        
        # é…ç½®
        self.max_concurrent_updates = 3
        self.update_interval_seconds = 30
        self.batch_size = 5
        
        # çµ±è¨ˆ
        self.update_stats = {
            'total_updates': 0,
            'successful_updates': 0,
            'failed_updates': 0,
            'average_execution_time': 0.0
        }
        
        # åˆå§‹åŒ–é»˜èªè¦å‰‡
        self._initialize_default_rules()
    
    def _initialize_default_rules(self):
        """åˆå§‹åŒ–é»˜èªæ›´æ–°è¦å‰‡"""
        # æ€§èƒ½é–¾å€¼è§¸ç™¼è¦å‰‡
        self.add_update_rule(UpdateRule(
            rule_id="performance_degradation",
            trigger_type=UpdateTrigger.PERFORMANCE_THRESHOLD,
            conditions={
                'capability_score_threshold': 0.1,  # èƒ½åŠ›è©•åˆ†ä¸‹é™è¶…é0.1
                'latency_increase_threshold': 2000,  # å»¶é²å¢åŠ è¶…é2ç§’
                'error_rate_threshold': 0.15,  # éŒ¯èª¤ç‡è¶…é15%
                'min_samples': 10  # æœ€å°‘æ¨£æœ¬æ•¸
            },
            strategy=UpdateStrategy.IMMEDIATE,
            priority=1,
            cooldown_minutes=30
        ))
        
        # å®šæœŸåŸºæº–æ¸¬è©¦è¦å‰‡
        self.add_update_rule(UpdateRule(
            rule_id="scheduled_benchmark",
            trigger_type=UpdateTrigger.BENCHMARK_SCHEDULE,
            conditions={
                'schedule_hours': [2, 14],  # æ¯æ—¥2é»å’Œ14é»åŸ·è¡Œ
                'min_days_since_last': 1  # æœ€å°‘é–“éš”1å¤©
            },
            strategy=UpdateStrategy.BATCHED,
            priority=2,
            cooldown_minutes=720  # 12å°æ™‚å†·å»æœŸ
        ))
        
        # æ¨¡å‹æ¼‚ç§»æª¢æ¸¬è¦å‰‡
        self.add_update_rule(UpdateRule(
            rule_id="capability_drift",
            trigger_type=UpdateTrigger.DRIFT_DETECTION,
            conditions={
                'drift_threshold': 0.05,  # 5%çš„æ€§èƒ½æ¼‚ç§»
                'monitoring_window_hours': 24,  # 24å°æ™‚ç›£æ§çª—å£
                'confidence_threshold': 0.8  # 80%ç½®ä¿¡åº¦
            },
            strategy=UpdateStrategy.GRADUAL,
            priority=2,
            cooldown_minutes=120
        ))
    
    def add_update_rule(self, rule: UpdateRule):
        """æ·»åŠ æ›´æ–°è¦å‰‡"""
        self.update_rules[rule.rule_id] = rule
        self.logger.info(f"âœ… Added update rule: {rule.rule_id}")
    
    def remove_update_rule(self, rule_id: str) -> bool:
        """ç§»é™¤æ›´æ–°è¦å‰‡"""
        if rule_id in self.update_rules:
            del self.update_rules[rule_id]
            self.logger.info(f"âœ… Removed update rule: {rule_id}")
            return True
        return False
    
    def enable_rule(self, rule_id: str, enabled: bool = True):
        """å•Ÿç”¨/ç¦ç”¨è¦å‰‡"""
        if rule_id in self.update_rules:
            self.update_rules[rule_id].enabled = enabled
            status = "enabled" if enabled else "disabled"
            self.logger.info(f"âœ… Rule {rule_id} {status}")
    
    async def start(self):
        """å•Ÿå‹•å‹•æ…‹æ›´æ–°å™¨"""
        if not self._running:
            self._running = True
            
            # å•Ÿå‹•èª¿åº¦å™¨
            self._scheduler_task = asyncio.create_task(self._scheduler_loop())
            
            # å•Ÿå‹•å·¥ä½œè€…
            for i in range(self.max_concurrent_updates):
                worker_task = asyncio.create_task(self._worker_loop(f"worker-{i}"))
                self._worker_tasks.append(worker_task)
            
            # è¨»å†Šæ€§èƒ½ç›£æ§å›èª¿
            if hasattr(self.performance_monitor, 'add_alert_callback'):
                self.performance_monitor.add_alert_callback(self._handle_performance_alert)
            
            self.logger.info("âœ… Dynamic capability updater started")
    
    async def stop(self):
        """åœæ­¢å‹•æ…‹æ›´æ–°å™¨"""
        if self._running:
            self._running = False
            
            # åœæ­¢èª¿åº¦å™¨
            if self._scheduler_task:
                self._scheduler_task.cancel()
                try:
                    await self._scheduler_task
                except asyncio.CancelledError:
                    pass
            
            # åœæ­¢å·¥ä½œè€…
            for worker_task in self._worker_tasks:
                worker_task.cancel()
            
            await asyncio.gather(*self._worker_tasks, return_exceptions=True)
            self._worker_tasks.clear()
            
            # ç§»é™¤æ€§èƒ½ç›£æ§å›èª¿
            if hasattr(self.performance_monitor, 'remove_alert_callback'):
                self.performance_monitor.remove_alert_callback(self._handle_performance_alert)
            
            self.logger.info("âœ… Dynamic capability updater stopped")
    
    async def _scheduler_loop(self):
        """èª¿åº¦å™¨ä¸»å¾ªç’°"""
        while self._running:
            try:
                # æª¢æŸ¥æ‰€æœ‰å•Ÿç”¨çš„è¦å‰‡
                for rule in self.update_rules.values():
                    if rule.enabled:
                        await self._evaluate_rule(rule)
                
                # è™•ç†å®šæ™‚æ›´æ–°
                await self._process_scheduled_updates()
                
                # ç­‰å¾…ä¸‹ä¸€æ¬¡æª¢æŸ¥
                await asyncio.sleep(self.update_interval_seconds)
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                self.logger.error(f"âŒ Error in scheduler loop: {e}")
                await asyncio.sleep(5)  # éŒ¯èª¤å¾ŒçŸ­æš«ä¼‘æ¯
    
    async def _worker_loop(self, worker_id: str):
        """å·¥ä½œè€…ä¸»å¾ªç’°"""
        while self._running:
            try:
                # å¾éšŠåˆ—ç²å–ä»»å‹™
                task = await self._get_next_task()
                
                if task:
                    self.active_tasks[task.task_id] = task
                    
                    try:
                        # åŸ·è¡Œæ›´æ–°ä»»å‹™
                        result = await self._execute_update_task(task)
                        self.logger.info(f"âœ… Worker {worker_id} completed task {task.task_id}")
                        
                    except Exception as e:
                        self.logger.error(f"âŒ Worker {worker_id} failed task {task.task_id}: {e}")
                        
                        # è™•ç†é‡è©¦
                        if task.retry_count < 3:  # æœ€å¤šé‡è©¦3æ¬¡
                            task.retry_count += 1
                            await self._schedule_task(task)
                    
                    finally:
                        # ç§»é™¤æ´»èºä»»å‹™
                        if task.task_id in self.active_tasks:
                            del self.active_tasks[task.task_id]
                
                else:
                    # æ²’æœ‰ä»»å‹™æ™‚çŸ­æš«ä¼‘æ¯
                    await asyncio.sleep(1)
                    
            except asyncio.CancelledError:
                break
            except Exception as e:
                self.logger.error(f"âŒ Error in worker {worker_id}: {e}")
                await asyncio.sleep(5)
    
    async def _evaluate_rule(self, rule: UpdateRule):
        """è©•ä¼°æ›´æ–°è¦å‰‡"""
        try:
            if rule.trigger_type == UpdateTrigger.PERFORMANCE_THRESHOLD:
                await self._check_performance_thresholds(rule)
            
            elif rule.trigger_type == UpdateTrigger.BENCHMARK_SCHEDULE:
                await self._check_benchmark_schedule(rule)
            
            elif rule.trigger_type == UpdateTrigger.DRIFT_DETECTION:
                await self._check_capability_drift(rule)
            
            elif rule.trigger_type == UpdateTrigger.TIME_BASED:
                await self._check_time_based_triggers(rule)
                
        except Exception as e:
            self.logger.error(f"âŒ Error evaluating rule {rule.rule_id}: {e}")
    
    async def _check_performance_thresholds(self, rule: UpdateRule):
        """æª¢æŸ¥æ€§èƒ½é–¾å€¼è§¸ç™¼æ¢ä»¶"""
        conditions = rule.conditions
        
        # ç²å–æ‰€æœ‰æ¨¡å‹çš„ç•¶å‰æŒ‡æ¨™
        current_metrics = self.performance_monitor.get_current_metrics()
        
        for window_key, metrics in current_metrics.items():
            if not metrics:
                continue
            
            parts = window_key.split('/')
            if len(parts) != 3:
                continue
            
            provider, model_id, metric_type = parts
            
            # æ‡‰ç”¨éæ¿¾æ¢ä»¶
            if rule.provider_filter and provider not in rule.provider_filter:
                continue
            if rule.model_filter and model_id not in rule.model_filter:
                continue
            
            # æª¢æŸ¥æ¨£æœ¬æ•¸
            sample_count = metrics.get('count', 0)
            if sample_count < conditions.get('min_samples', 10):
                continue
            
            # æª¢æŸ¥å„é …é–¾å€¼
            trigger_update = False
            trigger_reason = []
            
            # æª¢æŸ¥èƒ½åŠ›è©•åˆ†ä¸‹é™
            if metric_type == 'accuracy' or metric_type == 'success_rate':
                current_value = metrics.get('mean', 1.0)
                threshold = conditions.get('capability_score_threshold', 0.1)
                
                # ç²å–æ­·å²åŸºæº–
                model_capability = await self.model_db.get_model_capability(provider, model_id)
                if model_capability:
                    baseline = model_capability.accuracy_score or model_capability.capability_score or 0.8
                    if baseline - current_value > threshold:
                        trigger_update = True
                        trigger_reason.append(f"Performance degradation: {current_value:.3f} vs baseline {baseline:.3f}")
            
            # æª¢æŸ¥å»¶é²å¢åŠ 
            if metric_type == 'latency':
                current_latency = metrics.get('mean', 0)
                threshold = conditions.get('latency_increase_threshold', 2000)
                
                model_capability = await self.model_db.get_model_capability(provider, model_id)
                if model_capability:
                    baseline_latency = model_capability.avg_latency_ms or 1000
                    if current_latency - baseline_latency > threshold:
                        trigger_update = True
                        trigger_reason.append(f"Latency increase: {current_latency:.0f}ms vs baseline {baseline_latency:.0f}ms")
            
            # æª¢æŸ¥éŒ¯èª¤ç‡
            if metric_type == 'error_rate':
                error_rate = metrics.get('mean', 0)
                threshold = conditions.get('error_rate_threshold', 0.15)
                
                if error_rate > threshold:
                    trigger_update = True
                    trigger_reason.append(f"High error rate: {error_rate:.1%}")
            
            # è§¸ç™¼æ›´æ–°ä»»å‹™
            if trigger_update:
                await self._create_update_task(
                    provider=provider,
                    model_id=model_id,
                    update_type=ChangeType.PERFORMANCE_UPDATE,
                    trigger=rule.trigger_type,
                    priority=rule.priority,
                    context={
                        'rule_id': rule.rule_id,
                        'trigger_reasons': trigger_reason,
                        'metrics': metrics
                    }
                )
    
    async def _check_benchmark_schedule(self, rule: UpdateRule):
        """æª¢æŸ¥åŸºæº–æ¸¬è©¦èª¿åº¦æ¢ä»¶"""
        conditions = rule.conditions
        schedule_hours = conditions.get('schedule_hours', [])
        min_days_since_last = conditions.get('min_days_since_last', 1)
        
        current_hour = datetime.now(timezone.utc).hour
        
        if current_hour in schedule_hours:
            # ç²å–æ‰€æœ‰å¯ç”¨æ¨¡å‹
            models = await self.model_db.list_model_capabilities(is_available=True)
            
            for model in models:
                # æª¢æŸ¥æœ€å¾ŒåŸºæº–æ¸¬è©¦æ™‚é–“
                if model.last_benchmarked:
                    time_since_last = datetime.now(timezone.utc) - model.last_benchmarked
                    if time_since_last.days < min_days_since_last:
                        continue
                
                # å‰µå»ºåŸºæº–æ¸¬è©¦ä»»å‹™
                await self._create_update_task(
                    provider=model.provider,
                    model_id=model.model_id,
                    update_type=ChangeType.BENCHMARK_RESULT,
                    trigger=rule.trigger_type,
                    priority=rule.priority,
                    context={
                        'rule_id': rule.rule_id,
                        'benchmark_type': 'scheduled',
                        'last_benchmarked': model.last_benchmarked.isoformat() if model.last_benchmarked else None
                    }
                )
    
    async def _check_capability_drift(self, rule: UpdateRule):
        """æª¢æŸ¥èƒ½åŠ›æ¼‚ç§»æ¢ä»¶"""
        conditions = rule.conditions
        drift_threshold = conditions.get('drift_threshold', 0.05)
        window_hours = conditions.get('monitoring_window_hours', 24)
        confidence_threshold = conditions.get('confidence_threshold', 0.8)
        
        # ç²å–æ‰€æœ‰æ¨¡å‹çš„æ€§èƒ½æ­·å²
        models = await self.model_db.list_model_capabilities(is_available=True)
        
        for model in models:
            # æª¢æ¸¬æ€§èƒ½æ¼‚ç§»
            drift_detected = await self._detect_performance_drift(
                provider=model.provider,
                model_id=model.model_id,
                window_hours=window_hours,
                threshold=drift_threshold,
                confidence_threshold=confidence_threshold
            )
            
            if drift_detected:
                await self._create_update_task(
                    provider=model.provider,
                    model_id=model.model_id,
                    update_type=ChangeType.PERFORMANCE_UPDATE,
                    trigger=rule.trigger_type,
                    priority=rule.priority,
                    context={
                        'rule_id': rule.rule_id,
                        'drift_detected': True,
                        'monitoring_window_hours': window_hours
                    }
                )
    
    async def _detect_performance_drift(
        self,
        provider: str,
        model_id: str,
        window_hours: int,
        threshold: float,
        confidence_threshold: float
    ) -> bool:
        """æª¢æ¸¬æ€§èƒ½æ¼‚ç§»"""
        try:
            # ç²å–æ€§èƒ½çµ±è¨ˆ
            stats = self.performance_monitor.get_current_metrics(provider, model_id)
            
            for metric_key, metric_stats in stats.items():
                if not metric_stats or 'values' not in metric_stats:
                    continue
                
                values = metric_stats.get('values', [])
                if len(values) < 20:  # éœ€è¦è¶³å¤ çš„æ¨£æœ¬
                    continue
                
                # è¨ˆç®—æ»‘å‹•å¹³å‡å’Œè¶¨å‹¢
                window_size = min(10, len(values) // 2)
                recent_avg = statistics.mean(values[-window_size:])
                earlier_avg = statistics.mean(values[:window_size])
                
                # è¨ˆç®—ç›¸å°è®ŠåŒ–
                if earlier_avg != 0:
                    relative_change = abs(recent_avg - earlier_avg) / abs(earlier_avg)
                    
                    if relative_change > threshold:
                        # æª¢æŸ¥ç½®ä¿¡åº¦ï¼ˆç°¡åŒ–å¯¦ç¾ï¼‰
                        if len(values) > 30 and window_size >= 10:
                            return True
            
            return False
            
        except Exception as e:
            self.logger.error(f"âŒ Error detecting drift for {provider}/{model_id}: {e}")
            return False
    
    async def _create_update_task(
        self,
        provider: str,
        model_id: str,
        update_type: ChangeType,
        trigger: UpdateTrigger,
        priority: int,
        context: Optional[Dict[str, Any]] = None
    ):
        """å‰µå»ºæ›´æ–°ä»»å‹™"""
        task_id = f"{provider}-{model_id}-{int(time.time())}"
        
        task = UpdateTask(
            task_id=task_id,
            provider=provider,
            model_id=model_id,
            update_type=update_type,
            trigger=trigger,
            priority=priority,
            context=context or {}
        )
        
        await self._schedule_task(task)
    
    async def _schedule_task(self, task: UpdateTask):
        """èª¿åº¦ä»»å‹™"""
        # æª¢æŸ¥æ˜¯å¦å·²æœ‰ç›¸åŒçš„æ´»èºä»»å‹™
        for active_task in self.active_tasks.values():
            if (active_task.provider == task.provider and 
                active_task.model_id == task.model_id and 
                active_task.update_type == task.update_type):
                self.logger.debug(f"Task already active for {task.provider}/{task.model_id}, skipping")
                return
        
        # æ ¹æ“šç­–ç•¥èª¿åº¦ä»»å‹™
        rule_id = task.context.get('rule_id')
        rule = self.update_rules.get(rule_id) if rule_id else None
        
        if rule and rule.strategy == UpdateStrategy.IMMEDIATE:
            # ç«‹å³åŸ·è¡Œ
            self.task_queue.insert(0, task)  # å„ªå…ˆéšŠåˆ—
        else:
            # æ·»åŠ åˆ°æ™®é€šéšŠåˆ—
            self.task_queue.append(task)
        
        # æŒ‰å„ªå…ˆç´šæ’åº
        self.task_queue.sort(key=lambda t: t.priority)
        
        self.logger.debug(f"Scheduled task {task.task_id} (priority: {task.priority})")
    
    async def _get_next_task(self) -> Optional[UpdateTask]:
        """ç²å–ä¸‹ä¸€å€‹ä»»å‹™"""
        if self.task_queue:
            return self.task_queue.pop(0)
        return None
    
    async def _execute_update_task(self, task: UpdateTask) -> UpdateResult:
        """åŸ·è¡Œæ›´æ–°ä»»å‹™"""
        start_time = time.time()
        
        try:
            self.logger.info(f"ğŸ”„ Executing update task {task.task_id} for {task.provider}/{task.model_id}")
            
            result = UpdateResult(task_id=task.task_id, success=False)
            
            if task.update_type == ChangeType.BENCHMARK_RESULT:
                # åŸ·è¡ŒåŸºæº–æ¸¬è©¦
                benchmark_result = await self._run_benchmark_update(task)
                result.success = benchmark_result['success']
                result.version_id = benchmark_result.get('version_id')
                result.changes_applied = benchmark_result.get('changes_applied', [])
                result.error = benchmark_result.get('error')
            
            elif task.update_type == ChangeType.PERFORMANCE_UPDATE:
                # åŸ·è¡Œæ€§èƒ½æ›´æ–°
                perf_result = await self._run_performance_update(task)
                result.success = perf_result['success']
                result.version_id = perf_result.get('version_id')
                result.changes_applied = perf_result.get('changes_applied', [])
                result.error = perf_result.get('error')
            
            # æ›´æ–°çµ±è¨ˆ
            execution_time = (time.time() - start_time) * 1000
            result.execution_time_ms = execution_time
            
            self._update_stats(result)
            
            return result
            
        except Exception as e:
            execution_time = (time.time() - start_time) * 1000
            
            result = UpdateResult(
                task_id=task.task_id,
                success=False,
                error=str(e),
                execution_time_ms=execution_time
            )
            
            self._update_stats(result)
            raise
    
    async def _run_benchmark_update(self, task: UpdateTask) -> Dict[str, Any]:
        """åŸ·è¡ŒåŸºæº–æ¸¬è©¦æ›´æ–°"""
        try:
            # é‹è¡ŒåŸºæº–æ¸¬è©¦
            benchmark_result = await self.benchmark_runner.run_model_benchmark(
                provider=task.provider,
                model_id=task.model_id,
                suite_name="standard"
            )
            
            if benchmark_result['success']:
                # æå–åŸºæº–æ¸¬è©¦çµæœ
                scores = benchmark_result.get('results', {})
                
                # å‰µå»ºç‰ˆæœ¬æ›´æ–°è«‹æ±‚
                update_request = VersionUpdateRequest(
                    provider=task.provider,
                    model_id=task.model_id,
                    changes={
                        'benchmark_scores': scores,
                        'last_benchmarked': datetime.now(timezone.utc).isoformat()
                    },
                    change_type=ChangeType.BENCHMARK_RESULT,
                    change_summary=f"Automated benchmark update - {task.trigger.value}",
                    created_by="dynamic_updater",
                    reason=f"Triggered by rule: {task.context.get('rule_id', 'unknown')}",
                    context=task.context
                )
                
                # åŸ·è¡Œç‰ˆæœ¬åŒ–æ›´æ–°
                version_result = await self.version_control.update_model_capability_with_versioning(
                    update_request
                )
                
                return {
                    'success': version_result['success'],
                    'version_id': version_result.get('version_id'),
                    'changes_applied': version_result.get('changes_applied', []),
                    'benchmark_results': scores
                }
            
            else:
                return {
                    'success': False,
                    'error': benchmark_result.get('error', 'Benchmark failed')
                }
                
        except Exception as e:
            return {
                'success': False,
                'error': str(e)
            }
    
    async def _run_performance_update(self, task: UpdateTask) -> Dict[str, Any]:
        """åŸ·è¡Œæ€§èƒ½æ›´æ–°"""
        try:
            # ç²å–ç•¶å‰æ€§èƒ½æŒ‡æ¨™
            current_metrics = self.performance_monitor.get_current_metrics(
                task.provider, 
                task.model_id
            )
            
            # è¨ˆç®—æ–°çš„èƒ½åŠ›è©•åˆ†
            updates = {}
            
            for metric_key, stats in current_metrics.items():
                if not stats:
                    continue
                
                metric_type = metric_key.split('/')[-1]
                mean_value = stats.get('mean')
                
                if mean_value is not None:
                    if metric_type == 'latency':
                        updates['avg_latency_ms'] = mean_value
                    elif metric_type == 'accuracy':
                        updates['accuracy_score'] = min(1.0, mean_value)
                    elif metric_type == 'success_rate':
                        # æ›´æ–°æ•´é«”èƒ½åŠ›è©•åˆ†
                        capability_score = self._calculate_updated_capability_score(stats, current_metrics)
                        if capability_score is not None:
                            updates['capability_score'] = capability_score
            
            if not updates:
                return {
                    'success': False,
                    'error': 'No performance updates to apply'
                }
            
            # å‰µå»ºç‰ˆæœ¬æ›´æ–°è«‹æ±‚
            update_request = VersionUpdateRequest(
                provider=task.provider,
                model_id=task.model_id,
                changes=updates,
                change_type=ChangeType.PERFORMANCE_UPDATE,
                change_summary=f"Automated performance update - {task.trigger.value}",
                created_by="dynamic_updater",
                reason=f"Performance monitoring detected changes: {task.context.get('trigger_reasons', [])}",
                context=task.context
            )
            
            # åŸ·è¡Œç‰ˆæœ¬åŒ–æ›´æ–°
            version_result = await self.version_control.update_model_capability_with_versioning(
                update_request
            )
            
            return {
                'success': version_result['success'],
                'version_id': version_result.get('version_id'),
                'changes_applied': version_result.get('changes_applied', []),
                'performance_updates': updates
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e)
            }
    
    def _calculate_updated_capability_score(
        self, 
        primary_stats: Dict[str, float], 
        all_metrics: Dict[str, Dict[str, float]]
    ) -> Optional[float]:
        """è¨ˆç®—æ›´æ–°çš„èƒ½åŠ›è©•åˆ†"""
        try:
            scores = []
            weights = {
                'success_rate': 0.4,
                'accuracy': 0.3,
                'latency': 0.2,
                'error_rate': 0.1
            }
            
            total_weight = 0.0
            
            for metric_key, metric_stats in all_metrics.items():
                metric_type = metric_key.split('/')[-1]
                
                if metric_type in weights:
                    mean_value = metric_stats.get('mean')
                    
                    if mean_value is not None:
                        weight = weights[metric_type]
                        
                        if metric_type == 'latency':
                            # å»¶é²è½‰æ›ç‚ºè©•åˆ† (è¶Šä½è¶Šå¥½)
                            score = max(0.0, min(1.0, 2.0 - (mean_value / 3000.0)))
                        elif metric_type == 'error_rate':
                            # éŒ¯èª¤ç‡è½‰æ›ç‚ºè©•åˆ† (è¶Šä½è¶Šå¥½)
                            score = max(0.0, 1.0 - mean_value)
                        else:
                            # success_rate, accuracy (è¶Šé«˜è¶Šå¥½)
                            score = min(1.0, mean_value)
                        
                        scores.append(score * weight)
                        total_weight += weight
            
            if total_weight > 0:
                return sum(scores) / total_weight
            
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ Error calculating capability score: {e}")
            return None
    
    def _update_stats(self, result: UpdateResult):
        """æ›´æ–°çµ±è¨ˆä¿¡æ¯"""
        self.update_stats['total_updates'] += 1
        
        if result.success:
            self.update_stats['successful_updates'] += 1
        else:
            self.update_stats['failed_updates'] += 1
        
        # æ›´æ–°å¹³å‡åŸ·è¡Œæ™‚é–“
        total_time = self.update_stats['average_execution_time'] * (self.update_stats['total_updates'] - 1)
        self.update_stats['average_execution_time'] = (total_time + result.execution_time_ms) / self.update_stats['total_updates']
    
    def _handle_performance_alert(self, message: str, alert_data: Dict[str, Any]):
        """è™•ç†æ€§èƒ½å‘Šè­¦"""
        try:
            provider = alert_data.get('provider')
            model_id = alert_data.get('model_id')
            
            if provider and model_id:
                # å‰µå»ºå‘Šè­¦é©…å‹•çš„æ›´æ–°ä»»å‹™
                asyncio.create_task(self._create_update_task(
                    provider=provider,
                    model_id=model_id,
                    update_type=ChangeType.PERFORMANCE_UPDATE,
                    trigger=UpdateTrigger.ALERT_BASED,
                    priority=1,  # é«˜å„ªå…ˆç´š
                    context={
                        'alert_message': message,
                        'alert_data': alert_data,
                        'severity': alert_data.get('severity', 'medium')
                    }
                ))
        except Exception as e:
            self.logger.error(f"âŒ Error handling performance alert: {e}")
    
    async def _process_scheduled_updates(self):
        """è™•ç†é å®šçš„æ›´æ–°"""
        # æª¢æŸ¥æ˜¯å¦æœ‰éœ€è¦åŸ·è¡Œçš„é å®šä»»å‹™
        current_time = datetime.now(timezone.utc)
        
        for task in list(self.task_queue):
            if task.scheduled_at and task.scheduled_at <= current_time:
                # ç§»å‹•åˆ°éšŠåˆ—å‰ç«¯
                self.task_queue.remove(task)
                self.task_queue.insert(0, task)
    
    def get_update_status(self) -> Dict[str, Any]:
        """ç²å–æ›´æ–°å™¨ç‹€æ…‹"""
        return {
            'running': self._running,
            'active_tasks': len(self.active_tasks),
            'queued_tasks': len(self.task_queue),
            'enabled_rules': len([r for r in self.update_rules.values() if r.enabled]),
            'total_rules': len(self.update_rules),
            'statistics': self.update_stats.copy(),
            'worker_threads': self.max_concurrent_updates
        }
    
    def get_rule_status(self) -> Dict[str, Dict[str, Any]]:
        """ç²å–è¦å‰‡ç‹€æ…‹"""
        return {
            rule_id: {
                'enabled': rule.enabled,
                'trigger_type': rule.trigger_type.value,
                'strategy': rule.strategy.value,
                'priority': rule.priority,
                'cooldown_minutes': rule.cooldown_minutes,
                'conditions': rule.conditions
            }
            for rule_id, rule in self.update_rules.items()
        }
    
    async def manual_update_request(
        self,
        provider: str,
        model_id: str,
        update_type: ChangeType = ChangeType.PERFORMANCE_UPDATE,
        priority: int = 1
    ) -> str:
        """æ‰‹å‹•è«‹æ±‚æ›´æ–°"""
        task_id = await self._create_update_task(
            provider=provider,
            model_id=model_id,
            update_type=update_type,
            trigger=UpdateTrigger.MANUAL_REQUEST,
            priority=priority,
            context={'requested_by': 'manual'}
        )
        
        self.logger.info(f"âœ… Manual update request created: {task_id}")
        return task_id