#!/usr/bin/env python3
"""
A/B Testing and Canary Deployment System
A/Bæ¸¬è©¦å’Œç°åº¦æ›´æ–°ç³»çµ± - GPT-OSSæ•´åˆä»»å‹™1.2.3
"""

import asyncio
import logging
import time
import random
import statistics
from datetime import datetime, timezone, timedelta
from typing import Dict, Any, List, Optional, Tuple, Set
from dataclasses import dataclass, field
from enum import Enum
import hashlib
from contextlib import asynccontextmanager

from ..database.model_capability_db import ModelCapabilityDB
from ..monitoring.performance_monitor import PerformanceMonitor, MetricType
from .model_version_control import ModelVersionControl, VersionUpdateRequest, ChangeType, DeploymentStage

logger = logging.getLogger(__name__)

class ExperimentType(Enum):
    """å¯¦é©—é¡å‹"""
    AB_TEST = "ab_test"
    CANARY_DEPLOYMENT = "canary_deployment"
    GRADUAL_ROLLOUT = "gradual_rollout"
    BLUE_GREEN = "blue_green"
    SHADOW_TESTING = "shadow_testing"

class ExperimentStatus(Enum):
    """å¯¦é©—ç‹€æ…‹"""
    DRAFT = "draft"
    RUNNING = "running"
    PAUSED = "paused"
    COMPLETED = "completed"
    FAILED = "failed"
    ROLLED_BACK = "rolled_back"

class TrafficSplitStrategy(Enum):
    """æµé‡åˆ†å‰²ç­–ç•¥"""
    RANDOM = "random"
    USER_ID_HASH = "user_id_hash"
    GEOGRAPHIC = "geographic"
    FEATURE_FLAG = "feature_flag"
    PERCENTAGE = "percentage"

class ExperimentDecision(Enum):
    """å¯¦é©—æ±ºç­–"""
    CONTINUE = "continue"
    PROMOTE_VARIANT = "promote_variant"
    ROLLBACK = "rollback"
    EXTEND = "extend"
    TERMINATE = "terminate"

@dataclass
class TrafficSplitConfig:
    """æµé‡åˆ†å‰²é…ç½®"""
    strategy: TrafficSplitStrategy
    control_percentage: float  # æ§åˆ¶çµ„ç™¾åˆ†æ¯” (0-100)
    variant_percentage: float  # è®Šé«”çµ„ç™¾åˆ†æ¯” (0-100)
    ramp_up_schedule: Optional[List[Tuple[datetime, float]]] = None  # æ¼¸é€²å¼éƒ¨ç½²æ™‚é–“è¡¨
    filters: Dict[str, Any] = field(default_factory=dict)  # éæ¿¾æ¢ä»¶

@dataclass
class ExperimentVariant:
    """å¯¦é©—è®Šé«”"""
    variant_id: str
    variant_name: str
    provider: str
    model_id: str
    model_version: str
    configuration: Dict[str, Any]
    traffic_percentage: float
    
    # æ€§èƒ½æŒ‡æ¨™
    metrics: Dict[str, List[float]] = field(default_factory=dict)
    sample_count: int = 0
    
    # ç‹€æ…‹
    is_control: bool = False
    is_enabled: bool = True

@dataclass
class ExperimentMetrics:
    """å¯¦é©—æŒ‡æ¨™"""
    variant_id: str
    metric_name: str
    values: List[float]
    timestamps: List[datetime]
    
    def get_statistics(self) -> Dict[str, float]:
        """ç²å–çµ±è¨ˆä¿¡æ¯"""
        if not self.values:
            return {}
        
        return {
            'mean': statistics.mean(self.values),
            'median': statistics.median(self.values),
            'std': statistics.stdev(self.values) if len(self.values) > 1 else 0.0,
            'min': min(self.values),
            'max': max(self.values),
            'count': len(self.values),
            'p95': statistics.quantiles(self.values, n=20)[18] if len(self.values) >= 20 else max(self.values),
            'p99': statistics.quantiles(self.values, n=100)[98] if len(self.values) >= 100 else max(self.values)
        }

@dataclass
class StatisticalTestResult:
    """çµ±è¨ˆæª¢é©—çµæœ"""
    test_name: str
    p_value: float
    confidence_level: float
    is_significant: bool
    effect_size: float
    power: float
    sample_size_recommendation: Optional[int] = None

@dataclass
class Experiment:
    """A/Bæ¸¬è©¦å¯¦é©—"""
    experiment_id: str
    experiment_name: str
    experiment_type: ExperimentType
    description: str
    
    # å¯¦é©—é…ç½®
    variants: Dict[str, ExperimentVariant]
    traffic_split: TrafficSplitConfig
    
    # å¯¦é©—æ™‚é–“
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    planned_duration_hours: int = 24
    
    # ç‹€æ…‹
    status: ExperimentStatus = ExperimentStatus.DRAFT
    created_at: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    created_by: str = "system"
    
    # å¯¦é©—æŒ‡æ¨™
    primary_metrics: List[str] = field(default_factory=lambda: ["capability_score", "latency"])
    secondary_metrics: List[str] = field(default_factory=lambda: ["accuracy", "cost"])
    
    # åœæ­¢æ¢ä»¶
    stop_conditions: Dict[str, Any] = field(default_factory=dict)
    significance_threshold: float = 0.05
    minimum_sample_size: int = 100
    
    # çµæœ
    results: Dict[str, Any] = field(default_factory=dict)
    decision: Optional[ExperimentDecision] = None
    decision_reason: str = ""

class ABTestingSystem:
    """
    A/Bæ¸¬è©¦å’Œç°åº¦æ›´æ–°ç³»çµ±
    
    åŠŸèƒ½ï¼š
    1. A/Bæ¸¬è©¦å¯¦é©—ç®¡ç†
    2. ç°åº¦/é‡‘çµ²é›€éƒ¨ç½²
    3. æµé‡åˆ†å‰²å’Œè·¯ç”±
    4. çµ±è¨ˆé¡¯è‘—æ€§æª¢é©—
    5. è‡ªå‹•æ±ºç­–å’Œå›æ»¾
    6. å¯¦é©—çµæœåˆ†æ
    """
    
    def __init__(
        self,
        model_db: Optional[ModelCapabilityDB] = None,
        performance_monitor: Optional[PerformanceMonitor] = None,
        version_control: Optional[ModelVersionControl] = None
    ):
        """åˆå§‹åŒ–A/Bæ¸¬è©¦ç³»çµ±"""
        self.model_db = model_db or ModelCapabilityDB()
        self.performance_monitor = performance_monitor or PerformanceMonitor()
        self.version_control = version_control or ModelVersionControl()
        
        self.logger = logger
        
        # å¯¦é©—ç®¡ç†
        self.experiments: Dict[str, Experiment] = {}
        self.active_experiments: Dict[str, Experiment] = {}
        
        # æµé‡è·¯ç”±ç·©å­˜
        self.routing_cache: Dict[str, str] = {}  # user_id -> variant_id
        self.cache_ttl: Dict[str, datetime] = {}
        
        # é‹è¡Œæ™‚ç‹€æ…‹
        self._running = False
        self._monitor_task: Optional[asyncio.Task] = None
        
        # çµ±è¨ˆ
        self.experiment_stats = {
            'total_experiments': 0,
            'active_experiments': 0,
            'completed_experiments': 0,
            'successful_promotions': 0,
            'rollbacks': 0
        }
    
    async def start(self):
        """å•Ÿå‹•A/Bæ¸¬è©¦ç³»çµ±"""
        if not self._running:
            self._running = True
            self._monitor_task = asyncio.create_task(self._monitoring_loop())
            self.logger.info("âœ… A/B testing system started")
    
    async def stop(self):
        """åœæ­¢A/Bæ¸¬è©¦ç³»çµ±"""
        if self._running:
            self._running = False
            
            if self._monitor_task:
                self._monitor_task.cancel()
                try:
                    await self._monitor_task
                except asyncio.CancelledError:
                    pass
            
            self.logger.info("âœ… A/B testing system stopped")
    
    async def _monitoring_loop(self):
        """ç›£æ§ä¸»å¾ªç’°"""
        while self._running:
            try:
                await self._update_experiment_metrics()
                await self._evaluate_stop_conditions()
                await self._cleanup_cache()
                
                await asyncio.sleep(60)  # æ¯åˆ†é˜æª¢æŸ¥ä¸€æ¬¡
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                self.logger.error(f"âŒ Error in A/B testing monitoring loop: {e}")
                await asyncio.sleep(10)
    
    # ==================== å¯¦é©—ç®¡ç† ====================
    
    async def create_experiment(
        self,
        experiment_name: str,
        experiment_type: ExperimentType,
        description: str,
        control_variant: Dict[str, Any],
        test_variants: List[Dict[str, Any]],
        traffic_split: TrafficSplitConfig,
        primary_metrics: List[str],
        planned_duration_hours: int = 24,
        **kwargs
    ) -> str:
        """
        å‰µå»ºA/Bæ¸¬è©¦å¯¦é©—
        
        Args:
            experiment_name: å¯¦é©—åç¨±
            experiment_type: å¯¦é©—é¡å‹
            description: å¯¦é©—æè¿°
            control_variant: æ§åˆ¶çµ„è®Šé«”é…ç½®
            test_variants: æ¸¬è©¦çµ„è®Šé«”é…ç½®åˆ—è¡¨
            traffic_split: æµé‡åˆ†å‰²é…ç½®
            primary_metrics: ä¸»è¦æŒ‡æ¨™
            planned_duration_hours: è¨ˆåŠƒæŒçºŒæ™‚é–“ï¼ˆå°æ™‚ï¼‰
            **kwargs: å…¶ä»–åƒæ•¸
            
        Returns:
            å¯¦é©—ID
        """
        try:
            experiment_id = f"exp_{int(time.time())}_{experiment_name}".replace(' ', '_')
            
            # å‰µå»ºè®Šé«”
            variants = {}
            
            # å‰µå»ºæ§åˆ¶çµ„
            control_id = f"{experiment_id}_control"
            variants[control_id] = ExperimentVariant(
                variant_id=control_id,
                variant_name="Control",
                provider=control_variant['provider'],
                model_id=control_variant['model_id'],
                model_version=control_variant.get('model_version', 'current'),
                configuration=control_variant.get('configuration', {}),
                traffic_percentage=traffic_split.control_percentage,
                is_control=True
            )
            
            # å‰µå»ºæ¸¬è©¦çµ„è®Šé«”
            for i, test_variant in enumerate(test_variants):
                variant_id = f"{experiment_id}_variant_{i+1}"
                variants[variant_id] = ExperimentVariant(
                    variant_id=variant_id,
                    variant_name=test_variant.get('name', f"Variant {i+1}"),
                    provider=test_variant['provider'],
                    model_id=test_variant['model_id'],
                    model_version=test_variant.get('model_version', 'latest'),
                    configuration=test_variant.get('configuration', {}),
                    traffic_percentage=traffic_split.variant_percentage / len(test_variants)
                )
            
            # å‰µå»ºå¯¦é©—
            experiment = Experiment(
                experiment_id=experiment_id,
                experiment_name=experiment_name,
                experiment_type=experiment_type,
                description=description,
                variants=variants,
                traffic_split=traffic_split,
                planned_duration_hours=planned_duration_hours,
                primary_metrics=primary_metrics,
                secondary_metrics=kwargs.get('secondary_metrics', ['accuracy', 'cost']),
                significance_threshold=kwargs.get('significance_threshold', 0.05),
                minimum_sample_size=kwargs.get('minimum_sample_size', 100),
                stop_conditions=kwargs.get('stop_conditions', {}),
                created_by=kwargs.get('created_by', 'system')
            )
            
            self.experiments[experiment_id] = experiment
            self.experiment_stats['total_experiments'] += 1
            
            self.logger.info(f"âœ… Created experiment: {experiment_id}")
            return experiment_id
            
        except Exception as e:
            self.logger.error(f"âŒ Failed to create experiment: {e}")
            raise
    
    async def start_experiment(self, experiment_id: str) -> bool:
        """å•Ÿå‹•å¯¦é©—"""
        try:
            if experiment_id not in self.experiments:
                raise ValueError(f"Experiment {experiment_id} not found")
            
            experiment = self.experiments[experiment_id]
            
            if experiment.status != ExperimentStatus.DRAFT:
                raise ValueError(f"Experiment {experiment_id} is not in draft status")
            
            # é©—è­‰å¯¦é©—é…ç½®
            await self._validate_experiment_config(experiment)
            
            # å•Ÿå‹•å¯¦é©—
            experiment.status = ExperimentStatus.RUNNING
            experiment.start_time = datetime.now(timezone.utc)
            experiment.end_time = experiment.start_time + timedelta(hours=experiment.planned_duration_hours)
            
            # æ·»åŠ åˆ°æ´»èºå¯¦é©—
            self.active_experiments[experiment_id] = experiment
            self.experiment_stats['active_experiments'] += 1
            
            # å¦‚æœæ˜¯ç°åº¦éƒ¨ç½²ï¼Œè¨­ç½®ç‰ˆæœ¬éšæ®µ
            if experiment.experiment_type == ExperimentType.CANARY_DEPLOYMENT:
                await self._setup_canary_deployment(experiment)
            
            self.logger.info(f"âœ… Started experiment: {experiment_id}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Failed to start experiment {experiment_id}: {e}")
            return False
    
    async def _validate_experiment_config(self, experiment: Experiment):
        """é©—è­‰å¯¦é©—é…ç½®"""
        # æª¢æŸ¥è®Šé«”æ¨¡å‹æ˜¯å¦å­˜åœ¨
        for variant in experiment.variants.values():
            model = await self.model_db.get_model_capability(variant.provider, variant.model_id)
            if not model:
                raise ValueError(f"Model {variant.provider}/{variant.model_id} not found")
            if not model.is_available:
                raise ValueError(f"Model {variant.provider}/{variant.model_id} is not available")
        
        # æª¢æŸ¥æµé‡åˆ†å‰²æ˜¯å¦åˆç†
        total_traffic = sum(v.traffic_percentage for v in experiment.variants.values())
        if abs(total_traffic - 100.0) > 0.01:
            raise ValueError(f"Traffic split does not add up to 100%: {total_traffic}")
    
    async def _setup_canary_deployment(self, experiment: Experiment):
        """è¨­ç½®ç°åº¦éƒ¨ç½²"""
        for variant in experiment.variants.values():
            if not variant.is_control:
                # å°‡æ¸¬è©¦è®Šé«”è¨­ç‚ºcanaryéšæ®µ
                await self.version_control.promote_version_stage(
                    provider=variant.provider,
                    model_id=variant.model_id,
                    version=variant.model_version,
                    target_stage=DeploymentStage.CANARY
                )
    
    # ==================== æµé‡è·¯ç”± ====================
    
    async def route_request(
        self,
        experiment_id: str,
        user_id: str,
        context: Optional[Dict[str, Any]] = None
    ) -> Optional[ExperimentVariant]:
        """
        è·¯ç”±è«‹æ±‚åˆ°å¯¦é©—è®Šé«”
        
        Args:
            experiment_id: å¯¦é©—ID
            user_id: ç”¨æˆ¶ID
            context: è«‹æ±‚ä¸Šä¸‹æ–‡
            
        Returns:
            é¸ä¸­çš„è®Šé«”ï¼Œå¦‚æœå¯¦é©—ä¸æ´»èºå‰‡è¿”å›None
        """
        try:
            if experiment_id not in self.active_experiments:
                return None
            
            experiment = self.active_experiments[experiment_id]
            
            # æª¢æŸ¥å¯¦é©—æ˜¯å¦ä»ç„¶æ´»èº
            if experiment.status != ExperimentStatus.RUNNING:
                return None
            
            # æª¢æŸ¥ç·©å­˜
            cache_key = f"{experiment_id}:{user_id}"
            if cache_key in self.routing_cache:
                cache_time = self.cache_ttl.get(cache_key)
                if cache_time and datetime.now(timezone.utc) < cache_time:
                    variant_id = self.routing_cache[cache_key]
                    return experiment.variants.get(variant_id)
            
            # é€²è¡Œæµé‡åˆ†å‰²
            selected_variant = await self._split_traffic(experiment, user_id, context)
            
            # æ›´æ–°ç·©å­˜
            if selected_variant:
                self.routing_cache[cache_key] = selected_variant.variant_id
                self.cache_ttl[cache_key] = datetime.now(timezone.utc) + timedelta(hours=1)
            
            return selected_variant
            
        except Exception as e:
            self.logger.error(f"âŒ Error routing request for experiment {experiment_id}: {e}")
            return None
    
    async def _split_traffic(
        self,
        experiment: Experiment,
        user_id: str,
        context: Optional[Dict[str, Any]]
    ) -> Optional[ExperimentVariant]:
        """åŸ·è¡Œæµé‡åˆ†å‰²"""
        try:
            strategy = experiment.traffic_split.strategy
            
            if strategy == TrafficSplitStrategy.RANDOM:
                return self._random_split(experiment)
            
            elif strategy == TrafficSplitStrategy.USER_ID_HASH:
                return self._hash_based_split(experiment, user_id)
            
            elif strategy == TrafficSplitStrategy.PERCENTAGE:
                return self._percentage_split(experiment)
            
            elif strategy == TrafficSplitStrategy.FEATURE_FLAG:
                return self._feature_flag_split(experiment, context)
            
            else:
                # é»˜èªä½¿ç”¨éš¨æ©Ÿåˆ†å‰²
                return self._random_split(experiment)
                
        except Exception as e:
            self.logger.error(f"âŒ Error in traffic splitting: {e}")
            return None
    
    def _random_split(self, experiment: Experiment) -> ExperimentVariant:
        """éš¨æ©Ÿæµé‡åˆ†å‰²"""
        rand_val = random.random() * 100
        cumulative = 0
        
        for variant in experiment.variants.values():
            cumulative += variant.traffic_percentage
            if rand_val <= cumulative:
                return variant
        
        # é»˜èªè¿”å›æ§åˆ¶çµ„
        return next(v for v in experiment.variants.values() if v.is_control)
    
    def _hash_based_split(self, experiment: Experiment, user_id: str) -> ExperimentVariant:
        """åŸºæ–¼ç”¨æˆ¶IDå“ˆå¸Œçš„åˆ†å‰²"""
        # ä½¿ç”¨ç”¨æˆ¶IDå’Œå¯¦é©—IDå‰µå»ºç¢ºå®šæ€§å“ˆå¸Œ
        hash_input = f"{experiment.experiment_id}:{user_id}"
        hash_value = int(hashlib.md5(hash_input.encode()).hexdigest(), 16)
        bucket = (hash_value % 100) + 1  # 1-100
        
        cumulative = 0
        for variant in experiment.variants.values():
            cumulative += variant.traffic_percentage
            if bucket <= cumulative:
                return variant
        
        return next(v for v in experiment.variants.values() if v.is_control)
    
    def _percentage_split(self, experiment: Experiment) -> ExperimentVariant:
        """åŸºæ–¼ç™¾åˆ†æ¯”çš„åˆ†å‰²"""
        # æª¢æŸ¥æ˜¯å¦æœ‰æ¼¸é€²å¼éƒ¨ç½²è¨ˆåŠƒ
        ramp_up = experiment.traffic_split.ramp_up_schedule
        if ramp_up:
            current_time = datetime.now(timezone.utc)
            
            # æ‰¾åˆ°ç•¶å‰æ‡‰è©²ä½¿ç”¨çš„æµé‡ç™¾åˆ†æ¯”
            for schedule_time, percentage in ramp_up:
                if current_time >= schedule_time:
                    # æ›´æ–°è®Šé«”æµé‡ç™¾åˆ†æ¯”
                    for variant in experiment.variants.values():
                        if not variant.is_control:
                            variant.traffic_percentage = percentage
                            # ç›¸æ‡‰èª¿æ•´æ§åˆ¶çµ„
                            control_variant = next(v for v in experiment.variants.values() if v.is_control)
                            control_variant.traffic_percentage = 100 - percentage
                    break
        
        return self._random_split(experiment)
    
    def _feature_flag_split(
        self,
        experiment: Experiment,
        context: Optional[Dict[str, Any]]
    ) -> ExperimentVariant:
        """åŸºæ–¼ç‰¹æ€§æ¨™èªŒçš„åˆ†å‰²"""
        if not context:
            return next(v for v in experiment.variants.values() if v.is_control)
        
        # æª¢æŸ¥ç‰¹æ€§æ¨™èªŒ
        feature_flags = context.get('feature_flags', {})
        experiment_flag = feature_flags.get(experiment.experiment_id, False)
        
        if experiment_flag:
            # å¦‚æœå•Ÿç”¨ç‰¹æ€§æ¨™èªŒï¼Œä½¿ç”¨æ¸¬è©¦è®Šé«”
            test_variants = [v for v in experiment.variants.values() if not v.is_control]
            if test_variants:
                return random.choice(test_variants)
        
        # é»˜èªä½¿ç”¨æ§åˆ¶çµ„
        return next(v for v in experiment.variants.values() if v.is_control)
    
    # ==================== æŒ‡æ¨™æ”¶é›†å’Œåˆ†æ ====================
    
    async def record_experiment_metric(
        self,
        experiment_id: str,
        variant_id: str,
        metric_name: str,
        value: float,
        user_id: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None
    ):
        """è¨˜éŒ„å¯¦é©—æŒ‡æ¨™"""
        try:
            if experiment_id not in self.active_experiments:
                return
            
            experiment = self.active_experiments[experiment_id]
            
            if variant_id not in experiment.variants:
                return
            
            variant = experiment.variants[variant_id]
            
            # è¨˜éŒ„åˆ°è®Šé«”æŒ‡æ¨™
            if metric_name not in variant.metrics:
                variant.metrics[metric_name] = []
            
            variant.metrics[metric_name].append(value)
            variant.sample_count += 1
            
            self.logger.debug(f"ğŸ“Š Recorded metric {metric_name}={value} for {experiment_id}/{variant_id}")
            
        except Exception as e:
            self.logger.error(f"âŒ Error recording experiment metric: {e}")
    
    async def _update_experiment_metrics(self):
        """æ›´æ–°å¯¦é©—æŒ‡æ¨™"""
        for experiment in self.active_experiments.values():
            try:
                await self._collect_performance_metrics(experiment)
            except Exception as e:
                self.logger.error(f"âŒ Error updating metrics for experiment {experiment.experiment_id}: {e}")
    
    async def _collect_performance_metrics(self, experiment: Experiment):
        """æ”¶é›†æ€§èƒ½æŒ‡æ¨™"""
        for variant in experiment.variants.values():
            # å¾æ€§èƒ½ç›£æ§å™¨ç²å–æŒ‡æ¨™
            current_metrics = self.performance_monitor.get_current_metrics(
                variant.provider, variant.model_id
            )
            
            for metric_key, metric_stats in current_metrics.items():
                metric_type = metric_key.split('/')[-1]
                
                if metric_type in experiment.primary_metrics or metric_type in experiment.secondary_metrics:
                    mean_value = metric_stats.get('mean')
                    
                    if mean_value is not None:
                        await self.record_experiment_metric(
                            experiment.experiment_id,
                            variant.variant_id,
                            metric_type,
                            mean_value
                        )
    
    # ==================== çµ±è¨ˆåˆ†æ ====================
    
    async def analyze_experiment_results(self, experiment_id: str) -> Dict[str, Any]:
        """åˆ†æå¯¦é©—çµæœ"""
        try:
            if experiment_id not in self.experiments:
                raise ValueError(f"Experiment {experiment_id} not found")
            
            experiment = self.experiments[experiment_id]
            
            # ç²å–æ§åˆ¶çµ„å’Œæ¸¬è©¦çµ„
            control_variant = next(v for v in experiment.variants.values() if v.is_control)
            test_variants = [v for v in experiment.variants.values() if not v.is_control]
            
            analysis_results = {
                'experiment_id': experiment_id,
                'experiment_name': experiment.experiment_name,
                'status': experiment.status.value,
                'duration_hours': self._calculate_experiment_duration(experiment),
                'sample_sizes': {},
                'metric_comparisons': {},
                'statistical_tests': {},
                'recommendations': []
            }
            
            # è¨ˆç®—æ¨£æœ¬å¤§å°
            for variant in experiment.variants.values():
                analysis_results['sample_sizes'][variant.variant_id] = variant.sample_count
            
            # å°æ¯å€‹ä¸»è¦æŒ‡æ¨™é€²è¡Œåˆ†æ
            for metric_name in experiment.primary_metrics:
                metric_analysis = await self._analyze_metric(
                    metric_name, control_variant, test_variants, experiment.significance_threshold
                )
                analysis_results['metric_comparisons'][metric_name] = metric_analysis
            
            # ç”Ÿæˆå»ºè­°
            analysis_results['recommendations'] = self._generate_experiment_recommendations(
                experiment, analysis_results
            )
            
            # æ›´æ–°å¯¦é©—çµæœ
            experiment.results = analysis_results
            
            return analysis_results
            
        except Exception as e:
            self.logger.error(f"âŒ Error analyzing experiment results: {e}")
            return {'error': str(e)}
    
    async def _analyze_metric(
        self,
        metric_name: str,
        control_variant: ExperimentVariant,
        test_variants: List[ExperimentVariant],
        significance_threshold: float
    ) -> Dict[str, Any]:
        """åˆ†æå–®å€‹æŒ‡æ¨™"""
        try:
            control_values = control_variant.metrics.get(metric_name, [])
            
            metric_analysis = {
                'control_stats': self._calculate_metric_stats(control_values),
                'variant_comparisons': []
            }
            
            for test_variant in test_variants:
                test_values = test_variant.metrics.get(metric_name, [])
                
                if len(control_values) < 10 or len(test_values) < 10:
                    # æ¨£æœ¬ä¸è¶³
                    comparison = {
                        'variant_id': test_variant.variant_id,
                        'variant_name': test_variant.variant_name,
                        'test_stats': self._calculate_metric_stats(test_values),
                        'statistical_test': {
                            'test_name': 'insufficient_data',
                            'is_significant': False,
                            'p_value': None,
                            'message': 'Insufficient sample size for statistical testing'
                        }
                    }
                else:
                    # é€²è¡Œçµ±è¨ˆæª¢é©—
                    test_result = self._perform_statistical_test(
                        control_values, test_values, significance_threshold
                    )
                    
                    comparison = {
                        'variant_id': test_variant.variant_id,
                        'variant_name': test_variant.variant_name,
                        'test_stats': self._calculate_metric_stats(test_values),
                        'statistical_test': {
                            'test_name': test_result.test_name,
                            'is_significant': test_result.is_significant,
                            'p_value': test_result.p_value,
                            'effect_size': test_result.effect_size,
                            'confidence_level': test_result.confidence_level
                        },
                        'improvement': self._calculate_improvement(control_values, test_values)
                    }
                
                metric_analysis['variant_comparisons'].append(comparison)
            
            return metric_analysis
            
        except Exception as e:
            self.logger.error(f"âŒ Error analyzing metric {metric_name}: {e}")
            return {'error': str(e)}
    
    def _calculate_metric_stats(self, values: List[float]) -> Dict[str, float]:
        """è¨ˆç®—æŒ‡æ¨™çµ±è¨ˆä¿¡æ¯"""
        if not values:
            return {}
        
        return {
            'mean': statistics.mean(values),
            'median': statistics.median(values),
            'std': statistics.stdev(values) if len(values) > 1 else 0.0,
            'min': min(values),
            'max': max(values),
            'count': len(values)
        }
    
    def _perform_statistical_test(
        self,
        control_values: List[float],
        test_values: List[float],
        significance_threshold: float
    ) -> StatisticalTestResult:
        """åŸ·è¡Œçµ±è¨ˆé¡¯è‘—æ€§æª¢é©—"""
        try:
            # ç°¡åŒ–çš„tæª¢é©—å¯¦ç¾
            import scipy.stats as stats
            
            # åŸ·è¡Œç¨ç«‹æ¨£æœ¬tæª¢é©—
            t_stat, p_value = stats.ttest_ind(test_values, control_values)
            
            is_significant = p_value < significance_threshold
            
            # è¨ˆç®—æ•ˆæ‡‰å¤§å°ï¼ˆCohen's dï¼‰
            control_mean = statistics.mean(control_values)
            test_mean = statistics.mean(test_values)
            pooled_std = statistics.stdev(control_values + test_values)
            
            effect_size = (test_mean - control_mean) / pooled_std if pooled_std > 0 else 0.0
            
            return StatisticalTestResult(
                test_name="independent_t_test",
                p_value=p_value,
                confidence_level=1 - significance_threshold,
                is_significant=is_significant,
                effect_size=effect_size,
                power=0.8  # ç°¡åŒ–å‡è¨­
            )
            
        except ImportError:
            # å¦‚æœæ²’æœ‰scipyï¼Œä½¿ç”¨ç°¡åŒ–æª¢é©—
            return self._simple_significance_test(
                control_values, test_values, significance_threshold
            )
        except Exception as e:
            self.logger.error(f"âŒ Error in statistical test: {e}")
            return StatisticalTestResult(
                test_name="error",
                p_value=1.0,
                confidence_level=0.0,
                is_significant=False,
                effect_size=0.0,
                power=0.0
            )
    
    def _simple_significance_test(
        self,
        control_values: List[float],
        test_values: List[float],
        significance_threshold: float
    ) -> StatisticalTestResult:
        """ç°¡åŒ–çš„é¡¯è‘—æ€§æª¢é©—"""
        try:
            control_mean = statistics.mean(control_values)
            test_mean = statistics.mean(test_values)
            
            # ç°¡åŒ–çš„å·®ç•°æª¢é©—
            relative_difference = abs(test_mean - control_mean) / abs(control_mean) if control_mean != 0 else 0
            
            # ç°¡å–®è¦å‰‡ï¼šå¦‚æœå·®ç•°è¶…é10%ä¸”æ¨£æœ¬è¶³å¤ ï¼Œèªç‚ºé¡¯è‘—
            is_significant = (
                relative_difference > 0.1 and
                len(control_values) >= 30 and
                len(test_values) >= 30
            )
            
            # æ¨¡æ“¬på€¼
            p_value = 0.01 if is_significant else 0.5
            
            effect_size = (test_mean - control_mean) / control_mean if control_mean != 0 else 0
            
            return StatisticalTestResult(
                test_name="simple_difference_test",
                p_value=p_value,
                confidence_level=1 - significance_threshold,
                is_significant=is_significant,
                effect_size=effect_size,
                power=0.8
            )
            
        except Exception as e:
            self.logger.error(f"âŒ Error in simple significance test: {e}")
            return StatisticalTestResult(
                test_name="error",
                p_value=1.0,
                confidence_level=0.0,
                is_significant=False,
                effect_size=0.0,
                power=0.0
            )
    
    def _calculate_improvement(self, control_values: List[float], test_values: List[float]) -> Dict[str, float]:
        """è¨ˆç®—æ”¹é€²å¹…åº¦"""
        if not control_values or not test_values:
            return {}
        
        control_mean = statistics.mean(control_values)
        test_mean = statistics.mean(test_values)
        
        if control_mean == 0:
            return {'absolute_improvement': test_mean - control_mean}
        
        relative_improvement = ((test_mean - control_mean) / abs(control_mean)) * 100
        
        return {
            'absolute_improvement': test_mean - control_mean,
            'relative_improvement_percent': relative_improvement
        }
    
    def _calculate_experiment_duration(self, experiment: Experiment) -> float:
        """è¨ˆç®—å¯¦é©—æŒçºŒæ™‚é–“"""
        if not experiment.start_time:
            return 0.0
        
        end_time = experiment.end_time or datetime.now(timezone.utc)
        duration = end_time - experiment.start_time
        return duration.total_seconds() / 3600  # è½‰æ›ç‚ºå°æ™‚
    
    # ==================== åœæ­¢æ¢ä»¶å’Œæ±ºç­– ====================
    
    async def _evaluate_stop_conditions(self):
        """è©•ä¼°åœæ­¢æ¢ä»¶"""
        for experiment in list(self.active_experiments.values()):
            try:
                decision = await self._evaluate_experiment_decision(experiment)
                
                if decision != ExperimentDecision.CONTINUE:
                    await self._execute_experiment_decision(experiment, decision)
                    
            except Exception as e:
                self.logger.error(f"âŒ Error evaluating stop conditions for {experiment.experiment_id}: {e}")
    
    async def _evaluate_experiment_decision(self, experiment: Experiment) -> ExperimentDecision:
        """è©•ä¼°å¯¦é©—æ±ºç­–"""
        current_time = datetime.now(timezone.utc)
        
        # æª¢æŸ¥æ™‚é–“é™åˆ¶
        if experiment.end_time and current_time >= experiment.end_time:
            # åˆ†æçµæœæ±ºå®šæ˜¯å¦æ¨å»£
            analysis = await self.analyze_experiment_results(experiment.experiment_id)
            
            if self._should_promote_variant(analysis):
                return ExperimentDecision.PROMOTE_VARIANT
            else:
                return ExperimentDecision.TERMINATE
        
        # æª¢æŸ¥æœ€å°æ¨£æœ¬å¤§å°
        min_samples = experiment.minimum_sample_size
        all_variants_ready = all(
            v.sample_count >= min_samples for v in experiment.variants.values()
        )
        
        if not all_variants_ready:
            return ExperimentDecision.CONTINUE
        
        # æª¢æŸ¥åœæ­¢æ¢ä»¶
        stop_conditions = experiment.stop_conditions
        
        # æ—©æœŸåœæ­¢æ¢ä»¶ï¼šé¡¯è‘—å·®ç•°
        if stop_conditions.get('early_stop_on_significance', False):
            analysis = await self.analyze_experiment_results(experiment.experiment_id)
            
            for metric_name in experiment.primary_metrics:
                metric_analysis = analysis.get('metric_comparisons', {}).get(metric_name, {})
                
                for comparison in metric_analysis.get('variant_comparisons', []):
                    stat_test = comparison.get('statistical_test', {})
                    
                    if stat_test.get('is_significant', False):
                        improvement = comparison.get('improvement', {})
                        relative_improvement = improvement.get('relative_improvement_percent', 0)
                        
                        # å¦‚æœæœ‰é¡¯è‘—æ”¹é€²ï¼Œæ¨å»£è®Šé«”
                        if relative_improvement > 5:  # 5%ä»¥ä¸Šæ”¹é€²
                            return ExperimentDecision.PROMOTE_VARIANT
                        # å¦‚æœæœ‰é¡¯è‘—é€€åŒ–ï¼Œå›æ»¾
                        elif relative_improvement < -5:
                            return ExperimentDecision.ROLLBACK
        
        # æª¢æŸ¥æ€§èƒ½é€€åŒ–
        if self._check_performance_degradation(experiment):
            return ExperimentDecision.ROLLBACK
        
        return ExperimentDecision.CONTINUE
    
    def _should_promote_variant(self, analysis: Dict[str, Any]) -> bool:
        """åˆ¤æ–·æ˜¯å¦æ‡‰è©²æ¨å»£è®Šé«”"""
        try:
            metric_comparisons = analysis.get('metric_comparisons', {})
            
            promotion_score = 0
            total_metrics = 0
            
            for metric_name, metric_analysis in metric_comparisons.items():
                for comparison in metric_analysis.get('variant_comparisons', []):
                    total_metrics += 1
                    
                    stat_test = comparison.get('statistical_test', {})
                    improvement = comparison.get('improvement', {})
                    
                    if stat_test.get('is_significant', False):
                        relative_improvement = improvement.get('relative_improvement_percent', 0)
                        
                        if relative_improvement > 2:  # 2%ä»¥ä¸Šæ”¹é€²
                            promotion_score += 2
                        elif relative_improvement > 0:
                            promotion_score += 1
                        else:
                            promotion_score -= 1
                    
            # å¦‚æœå¤§éƒ¨åˆ†æŒ‡æ¨™æœ‰æ”¹é€²ï¼Œæ¨å»£è®Šé«”
            return promotion_score > total_metrics * 0.5
            
        except Exception as e:
            self.logger.error(f"âŒ Error in promotion decision: {e}")
            return False
    
    def _check_performance_degradation(self, experiment: Experiment) -> bool:
        """æª¢æŸ¥æ€§èƒ½æ˜¯å¦é€€åŒ–"""
        try:
            # æª¢æŸ¥æ¸¬è©¦è®Šé«”çš„é—œéµæŒ‡æ¨™æ˜¯å¦å¤§å¹…ä¸‹é™
            for variant in experiment.variants.values():
                if variant.is_control:
                    continue
                
                # æª¢æŸ¥éŒ¯èª¤ç‡
                error_rates = variant.metrics.get('error_rate', [])
                if error_rates and len(error_rates) >= 10:
                    recent_error_rate = statistics.mean(error_rates[-10:])
                    if recent_error_rate > 0.2:  # éŒ¯èª¤ç‡è¶…é20%
                        return True
                
                # æª¢æŸ¥å»¶é²
                latencies = variant.metrics.get('latency', [])
                if latencies and len(latencies) >= 10:
                    recent_latency = statistics.mean(latencies[-10:])
                    if recent_latency > 10000:  # å»¶é²è¶…é10ç§’
                        return True
            
            return False
            
        except Exception as e:
            self.logger.error(f"âŒ Error checking performance degradation: {e}")
            return True  # å‡ºéŒ¯æ™‚ä¿å®ˆè™•ç†
    
    async def _execute_experiment_decision(self, experiment: Experiment, decision: ExperimentDecision):
        """åŸ·è¡Œå¯¦é©—æ±ºç­–"""
        try:
            experiment.decision = decision
            experiment.decision_reason = f"Automated decision based on {decision.value}"
            
            if decision == ExperimentDecision.PROMOTE_VARIANT:
                await self._promote_winning_variant(experiment)
                
            elif decision == ExperimentDecision.ROLLBACK:
                await self._rollback_experiment(experiment)
                
            elif decision == ExperimentDecision.TERMINATE:
                await self._terminate_experiment(experiment)
            
            # å¾æ´»èºå¯¦é©—ä¸­ç§»é™¤
            if experiment.experiment_id in self.active_experiments:
                del self.active_experiments[experiment.experiment_id]
                self.experiment_stats['active_experiments'] -= 1
                
        except Exception as e:
            self.logger.error(f"âŒ Error executing experiment decision: {e}")
    
    async def _promote_winning_variant(self, experiment: Experiment):
        """æ¨å»£ç²å‹è®Šé«”"""
        try:
            # æ‰¾åˆ°æœ€ä½³è®Šé«”
            winning_variant = self._find_winning_variant(experiment)
            
            if winning_variant and not winning_variant.is_control:
                # å°‡ç²å‹è®Šé«”æ¨å»£åˆ°ç”Ÿç”¢ç’°å¢ƒ
                await self.version_control.promote_version_stage(
                    provider=winning_variant.provider,
                    model_id=winning_variant.model_id,
                    version=winning_variant.model_version,
                    target_stage=DeploymentStage.PRODUCTION
                )
                
                # æ›´æ–°æ¨¡å‹èƒ½åŠ›æ•¸æ“šåº«
                updates = {}
                for metric_name in experiment.primary_metrics:
                    values = winning_variant.metrics.get(metric_name, [])
                    if values:
                        if metric_name == 'capability_score':
                            updates['capability_score'] = statistics.mean(values)
                        elif metric_name == 'latency':
                            updates['avg_latency_ms'] = statistics.mean(values)
                        elif metric_name == 'accuracy':
                            updates['accuracy_score'] = statistics.mean(values)
                
                if updates:
                    await self.model_db.update_model_capability(
                        provider=winning_variant.provider,
                        model_id=winning_variant.model_id,
                        updates=updates
                    )
                
                experiment.status = ExperimentStatus.COMPLETED
                self.experiment_stats['completed_experiments'] += 1
                self.experiment_stats['successful_promotions'] += 1
                
                self.logger.info(f"âœ… Promoted winning variant for experiment {experiment.experiment_id}")
        
        except Exception as e:
            self.logger.error(f"âŒ Error promoting winning variant: {e}")
            experiment.status = ExperimentStatus.FAILED
    
    def _find_winning_variant(self, experiment: Experiment) -> Optional[ExperimentVariant]:
        """æ‰¾åˆ°ç²å‹è®Šé«”"""
        try:
            best_variant = None
            best_score = float('-inf')
            
            for variant in experiment.variants.values():
                if variant.is_control:
                    continue
                
                # è¨ˆç®—è®Šé«”ç¶œåˆè©•åˆ†
                score = 0
                metric_count = 0
                
                for metric_name in experiment.primary_metrics:
                    values = variant.metrics.get(metric_name, [])
                    if values:
                        mean_value = statistics.mean(values)
                        
                        # æ ¹æ“šæŒ‡æ¨™é¡å‹èª¿æ•´è©•åˆ†
                        if metric_name in ['capability_score', 'accuracy']:
                            score += mean_value
                        elif metric_name == 'latency':
                            score += max(0, 1.0 - (mean_value / 5000.0))  # å»¶é²æ­¸ä¸€åŒ–
                        
                        metric_count += 1
                
                if metric_count > 0:
                    avg_score = score / metric_count
                    if avg_score > best_score:
                        best_score = avg_score
                        best_variant = variant
            
            return best_variant
            
        except Exception as e:
            self.logger.error(f"âŒ Error finding winning variant: {e}")
            return None
    
    async def _rollback_experiment(self, experiment: Experiment):
        """å›æ»¾å¯¦é©—"""
        try:
            # å°‡æ‰€æœ‰æ¸¬è©¦è®Šé«”å›æ»¾åˆ°é–‹ç™¼éšæ®µ
            for variant in experiment.variants.values():
                if not variant.is_control:
                    await self.version_control.promote_version_stage(
                        provider=variant.provider,
                        model_id=variant.model_id,
                        version=variant.model_version,
                        target_stage=DeploymentStage.DEVELOPMENT
                    )
            
            experiment.status = ExperimentStatus.ROLLED_BACK
            self.experiment_stats['rollbacks'] += 1
            
            self.logger.warning(f"âš ï¸ Rolled back experiment {experiment.experiment_id}")
            
        except Exception as e:
            self.logger.error(f"âŒ Error rolling back experiment: {e}")
            experiment.status = ExperimentStatus.FAILED
    
    async def _terminate_experiment(self, experiment: Experiment):
        """çµ‚æ­¢å¯¦é©—"""
        experiment.status = ExperimentStatus.COMPLETED
        self.experiment_stats['completed_experiments'] += 1
        self.logger.info(f"âœ… Terminated experiment {experiment.experiment_id}")
    
    def _generate_experiment_recommendations(
        self,
        experiment: Experiment,
        analysis: Dict[str, Any]
    ) -> List[str]:
        """ç”Ÿæˆå¯¦é©—å»ºè­°"""
        recommendations = []
        
        try:
            sample_sizes = analysis.get('sample_sizes', {})
            min_sample_size = experiment.minimum_sample_size
            
            # æª¢æŸ¥æ¨£æœ¬å¤§å°
            for variant_id, sample_count in sample_sizes.items():
                if sample_count < min_sample_size:
                    recommendations.append(
                        f"Variant {variant_id} has insufficient sample size ({sample_count} < {min_sample_size}). "
                        "Consider extending experiment duration."
                    )
            
            # æª¢æŸ¥çµ±è¨ˆé¡¯è‘—æ€§
            metric_comparisons = analysis.get('metric_comparisons', {})
            significant_improvements = 0
            total_comparisons = 0
            
            for metric_name, metric_analysis in metric_comparisons.items():
                for comparison in metric_analysis.get('variant_comparisons', []):
                    total_comparisons += 1
                    
                    stat_test = comparison.get('statistical_test', {})
                    improvement = comparison.get('improvement', {})
                    
                    if stat_test.get('is_significant', False):
                        relative_improvement = improvement.get('relative_improvement_percent', 0)
                        
                        if relative_improvement > 5:
                            significant_improvements += 1
                            recommendations.append(
                                f"Significant improvement detected in {metric_name} "
                                f"({relative_improvement:.1f}% improvement). Consider promoting variant."
                            )
                        elif relative_improvement < -5:
                            recommendations.append(
                                f"Significant degradation detected in {metric_name} "
                                f"({relative_improvement:.1f}% degradation). Consider rolling back."
                            )
            
            # ç¸½é«”å»ºè­°
            if significant_improvements == 0 and total_comparisons > 0:
                recommendations.append(
                    "No significant improvements detected. Consider terminating experiment or "
                    "extending duration to collect more data."
                )
            elif significant_improvements >= total_comparisons * 0.5:
                recommendations.append(
                    "Multiple significant improvements detected. Strong candidate for promotion."
                )
            
        except Exception as e:
            self.logger.error(f"âŒ Error generating recommendations: {e}")
            recommendations.append("Error generating recommendations. Please review results manually.")
        
        return recommendations
    
    # ==================== æ¸…ç†å’Œç¶­è­· ====================
    
    async def _cleanup_cache(self):
        """æ¸…ç†è·¯ç”±ç·©å­˜"""
        try:
            current_time = datetime.now(timezone.utc)
            expired_keys = []
            
            for cache_key, expire_time in self.cache_ttl.items():
                if current_time >= expire_time:
                    expired_keys.append(cache_key)
            
            for key in expired_keys:
                if key in self.routing_cache:
                    del self.routing_cache[key]
                if key in self.cache_ttl:
                    del self.cache_ttl[key]
            
        except Exception as e:
            self.logger.error(f"âŒ Error cleaning up cache: {e}")
    
    # ==================== å…¬å…±æ¥å£ ====================
    
    def get_experiment_status(self, experiment_id: str) -> Optional[Dict[str, Any]]:
        """ç²å–å¯¦é©—ç‹€æ…‹"""
        if experiment_id not in self.experiments:
            return None
        
        experiment = self.experiments[experiment_id]
        
        return {
            'experiment_id': experiment_id,
            'experiment_name': experiment.experiment_name,
            'experiment_type': experiment.experiment_type.value,
            'status': experiment.status.value,
            'created_at': experiment.created_at.isoformat(),
            'start_time': experiment.start_time.isoformat() if experiment.start_time else None,
            'end_time': experiment.end_time.isoformat() if experiment.end_time else None,
            'duration_hours': self._calculate_experiment_duration(experiment),
            'variants': {
                variant_id: {
                    'variant_name': variant.variant_name,
                    'provider': variant.provider,
                    'model_id': variant.model_id,
                    'traffic_percentage': variant.traffic_percentage,
                    'sample_count': variant.sample_count,
                    'is_control': variant.is_control
                }
                for variant_id, variant in experiment.variants.items()
            },
            'decision': experiment.decision.value if experiment.decision else None,
            'decision_reason': experiment.decision_reason
        }
    
    def list_experiments(
        self,
        status: Optional[ExperimentStatus] = None,
        experiment_type: Optional[ExperimentType] = None
    ) -> List[Dict[str, Any]]:
        """åˆ—å‡ºå¯¦é©—"""
        experiments = []
        
        for experiment in self.experiments.values():
            # æ‡‰ç”¨éæ¿¾æ¢ä»¶
            if status and experiment.status != status:
                continue
            if experiment_type and experiment.experiment_type != experiment_type:
                continue
            
            experiments.append(self.get_experiment_status(experiment.experiment_id))
        
        # æŒ‰å‰µå»ºæ™‚é–“æ’åº
        experiments.sort(key=lambda x: x['created_at'], reverse=True)
        
        return experiments
    
    def get_system_statistics(self) -> Dict[str, Any]:
        """ç²å–ç³»çµ±çµ±è¨ˆ"""
        return {
            **self.experiment_stats.copy(),
            'cache_size': len(self.routing_cache),
            'running': self._running
        }