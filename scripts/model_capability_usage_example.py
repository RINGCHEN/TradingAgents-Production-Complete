#!/usr/bin/env python3
"""
Model Capability System Usage Example
æ¨¡å‹èƒ½åŠ›ç³»çµ±ä½¿ç”¨ç¤ºä¾‹ - GPT-OSSæ•´åˆä»»å‹™1.2.2

å±•ç¤ºå¦‚ä½•ä½¿ç”¨å®Œæ•´çš„æ¨¡å‹èƒ½åŠ›ç³»çµ±é€²è¡Œæ™ºèƒ½è·¯ç”±å’ŒåŸºæº–æ¸¬è©¦
"""

import asyncio
import logging
import sys
from pathlib import Path

# æ·»åŠ  TradingAgents åˆ° Python è·¯å¾‘
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root / "TradingAgents"))

from tradingagents.services.model_capability_service import ModelCapabilityService
from tradingagents.utils.llm_client import LLMClient

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def example_intelligent_analysis():
    """ç¤ºä¾‹ï¼šä½¿ç”¨æ™ºèƒ½è·¯ç”±é€²è¡Œåˆ†æ"""
    logger.info("ğŸ¯ ç¤ºä¾‹ï¼šæ™ºèƒ½è·¯ç”±åˆ†æ")
    
    # åˆå§‹åŒ–æœå‹™
    service = ModelCapabilityService()
    await service.initialize()
    
    try:
        # ä½¿ç”¨æœ€å„ªæ¨¡å‹é€²è¡Œè²¡å‹™åˆ†æ
        result = await service.analyze_with_optimal_model(
            prompt="è«‹åˆ†æå°ç©é›»(2330)çš„æœ€æ–°è²¡å‹™è¡¨ç¾å’ŒæŠ•è³‡å‰æ™¯",
            task_type="financial_summary",
            context={
                "stock_id": "2330",
                "market": "taiwan",
                "data_source": "finmind"
            },
            user_id="user_001",
            requires_high_quality=True,
            max_acceptable_latency=10000  # 10ç§’
        )
        
        if result['success']:
            logger.info(f"âœ… åˆ†ææˆåŠŸ!")
            logger.info(f"   é¸ç”¨æ¨¡å‹: {result['routing_decision']['selected_provider']}/{result['routing_decision']['selected_model']}")
            logger.info(f"   è·¯ç”±æ¨ç†: {result['routing_decision']['reasoning']}")
            logger.info(f"   å¯¦éš›å»¶é²: {result['performance']['actual_latency_ms']:.0f}ms")
            logger.info(f"   é æœŸæˆæœ¬: ${result['routing_decision']['expected_cost']:.4f}")
            logger.info(f"   åˆ†æå…§å®¹: {result['content'][:200]}...")
        else:
            logger.error(f"âŒ åˆ†æå¤±æ•—: {result['error']}")
    
    finally:
        await service.shutdown()

async def example_model_benchmarking():
    """ç¤ºä¾‹ï¼šæ¨¡å‹åŸºæº–æ¸¬è©¦"""
    logger.info("ğŸ“Š ç¤ºä¾‹ï¼šæ¨¡å‹åŸºæº–æ¸¬è©¦")
    
    service = ModelCapabilityService()
    await service.initialize()
    
    try:
        # å°æ‰€æœ‰æ¨¡å‹é€²è¡ŒåŸºæº–æ¸¬è©¦
        logger.info("é–‹å§‹å°æ‰€æœ‰è¨»å†Šæ¨¡å‹é€²è¡ŒåŸºæº–æ¸¬è©¦...")
        
        # æ³¨æ„ï¼šé€™æœƒä½¿ç”¨æ¨¡æ“¬å®¢æˆ¶ç«¯ï¼Œå¯¦éš›ä½¿ç”¨æ™‚éœ€è¦çœŸå¯¦çš„LLMå®¢æˆ¶ç«¯
        benchmark_result = await service.run_all_model_benchmarks(suite_name="standard")
        
        summary = benchmark_result.get('summary', {})
        logger.info(f"âœ… åŸºæº–æ¸¬è©¦å®Œæˆ!")
        logger.info(f"   ç¸½æ¨¡å‹æ•¸: {summary.get('total_models', 0)}")
        logger.info(f"   æˆåŠŸæ¸¬è©¦: {summary.get('successful_models', 0)}")
        logger.info(f"   å¹³å‡è©•åˆ†: {summary.get('avg_overall_score', 0):.3f}")
        
        # é¡¯ç¤ºæœ€ä½³æ¨¡å‹
        best_model = summary.get('best_model')
        if best_model:
            logger.info(f"   æœ€ä½³æ¨¡å‹: {best_model['name']} (è©•åˆ†: {best_model['score']:.3f})")
    
    finally:
        await service.shutdown()

async def example_model_recommendations():
    """ç¤ºä¾‹ï¼šç²å–æ¨¡å‹æ¨è–¦"""
    logger.info("ğŸ’¡ ç¤ºä¾‹ï¼šæ¨¡å‹æ¨è–¦")
    
    service = ModelCapabilityService()
    await service.initialize()
    
    try:
        # å®šç¾©ä»»å‹™éœ€æ±‚
        task_requirements = {
            'min_capability_score': 0.8,    # æœ€ä½èƒ½åŠ›è¦æ±‚
            'max_cost_per_1k': 0.02,       # æœ€å¤§æˆæœ¬é™åˆ¶
            'max_latency_ms': 3000,         # æœ€å¤§å»¶é²è¦æ±‚
            'privacy_level': 'local',       # éš±ç§è¦æ±‚ï¼šæœ¬åœ°è™•ç†
            'required_features': ['reasoning', 'analysis']  # å¿…éœ€åŠŸèƒ½
        }
        
        recommendations = await service.get_model_recommendations(
            task_requirements=task_requirements,
            limit=5
        )
        
        logger.info(f"âœ… ç²å¾— {len(recommendations)} å€‹æ¨¡å‹æ¨è–¦:")
        for i, rec in enumerate(recommendations, 1):
            model = rec['model']
            score = rec['match_score']
            reason = rec['recommendation_reason']
            
            logger.info(f"   {i}. {model['provider']}/{model['model_id']}")
            logger.info(f"      åŒ¹é…åº¦: {score:.3f}")
            logger.info(f"      æ¨è–¦ç†ç”±: {reason}")
            logger.info(f"      èƒ½åŠ›è©•åˆ†: {model['capability_score']:.3f}")
            logger.info(f"      å¹³å‡å»¶é²: {model['avg_latency_ms']:.0f}ms")
            logger.info(f"      éš±ç§ç´šåˆ¥: {model['privacy_level']}")
            logger.info("")
    
    finally:
        await service.shutdown()

async def example_performance_monitoring():
    """ç¤ºä¾‹ï¼šæ€§èƒ½ç›£æ§"""
    logger.info("ğŸ“ˆ ç¤ºä¾‹ï¼šæ€§èƒ½ç›£æ§")
    
    service = ModelCapabilityService()
    await service.initialize()
    
    try:
        # ç²å–ç•¶å‰æ€§èƒ½æŒ‡æ¨™
        metrics = service.get_current_metrics()
        logger.info(f"âœ… ç•¶å‰ç›£æ§ {len(metrics)} å€‹æ€§èƒ½æŒ‡æ¨™")
        
        for metric_key, stats in list(metrics.items())[:3]:  # åªé¡¯ç¤ºå‰3å€‹
            if stats:
                logger.info(f"   {metric_key}:")
                logger.info(f"     å¹³å‡å€¼: {stats.get('mean', 0):.3f}")
                logger.info(f"     æœ€å¤§å€¼: {stats.get('max', 0):.3f}")
                logger.info(f"     æœ€å°å€¼: {stats.get('min', 0):.3f}")
                logger.info(f"     æ¨£æœ¬æ•¸: {stats.get('count', 0)}")
        
        # ç²å–æ€§èƒ½å ±å‘Š
        report = await service.get_performance_report(hours_back=24)
        logger.info(f"âœ… æ€§èƒ½å ±å‘Šç”Ÿæˆå®Œæˆ")
        logger.info(f"   ç¸½æŒ‡æ¨™æ•¸: {report['summary']['total_metrics']}")
        
        # é¡¯ç¤ºå»ºè­°
        recommendations = report.get('recommendations', [])
        if recommendations:
            logger.info("   ğŸ“‹ ç³»çµ±å»ºè­°:")
            for rec in recommendations[:3]:
                logger.info(f"     - {rec}")
    
    finally:
        await service.shutdown()

async def example_system_health_check():
    """ç¤ºä¾‹ï¼šç³»çµ±å¥åº·æª¢æŸ¥"""
    logger.info("ğŸ” ç¤ºä¾‹ï¼šç³»çµ±å¥åº·æª¢æŸ¥")
    
    service = ModelCapabilityService()
    await service.initialize()
    
    try:
        # åŸ·è¡Œå¥åº·æª¢æŸ¥
        health = await service.health_check()
        
        logger.info(f"âœ… ç³»çµ±å¥åº·ç‹€æ…‹: {health['overall_status']}")
        logger.info("   çµ„ä»¶ç‹€æ…‹:")
        
        for component, status in health.get('components', {}).items():
            if isinstance(status, dict):
                comp_status = status.get('status', 'unknown')
                logger.info(f"     {component}: {comp_status}")
                if 'error' in status:
                    logger.info(f"       éŒ¯èª¤: {status['error']}")
            else:
                logger.info(f"     {component}: {status}")
        
        # ç²å–ç³»çµ±çµ±è¨ˆ
        stats = await service.get_system_statistics()
        logger.info("   ğŸ“Š ç³»çµ±çµ±è¨ˆ:")
        logger.info(f"     è¨»å†Šæ¨¡å‹æ•¸: {stats['models']['total_models']}")
        logger.info(f"     ä»»å‹™é¡å‹æ•¸: {stats['tasks']['total_task_types']}")
        logger.info(f"     LLMè«‹æ±‚æ•¸: {stats['llm_client']['request_count']}")
        logger.info(f"     åŸºæº–æ¸¬è©¦æ•¸: {stats['benchmarks']['total_runs']}")
    
    finally:
        await service.shutdown()

async def main():
    """ä¸»å‡½æ•¸ - åŸ·è¡Œæ‰€æœ‰ç¤ºä¾‹"""
    logger.info("ğŸš€ Model Capability System ä½¿ç”¨ç¤ºä¾‹")
    logger.info("=" * 60)
    
    examples = [
        ("æ™ºèƒ½è·¯ç”±åˆ†æ", example_intelligent_analysis),
        ("æ¨¡å‹æ¨è–¦", example_model_recommendations), 
        ("æ€§èƒ½ç›£æ§", example_performance_monitoring),
        ("ç³»çµ±å¥åº·æª¢æŸ¥", example_system_health_check),
        # ("æ¨¡å‹åŸºæº–æ¸¬è©¦", example_model_benchmarking)  # è¨»é‡‹æ‰é¿å…é•·æ™‚é–“åŸ·è¡Œ
    ]
    
    for name, example_func in examples:
        logger.info(f"\n{'='*60}")
        logger.info(f"åŸ·è¡Œç¤ºä¾‹: {name}")
        logger.info(f"{'='*60}")
        
        try:
            await example_func()
            logger.info(f"âœ… {name} ç¤ºä¾‹åŸ·è¡ŒæˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ {name} ç¤ºä¾‹åŸ·è¡Œå¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
    
    logger.info(f"\n{'='*60}")
    logger.info("ğŸ‰ æ‰€æœ‰ç¤ºä¾‹åŸ·è¡Œå®Œç•¢!")
    logger.info("ğŸ’¡ æç¤ºï¼šå¯¦éš›ä½¿ç”¨æ™‚è«‹ç¢ºä¿ï¼š")
    logger.info("   1. é…ç½®æ­£ç¢ºçš„LLM APIé‡‘é‘°")
    logger.info("   2. GPT-OSSæœ¬åœ°æœå‹™æ­£å¸¸é‹è¡Œ")
    logger.info("   3. æ•¸æ“šåº«é€£æ¥æ­£å¸¸")
    logger.info(f"{'='*60}")

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("\nğŸ›‘ ç¤ºä¾‹è¢«ç”¨æˆ¶ä¸­æ–·")
    except Exception as e:
        logger.error(f"âŒ åŸ·è¡Œéç¨‹ä¸­å‡ºç¾éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()