#!/usr/bin/env python3
"""
LLaMA æ¨¡å‹è¨ªå•é…ç½®è…³æœ¬
è¨­ç½® HuggingFace Token å’Œé©—è­‰ LLaMA æ¨¡å‹è¨ªå•æ¬Šé™
"""

import os
import sys
from pathlib import Path
from huggingface_hub import login, whoami, model_info
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

def setup_hf_token():
    """è¨­ç½® HuggingFace Token"""
    print("Setting up HuggingFace Token...")
    
    # æª¢æŸ¥æ˜¯å¦å·²ç¶“ç™»éŒ„
    try:
        user_info = whoami()
        print(f"Already logged in as: {user_info['name']}")
        return True
    except Exception as e:
        print(f"Not logged in or invalid token: {e}")
        
        # æç¤ºç”¨æˆ¶è¼¸å…¥Token
        print("\nè«‹æŒ‰ç…§ä»¥ä¸‹æ­¥é©Ÿç²å–ä¸¦è¨­ç½® HuggingFace Token:")
        print("1. è¨ªå• https://huggingface.co/settings/tokens")
        print("2. å‰µå»ºæ–°çš„ Token (è‡³å°‘éœ€è¦ 'Read' æ¬Šé™)")
        print("3. è¤‡è£½ Token")
        print("4. åœ¨ä¸‹æ–¹è¼¸å…¥ Token\n")
        
        token = input("è«‹è¼¸å…¥æ‚¨çš„ HuggingFace Token: ").strip()
        
        if not token:
            print("âŒ Token ä¸èƒ½ç‚ºç©º")
            return False
            
        try:
            login(token=token, add_to_git_credential=True)
            print("âœ… Token è¨­ç½®æˆåŠŸ")
            return True
        except Exception as e:
            print(f"âŒ Token è¨­ç½®å¤±æ•—: {e}")
            return False

def verify_llama_access():
    """é©—è­‰ LLaMA æ¨¡å‹è¨ªå•æ¬Šé™"""
    print("\nğŸ” é©—è­‰ LLaMA æ¨¡å‹è¨ªå•æ¬Šé™...")
    
    llama_models = [
        "meta-llama/Llama-3.2-1B-Instruct",
        "meta-llama/Llama-3.2-3B-Instruct",
        "meta-llama/Llama-3.2-7B-Instruct"
    ]
    
    accessible_models = []
    
    for model_name in llama_models:
        try:
            print(f"  æª¢æŸ¥æ¨¡å‹: {model_name}")
            model_info_result = model_info(model_name)
            print(f"  âœ… {model_name} - è¨ªå•æ¬Šé™ç¢ºèª")
            accessible_models.append(model_name)
        except Exception as e:
            print(f"  âŒ {model_name} - ç„¡æ³•è¨ªå•: {e}")
    
    return accessible_models

def test_llama_loading():
    """æ¸¬è©¦ LLaMA æ¨¡å‹è¼‰å…¥"""
    print("\nğŸ§ª æ¸¬è©¦ LLaMA æ¨¡å‹è¼‰å…¥...")
    
    # å…ˆæ¸¬è©¦æœ€å°çš„æ¨¡å‹
    model_name = "meta-llama/Llama-3.2-1B-Instruct"
    
    try:
        print(f"  è¼‰å…¥ Tokenizer: {model_name}")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        print("  âœ… Tokenizer è¼‰å…¥æˆåŠŸ")
        
        print(f"  æª¢æŸ¥ GPU å¯ç”¨æ€§...")
        if torch.cuda.is_available():
            print(f"  âœ… GPU å¯ç”¨: {torch.cuda.get_device_name(0)}")
            print(f"  ğŸ’¾ GPU è¨˜æ†¶é«”: {torch.cuda.get_device_properties(0).total_memory/1024**3:.1f}GB")
        else:
            print("  âš ï¸  GPU ä¸å¯ç”¨ï¼Œå°‡ä½¿ç”¨ CPU")
        
        # æ¸¬è©¦å¿«é€Ÿè¼‰å…¥ (ä¸è¼‰å…¥å®Œæ•´æ¨¡å‹ä»¥ç¯€çœæ™‚é–“)
        print(f"  æ¸¬è©¦æ¨¡å‹é…ç½®è¼‰å…¥...")
        from transformers import AutoConfig
        config = AutoConfig.from_pretrained(model_name)
        print(f"  âœ… æ¨¡å‹é…ç½®è¼‰å…¥æˆåŠŸ")
        print(f"     - éš±è—å±¤å¤§å°: {config.hidden_size}")
        print(f"     - æ³¨æ„åŠ›é ­æ•¸: {config.num_attention_heads}")
        print(f"     - å±¤æ•¸: {config.num_hidden_layers}")
        
        return True
        
    except Exception as e:
        print(f"  âŒ æ¨¡å‹æ¸¬è©¦å¤±æ•—: {e}")
        return False

def update_system_config(accessible_models):
    """æ›´æ–°ç³»çµ±é…ç½®ä»¥æ”¯æ´ LLaMA æ¨¡å‹"""
    print("\nğŸ“ æ›´æ–°ç³»çµ±é…ç½®...")
    
    # æ›´æ–°è‡ªå‹•åŒ–è¨“ç·´é…ç½®
    config_file = "config/training_automation.yaml"
    
    try:
        import yaml
        
        # è®€å–ç¾æœ‰é…ç½®
        if os.path.exists(config_file):
            with open(config_file, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
        else:
            config = {}
        
        # æ·»åŠ  LLaMA æ¨¡å‹åˆ°é»˜èªæ¨¡å‹åˆ—è¡¨
        if 'default_models' not in config:
            config['default_models'] = []
        
        # æ·»åŠ å¯è¨ªå•çš„ LLaMA æ¨¡å‹
        for model in accessible_models:
            if model not in config['default_models']:
                config['default_models'].append(model)
        
        # æ·»åŠ  LLaMA ç‰¹å®šé…ç½®
        config['llama_models'] = {
            "meta-llama/Llama-3.2-1B-Instruct": {
                "max_memory": "6GB",
                "recommended_batch_size": 4,
                "lora_r": 16,
                "lora_alpha": 32,
                "context_length": 4096,
                "priority": "high"
            },
            "meta-llama/Llama-3.2-3B-Instruct": {
                "max_memory": "8GB", 
                "recommended_batch_size": 2,
                "lora_r": 8,
                "lora_alpha": 16,
                "context_length": 4096,
                "priority": "medium"
            },
            "meta-llama/Llama-3.2-7B-Instruct": {
                "max_memory": "12GB",
                "recommended_batch_size": 1,
                "lora_r": 4,
                "lora_alpha": 8, 
                "context_length": 4096,
                "priority": "high"
            }
        }
        
        # ä¿å­˜æ›´æ–°çš„é…ç½®
        with open(config_file, 'w', encoding='utf-8') as f:
            yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
        
        print(f"  âœ… é…ç½®æ›´æ–°å®Œæˆ: {config_file}")
        print(f"     - æ–°å¢ {len(accessible_models)} å€‹ LLaMA æ¨¡å‹")
        
        return True
        
    except Exception as e:
        print(f"  âŒ é…ç½®æ›´æ–°å¤±æ•—: {e}")
        return False

def create_llama_training_dataset():
    """å‰µå»º LLaMA å°ˆç”¨çš„è¨“ç·´æ•¸æ“šé›†"""
    print("\nğŸ“Š å‰µå»º LLaMA è¨“ç·´æ•¸æ“šé›†...")
    
    # å‰µå»ºæ›´è±å¯Œçš„é‡‘èåˆ†ææ•¸æ“šé›†
    llama_dataset = [
        {
            "instruction": "é€²è¡Œå°ç©é›»æŠ€è¡“é¢åˆ†æ",
            "input": "å°ç©é›» (2330) ä»Šæ—¥æ”¶ç›¤åƒ¹ 520 å…ƒï¼Œæˆäº¤é‡è¼ƒæ˜¨æ—¥å¢åŠ  25%ï¼ŒRSI æŒ‡æ¨™ 65",
            "output": "æ ¹æ“šæŠ€è¡“åˆ†æï¼Œå°ç©é›»ç›®å‰å‘ˆç¾ä»¥ä¸‹ç‰¹å¾µï¼š1) RSI 65 è™•æ–¼ä¸­æ€§åå¤šå€é–“ï¼Œé¡¯ç¤ºè‚¡åƒ¹ä¸Šæ¼²å‹•èƒ½ä»å­˜ä½†éœ€æ³¨æ„éç†±é¢¨éšªï¼›2) æˆäº¤é‡æ”¾å¤§ 25% é¡¯ç¤ºå¸‚å ´é—œæ³¨åº¦æå‡ï¼Œè‹¥èƒ½é…åˆåƒ¹æ ¼çªç ´å°‡å½¢æˆå¼·å‹¢è¨Šè™Ÿï¼›3) å»ºè­°é—œæ³¨ 530 å…ƒé˜»åŠ›ä½ï¼Œçªç ´å¾Œæœ‰æœ›æŒ‘æˆ° 550 å…ƒï¼›4) æ­¢æè¨­åœ¨ 500 å…ƒæ”¯æ’ä½ã€‚"
        },
        {
            "instruction": "è©•ä¼°é´»æµ·åŸºæœ¬é¢æŠ•è³‡åƒ¹å€¼",
            "input": "é´»æµ· (2317) Q3 ç‡Ÿæ”¶ 1.8 å…†æ–°å°å¹£ï¼Œå¹´å¢ 20%ï¼Œæ¯›åˆ©ç‡æå‡è‡³ 7.2%ï¼ŒAI ä¼ºæœå™¨è¨‚å–®å¤§å¢",
            "output": "é´»æµ·åŸºæœ¬é¢åˆ†æçµæœï¼š**æ­£é¢å› ç´ **ï¼š1) ç‡Ÿæ”¶å¹´å¢ 20% é¡¯ç¤ºæ¥­å‹™æˆé•·å¼·å‹ï¼›2) æ¯›åˆ©ç‡æå‡è‡³ 7.2% åæ˜ ç”¢å“çµ„åˆå„ªåŒ–å’Œæˆæœ¬æ§åˆ¶æˆæ•ˆï¼›3) AI ä¼ºæœå™¨éœ€æ±‚çˆ†ç™¼å°‡æˆç‚ºæ–°æˆé•·å‹•èƒ½ï¼›4) å—æƒ æ–¼ AI åŸºç¤è¨­æ–½å»ºè¨­é•·æœŸè¶¨å‹¢ã€‚**æŠ•è³‡å»ºè­°**ï¼šç›®æ¨™åƒ¹èª¿å‡ 15%ï¼Œå»ºè­°é€¢ä½ä½ˆå±€ï¼Œé•·æœŸæŒæœ‰äº«å— AI è½‰å‹ç´…åˆ©ã€‚é¢¨éšªæé†’ï¼šéœ€é—œæ³¨ä¸­ç¾è²¿æ˜“æ”¿ç­–è®ŠåŒ–å°ä¾›æ‡‰éˆçš„å½±éŸ¿ã€‚"
        },
        {
            "instruction": "åˆ†æè¯ç™¼ç§‘ç«¶çˆ­å„ªå‹¢èˆ‡é¢¨éšª",
            "input": "è¯ç™¼ç§‘ (2454) æ¨å‡ºå¤©ç’£ 9400 æ——è‰¦æ™¶ç‰‡ï¼Œæ¡ç”¨ 3nm è£½ç¨‹ï¼ŒAI æ€§èƒ½æå‡ 40%ï¼Œèˆ‡é«˜é€š Snapdragon 8 Gen 4 æ­£é¢ç«¶çˆ­",
            "output": "è¯ç™¼ç§‘ç«¶çˆ­åˆ†æï¼š**æ ¸å¿ƒå„ªå‹¢**ï¼š1) å¤©ç’£ 9400 æ¡ç”¨æœ€å…ˆé€² 3nm è£½ç¨‹ï¼ŒåŠŸè€—æ§åˆ¶å„ªç§€ï¼›2) AI æ€§èƒ½å¤§å¹…æå‡ 40% ç¬¦åˆæ‰‹æ©Ÿ AI åŒ–è¶¨å‹¢ï¼›3) æˆæœ¬æ§åˆ¶èƒ½åŠ›å¼·ï¼Œæœ‰æœ›åœ¨ä¸­é«˜éšå¸‚å ´æ¶å æ›´å¤šä»½é¡ã€‚**é¢è‡¨æŒ‘æˆ°**ï¼š1) èˆ‡é«˜é€šæ——è‰¦ç”¢å“ç›´æ¥ç«¶çˆ­ï¼Œéœ€è¦åœ¨æ€§èƒ½å’Œåƒ¹æ ¼é–“æ‰¾åˆ°å¹³è¡¡ï¼›2) è˜‹æœè‡ªç ”æ™¶ç‰‡å°é«˜éšå¸‚å ´çš„å£“ç¸®ï¼›3) åœ°ç·£æ”¿æ²»é¢¨éšªå½±éŸ¿å…¨çƒä½ˆå±€ã€‚**æŠ•è³‡ç­–ç•¥**ï¼šå»ºè­°é—œæ³¨ç”¢å“ç™¼å¸ƒå¾Œçš„å¸‚å ´æ¥å—åº¦ï¼Œè‹¥ç²å¾—ä¸»æµå“ç‰Œæ¡ç”¨å°‡æœ‰æœ›é‡æ–°è©•ä¼°ä¼°å€¼ã€‚"
        },
        {
            "instruction": "æä¾›åŠå°é«”æ—ç¾¤æŠ•è³‡é…ç½®å»ºè­°",
            "input": "ç›®å‰åŠå°é«”åº«å­˜èª¿æ•´æ¥è¿‘å°¾è²ï¼ŒAI éœ€æ±‚æŒçºŒå¼·å‹ï¼Œè¨˜æ†¶é«”åƒ¹æ ¼é–‹å§‹å›å‡ï¼Œå®¢æˆ¶è£œåº«å­˜éœ€æ±‚å¢åŠ ",
            "output": "åŠå°é«”æŠ•è³‡é…ç½®ç­–ç•¥ï¼š**æ ¸å¿ƒé…ç½® (40%)**ï¼šå°ç©é›» - AI æ™¶ç‰‡ä»£å·¥é¾é ­ï¼Œå—æƒ æ˜ç¢ºä¸”æŒçºŒï¼›**æˆé•·é…ç½® (30%)**ï¼šè¯ç™¼ç§‘ã€å‰µæ„ - AI æ™¶ç‰‡è¨­è¨ˆèˆ‡ IP æˆæ¬Šï¼Œå½ˆæ€§è¼ƒå¤§ï¼›**å¾ªç’°é…ç½® (20%)**ï¼šå—äºç§‘ã€è¯é‚¦é›» - è¨˜æ†¶é«”åƒ¹æ ¼å›å‡å—æƒ ï¼›**è¡›æ˜Ÿé…ç½® (10%)**ï¼šçŸ½åŠ›-KYã€è¯è©  - é›»æºç®¡ç†å’Œé©…å‹• ICï¼Œä¸‹æ¸¸å¾©ç”¦å—æƒ ã€‚**æ™‚æ©Ÿåˆ¤æ–·**ï¼šå»ºè­°åˆ†æ‰¹é€²å ´ï¼Œé‡é»é—œæ³¨ Q4 è²¡å ±å’Œ 2025 å¹´è³‡æœ¬æ”¯å‡ºæŒ‡å¼•ã€‚é¢¨éšªæ§ç®¡ï¼šè¨­å®šåœæé»ä½ï¼Œé—œæ³¨ç¾åœ‹å°ä¸­æ”¿ç­–è®ŠåŒ–ã€‚"
        },
        {
            "instruction": "åˆ†æé‡‘èè‚¡å‡æ¯å½±éŸ¿è©•ä¼°",
            "input": "å¤®è¡Œå‡æ¯ 1 ç¢¼è‡³ 2.0%ï¼ŒéŠ€è¡Œæ·¨åˆ©å·®å¯æœ›æ”¹å–„ï¼Œä½†æˆ¿è²¸éœ€æ±‚å¯èƒ½å—å½±éŸ¿ï¼Œä¿¡ç”¨é¢¨éšªéœ€è¦é—œæ³¨",
            "output": "é‡‘èè‚¡å‡æ¯å½±éŸ¿è©•ä¼°ï¼š**æ­£é¢æ•ˆæ‡‰**ï¼š1) æ·¨åˆ©å·®æ”¹å–„ï¼šå‡æ¯ç›´æ¥æœ‰åˆ©éŠ€è¡Œç²åˆ©ï¼Œé ä¼°å¤§å‹é‡‘æ§ NIM å¯æå‡ 5-10 bpï¼›2) è³‡ç”¢å“è³ªï¼šé«˜åˆ©ç‡ç’°å¢ƒæœ‰åŠ©ç¯©é¸å„ªè³ªå€Ÿæ¬¾äººï¼›3) æŠ•è³‡æ”¶ç›Šï¼šå‚µåˆ¸æŠ•è³‡éƒ¨ä½é‡è¨‚åƒ¹å—æƒ ã€‚**è² é¢é¢¨éšª**ï¼š1) ä¿¡è²¸éœ€æ±‚ä¸‹é™ï¼šæˆ¿è²¸ã€ä¼æ¥­æ”¾æ¬¾æˆé•·å¯èƒ½è¶¨ç·©ï¼›2) ä¿¡ç”¨æˆæœ¬ä¸Šå‡ï¼šç¶“æ¿Ÿæ”¾ç·©æ™‚é•ç´„ç‡å¯èƒ½æé«˜ï¼›3) ä¼°å€¼å£“åˆ¶ï¼šé‡‘èè‚¡ P/B åœ¨å‡æ¯ç’°å¢ƒä¸‹é€šå¸¸æ‰¿å£“ã€‚**æŠ•è³‡å»ºè­°**ï¼šå„ªé¸è³‡æœ¬é©è¶³ç‡é«˜ã€è³‡ç”¢å“è³ªä½³çš„å¤§å‹é‡‘æ§ï¼Œå¦‚å¯Œé‚¦é‡‘ã€åœ‹æ³°é‡‘ï¼Œå»ºè­°æ¬Šé‡ 15-20%ã€‚"
        },
        {
            "instruction": "é æ¸¬å°è‚¡æœªä¾† 6 å€‹æœˆèµ°å‹¢",
            "input": "å°è‚¡åŠ æ¬ŠæŒ‡æ•¸ç›®å‰ 17,800 é»ï¼Œå¤–è³‡æŒçºŒè³£è¶…ï¼Œä½†å…§è³‡è²·ç›¤æ”¯æ’ï¼Œç¾è‚¡é‚£æ–¯é”å…‹å‰µæ–°é«˜ï¼Œå°ç©é›» ADR è¡¨ç¾å¼·å‹¢",
            "output": "å°è‚¡ 6 å€‹æœˆå±•æœ›åˆ†æï¼š**æŠ€è¡“é¢**ï¼š17,800 é»ç‚ºé‡è¦æ”¯æ’å€ï¼Œéœ€å®ˆä½ç¢ºä¿ä¸­æœŸè¶¨å‹¢ï¼›ä¸Šæª” 18,500-19,000 ç‚ºå£“åŠ›å€ã€‚**è³‡é‡‘é¢**ï¼šå¤–è³‡è³£å£“ä¸»å› åŒ¯ç‡å’Œä¸­ç¾é—œä¿‚ï¼Œå…§è³‡å‹•èƒ½ä¾†è‡ªä¿éšªè³‡é‡‘å’Œé€€ä¼‘åŸºé‡‘ï¼›é æœŸ Q4 è³‡é‡‘é¢å°‡è¶¨æ–¼å¹³è¡¡ã€‚**åŸºæœ¬é¢**ï¼šAI æ¦‚å¿µè‚¡æ¥­ç¸¾æ”¯æ’æŒ‡æ•¸ï¼Œä½†å‚³çµ±ç”¢æ¥­å¾©ç”¦ä»éœ€æ™‚é–“ï¼›é—œæ³¨å°ç©é›»æ³•èªªæœƒå’Œè˜‹æœæ–°å“éŠ·å”®ã€‚**é æ¸¬å€é–“**ï¼šæ¨‚è§€æƒ…å¢ƒ 19,500 é»ï¼ˆAI è¶…é æœŸ+è³‡é‡‘å›æµï¼‰ï¼ŒåŸºæº–æƒ…å¢ƒ 18,200-18,800 é»ï¼ˆéœ‡ç›ªæ•´ç†ï¼‰ï¼Œæ‚²è§€æƒ…å¢ƒ 17,000 é»ï¼ˆåœ°ç·£é¢¨éšª+ç¶“æ¿Ÿè¡°é€€ï¼‰ã€‚**æ“ä½œç­–ç•¥**ï¼šå»ºè­°ç¶­æŒ 6-7 æˆå€‰ä½ï¼Œé‡é»å¸ƒå±€ AIã€ç¶ èƒ½ã€è»å·¥ç­‰é¡Œæã€‚"
        }
    ]
    
    # ä¿å­˜æ•¸æ“šé›†
    dataset_dir = "data/datasets/llama_financial"
    os.makedirs(dataset_dir, exist_ok=True)
    
    import json
    dataset_file = f"{dataset_dir}/training_data.jsonl"
    
    with open(dataset_file, 'w', encoding='utf-8') as f:
        for item in llama_dataset:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    print(f"  âœ… LLaMA è¨“ç·´æ•¸æ“šé›†å‰µå»ºå®Œæˆ")
    print(f"     - æ–‡ä»¶è·¯å¾‘: {dataset_file}")
    print(f"     - æ¨£æœ¬æ•¸é‡: {len(llama_dataset)}")
    
    return dataset_file

def main():
    """ä¸»å‡½æ•¸"""
    print("LLaMA model access configuration starting...")
    print("=" * 50)
    
    # Step 1: è¨­ç½® HuggingFace Token
    if not setup_hf_token():
        print("âŒ Token è¨­ç½®å¤±æ•—ï¼Œè«‹é‡æ–°é‹è¡Œè…³æœ¬")
        return False
    
    # Step 2: é©—è­‰æ¨¡å‹è¨ªå•æ¬Šé™
    accessible_models = verify_llama_access()
    if not accessible_models:
        print("âŒ ç„¡æ³•è¨ªå•ä»»ä½• LLaMA æ¨¡å‹ï¼Œè«‹æª¢æŸ¥æ¬Šé™ç”³è«‹ç‹€æ…‹")
        return False
    
    print(f"\nâœ… å¯è¨ªå•çš„æ¨¡å‹æ•¸é‡: {len(accessible_models)}")
    
    # Step 3: æ¸¬è©¦æ¨¡å‹è¼‰å…¥
    if not test_llama_loading():
        print("âš ï¸  æ¨¡å‹è¼‰å…¥æ¸¬è©¦å¤±æ•—ï¼Œä½†å¯ä»¥ç¹¼çºŒé…ç½®")
    
    # Step 4: æ›´æ–°ç³»çµ±é…ç½®
    if not update_system_config(accessible_models):
        print("âŒ ç³»çµ±é…ç½®æ›´æ–°å¤±æ•—")
        return False
    
    # Step 5: å‰µå»ºè¨“ç·´æ•¸æ“šé›†
    dataset_file = create_llama_training_dataset()
    
    print("\n" + "=" * 50)
    print("ğŸ‰ LLaMA æ¨¡å‹è¨ªå•é…ç½®å®Œæˆï¼")
    print("\nä¸‹ä¸€æ­¥æ“ä½œ:")
    print("1. é‹è¡Œè‡ªå‹•åŒ–è¨“ç·´æ¸¬è©¦:")
    print("   python training_automation/automated_training_pipeline.py --action create_job --model meta-llama/Llama-3.2-1B-Instruct --dataset data/datasets/llama_financial")
    print("2. å•Ÿå‹•æ€§èƒ½åŸºæº–æ¸¬è©¦:")
    print("   python scripts/benchmark_llama_models.py")
    print("3. æŸ¥çœ‹è¨“ç·´å„€è¡¨æ¿:")
    print("   python training_automation/dashboard_app.py (è¨ªå• http://localhost:8888)")
    
    return True


# è‡ªå‹•æª¢æ¸¬å’Œåˆ‡æ›åˆ° TradingAgents ç›®éŒ„
def ensure_tradingagents_directory():
    """ç¢ºä¿ç•¶å‰å·¥ä½œç›®éŒ„åœ¨ TradingAgents/ ä¸‹ï¼Œä»¥æ­£ç¢ºè¨ªå•é…ç½®æ–‡ä»¶"""
    current_dir = Path.cwd()
    
    # å¦‚æœç•¶å‰ç›®éŒ„æ˜¯ TradingAgents çš„çˆ¶ç›®éŒ„ï¼Œåˆ‡æ›åˆ° TradingAgents
    if (current_dir / 'TradingAgents').exists():
        os.chdir(current_dir / 'TradingAgents')
        print(f"[DIR] è‡ªå‹•åˆ‡æ›å·¥ä½œç›®éŒ„åˆ°: {Path.cwd()}")
    
    # é©—è­‰å¿…è¦çš„ç›®éŒ„å­˜åœ¨
    required_dirs = ['configs', 'training', 'tradingagents']
    missing_dirs = [d for d in required_dirs if not Path(d).exists()]
    
    if missing_dirs:
        raise FileNotFoundError(f"ç¼ºå°‘å¿…è¦ç›®éŒ„: {missing_dirs}. è«‹ç¢ºä¿å¾ TradingAgents/ ç›®éŒ„åŸ·è¡Œæ­¤è…³æœ¬")

# ç›®éŒ„æª¢æŸ¥å‡½æ•¸å·²æº–å‚™å¥½ï¼Œä½†ä¸åœ¨æ¨¡çµ„å°å…¥æ™‚è‡ªå‹•åŸ·è¡Œ
# åªåœ¨éœ€è¦æ™‚æ‰‹å‹•èª¿ç”¨ ensure_tradingagents_directory()

if __name__ == "__main__":
    main()